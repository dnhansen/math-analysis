% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Miscellaneous analysis notes}
\newcommand\docauthor{Danny NygÃ¥rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{latex-sty/topologycommands}
% \usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{fit, patterns}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}
\setlist{
    listparindent=\parindent,
    parsep=0pt,
}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\setF}{\mathbb{F}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\borel}{\mathcal{B}}
\newcommand{\measurable}{\mathcal{M}}
\newcommand{\wto}{\Rightarrow}
\DeclarePairedDelimiter{\net}{\langle}{\rangle}
\newcommand{\strucS}{\mathfrak{S}}
\DeclarePairedDelimiter{\gen}{\langle}{\rangle} % Generating set
\newcommand{\frakL}{\mathfrak{L}}

\newenvironment{displaytheorem}{%
	\begin{displayquote}\itshape%
}{%
	\end{displayquote}%
}

% Break
\usepackage{adforn}
\newcommand\fleuronbreak{\fancybreak{\textcolor{linkcolor}{\adfhangingflatleafleft}}}


\begin{document}

\maketitle

\chapter{The exponential function}

\begin{theorem}
    Given $b > 0$ there exists a unique continuous function $E_b \colon \reals \to \reals$ such that $E_b(m/n) = (b^m)^{1/n}$ for all $m,n \in \ints$ with $n > 0$.
\end{theorem}

\begin{proof}
    We first assume that $b > 1$. The proof of the theorem in this case is by the following stages:
    %
    \begin{enumerate}
        \item Given $m,n,p,q \in \ints$ with $n,q > 0$ and $r = m/n = p/q$, then
        %
        \begin{equation*}
            (b^m)^{1/n}
                = (b^p)^{1/q}.
        \end{equation*}
        %
        Thus defining $E_b(r) = (b^m)^{1/n}$ makes sense.

        \item If $r,s \in \rationals$, then $E_b(r+s) = E_b(r) E_b(s)$. In particular, $E_b$ is increasing on $\rationals$.
        
        \item The map $E_b$ is continuous on $\rationals$.
        
        \item For $x \in \reals$, let
        %
        \begin{equation*}
            L_x
                = \set{E_b(r)}{t \in \rationals, t \leq x}.
        \end{equation*}
        %
        Then $E_b(r) = \sup L_r$ for $r \in \rationals$, so it makes sense to define $E_b(x) = \sup L_x$ for all $x \in \reals$. Hence $E_b$ is increasing and therefore continuous on $\reals$.

        \item For $x,y \in \reals$ we have $E_b(x+y) = E_b(x) E_b(y)$.
    \end{enumerate}
    %
    (1) Notice that $mq = pn$, so
    %
    \begin{equation*}
        \bigl[ (b^m)^{1/n} \bigr]^{pn}
            = (b^m)^p
            = (b^p)^m
            = \bigl[ (b^p)^{1/q} \bigr]^{mq}
            = \bigl[ (b^p)^{1/q} \bigr]^{pn}.
    \end{equation*}
    %
    The (positive) $pn$th root of this number is unique, so $(b^m)^{1/n} = (b^p)^{1/q}$.

    (2) Write $r = m/n$ and $s = p/q$ for appropriate $m,n,p,q \in \ints$ with $n,q > 0$. Then
    %
    \begin{equation*}
        [ (b^m)^{1/n} (b^p)^{1/q} ]^{nq}
            = b^{mq} b^{pn}
            = b^{mq + pn}.
    \end{equation*}
    %
    Taking the $nq$th root implies that
    %
    \begin{equation*}
        E_b(r) E_b(s)
            = (b^m)^{1/n} (b^p)^{1/q}
            = (b^{mq+pn})^{1/nq}
            = E_b(r + s),
    \end{equation*}
    %
    where we in the last equality use that
    %
    \begin{equation*}
        r + s
            = \frac{m}{n} + \frac{p}{q}
            = \frac{mq + pn}{nq}.
    \end{equation*}
    %
    To see that $E_b$ is increasing on $\rationals$, notice that the assumption that $b > 1$ implies that $E_b(s) > 1$ for $s > 0$. If also $r \in \rationals$, then
    %
    \begin{equation*}
        E_b(r + s)
            = E_b(r) E_b(s)
            \geq E_b(r),
    \end{equation*}
    %
    so $E_b$ is increasing.
    
    (3) We begin by showing that $E_b$ is continuous from above at $0$. Since $E_b$ is monotonic and $E_b(0) = 1$, it suffices to show that $\lim_{n\to\infty} E_b(r_n) = 1$ for some sequence $(r_n)_{n\in\naturals}$ in $\rationals$ that decreases to $0$. We claim that the sequence given by $r_n = 1/n$ has this property. Clearly $1/n \downarrow 0$, so assume that $E_b(1/n)$ did not converge to $1$. Then there would be some $\epsilon > 0$ such that $E_b(1/n) \geq 1 + \epsilon$, i.e. $b \geq (1 + \epsilon)^n$, for all $n \in \naturals$. But by [Bernoulli's inequality] this is impossible, so we must have $E_b(1/n) \to 1$. A similar argument shows that $E_b$ is continuous from below at $0$, using the fact that $E_b(-1/n) = (1/b)^n$.

    Finally let $r,h \in \rationals$, and notice that
    %
    \begin{equation*}
        E_b(r+h)
            = E_b(r) E_b(h)
            \to E_b(r) E_b(0)
            = E_b(r)
    \end{equation*}
    %
    as $n \to \infty$. Thus $E_b$ is also continuous at $r$.

    (4) We clearly have $E_b(r) \leq \sup L_r$. For the opposite inequality, notice that $E_b(r)$ is an upper bound for $L_r$ since $E_b$ is increasing on $\rationals$. Hence also $\sup L_r \leq E_b(r)$.

    If $x \leq y$ then $L_x \subseteq L_y$, and hence $E_b(x) \leq E_b(y)$. Thus $E_b$ is monotonic on $\reals$. But then since $E_b$ continuous on a dense subset of $\reals$, it is clearly also continuous on $\reals$.

    (5) Let $(r_n)$ and $(s_n)$ be sequences in $\rationals$ with limits $x$ and $y$ respectively. Then $r_n+s_n$ converges to $x+y$, so
    %
    \begin{equation*}
        E_b(x+y)
            = \lim_{n\to\infty} E_b(r_n + s_n)
            = \lim_{n\to\infty} E_b(r_n) E_b(s_n)
            = E_b(x) E_b(y).
    \end{equation*}
\end{proof}


\chapter{Introduction}

\begin{definition}
    Let $X$ be a set. A \emph{sequence} in $X$ is a map $a \colon \naturals \to X$. For $n \in \naturals$, we usually write $a_n$ for $a(n)$ and denote $a$ by $(a_n)_{n\in\naturals}$ or simply $(a_n)$.
\end{definition}


\begin{definition}
    Let $(S,\rho)$ be a metric space, and let $(a_n)_{n\in\naturals}$ be a sequence in $S$. We say that $(a_n)$ \emph{converges} to a point $a \in S$ if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $n \geq N$ implies that $\rho(a_n,a) < \epsilon$. In this case we call $a$ the \emph{limit} of $(a_n)$ and write $a_n \to a$ as $n \to \infty$, and we say that $(a_n)$ is \emph{convergent}.

    Furthermore, $(a_n)$ is called a \emph{Cauchy sequence} if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $m,n \geq N$ implies that $\rho(a_m,a_n) < \epsilon$. If every Cauchy sequence in $S$ is convergent, then $S$ is said to be \emph{complete}.
\end{definition}
%
Notice that limits of sequences in metric spaces are unique. It is also clear that convergent sequences are Cauchy, and that Cauchy sequences are bounded: We say that a sequence $(a_n)_{n\in\naturals}$ in a metric space is \emph{bounded} if the set $\set{a_n}{n\in\naturals}$ is bounded.

If $(X,\leq)$ is a poset, a sequence $(a_n)_{n\in\naturals}$ in $X$ is \emph{increasing} (\emph{decreasing}) if $m \leq n$ implies $a_m \leq a_n$ ($a_m \geq a_n$) for all $m,n \in \naturals$. If $m < n$ implies $a_m < a_n$ ($a_m > a_n$), then $(a_n)$ is \emph{strictly} increasing (decreasing). A sequence that is either (strictly) increasing or (strictly) decreasing is called \emph{(strictly) monotonic}.

Given a sequence $(a_n)_{n\in\naturals}$ in a set $X$ and a strictly increasing sequence $(n_k)_{k\in\naturals}$ in $\naturals$, the sequence $(a_{n_k})_{k\in\naturals}$ is called a \emph{subsequence} of $(a_n)$. In particular, every sequence is a subsequence of itself.

\begin{lemma}
    Let $(a_n)_{n\in\naturals}$ be a sequence in a metric space $(S,\rho)$. If $(a_n)$ is both Cauchy and has a convergent subsequence, then $(a_n)$ itself is convergent.
\end{lemma}

\begin{proof}
    Let $(a_{n_k})_{k\in\naturals}$ be a convergent subsequence of $(a_n)$, and let $\epsilon > 0$. Choose $N_1, N_2 \in \naturals$ such that
    %
    \begin{equation*}
        m,n \geq N_1
        \quad \implies \quad
        \rho(a_m, a_n) < \frac{\epsilon}{2}
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        k \geq N_2
        \quad \implies \quad
        \rho(a_{n_k}, a) < \frac{\epsilon}{2},
    \end{equation*}
    %
    where $a \in S$ is the limit of $(a_{n_k})$. For $n \geq N_1 \join N_2$ we thus have
    %
    \begin{equation*}
        \rho(a_n, a)
            \leq \rho(a_n, a_m) + \rho(a_m, a)
            < \frac{\epsilon}{2} + \frac{\epsilon}{2}
            = \epsilon,
    \end{equation*}
    %
    showing that $a_n \to a$ as $n \to \infty$.
\end{proof}


\section{Sequences of real numbers}

\begin{proposition}
    Let $(a_n)_{n\in\naturals}$ be a monotonic sequence in $\reals$. Then $(a_n)$ is convergent if and only if it is bounded, in which case it converges to $\sup_{n\in\naturals} a_n$ if it is increasing and $\inf_{n\in\naturals} a_n$ if it is decreasing.
\end{proposition}

\begin{proof}
    If $(a_n)$ is convergent then it is bounded, so assume that it is bounded and let $\epsilon > 0$. For definiteness we assume that it is increasing and let $s = \sup_{n\in\naturals} a_n$. By definition of $s$ there exists an $N \in \naturals$ such that $s - a_N < \epsilon$. Since $(a_n)$ is increasing and $s$ is an upper bound of the sequence, we thus have
    %
    \begin{equation*}
        0 \leq s - a_n < \epsilon
    \end{equation*}
    %
    for all $n \geq N$, proving that $a_n \to s$.
\end{proof}

\begin{lemma}
    Every sequence in $\reals$ has a monotonic subsequence.
\end{lemma}

\begin{proof}
    Let $(a_n)_{n\in\naturals}$ be a sequence in $\reals$. We say that $n \in \naturals$ is a \emph{peak} if $a_n \geq a_m$ for all $m \geq n$. If $(a_n)$ has infinitely many peaks, the subsequence consisting of these constitute a decreasing subsequence.

    Hence we assume that $(a_n)$ only has finitely many peaks. We construct an increasing sequence $(n_k)_{n\in\naturals}$ in $\naturals$ as follows: Let $n_1 \in \naturals$ be such that all peaks are strictly less than $n_1$, and assume that $n_1, \ldots, n_{k-1}$ have been chosen such that $a_1 \leq \cdots \leq a_{n_{k-1}}$. Since $a_{n_{k-1}}$ is not a peak there is an $n' > n_{k-1}$ such that $a_{n_{k-1}} < a_{n'}$. Letting $n_k = n'$ we obtain an increasing subsequence $(a_{n_k})$ of $(a_n)$, proving the claim.
\end{proof}

\begin{theorem}[The Bolzano--Weierstrass theorem]
    Every subset of $\reals^d$ is sequentially compact if and only if it is closed and bounded.
\end{theorem}
%
We recall that a topological space $X$ is \emph{sequentially compact} if every sequence in $X$ has a convergent subsequence.

\begin{proof}
    We begin with the case $d = 1$. Let $A \subseteq \reals$ be closed and bounded, and let $(a_n)_{n\in\naturals}$ be a sequence in $A$. Let $(a_{n_k})$ be a monotonic subsequence of $(a_n)$, and notice that $(a_{n_k})$ is convergent since it is bounded.

    The case for general $d$ follows by induction in $d$, by noticing that a sequence in $\reals^d$ converges if and only if each coordinate sequence converges.

    For the converse, let $A \subseteq \reals^d$ be sequentially compact. If $A$ were not bounded we could choose $a_n \in A \intersect B(0,n)$ for all $n \in \naturals$, yielding a sequence $(a_n)$ with no convergent subsequence. Furthermore, if $(a_n)$ is a sequence in $A$ converging to a point $a \in \reals^d$, it is in particular a Cauchy sequence. Since it has a subsequence converging to a point $a' \in A$, and by [lemma] we must have $a = a'$. Thus $A$ is also closed.
\end{proof}


\begin{theorem}[Completeness of $\reals^d$]
    The Euclidean space $\reals^d$ is complete.
\end{theorem}

\begin{proof}
    Let $(a_n)_{n\in\naturals}$ be a Cauchy sequence in $\reals^d$. Hence it is bounded, and so it has a convergent subsequence by the Bolzano--Weierstrass theorem. But then $(a_n)$ itself converges by [lemma], so $\reals^d$ is complete.
\end{proof}


\begin{theorem}[The Heine--Borel theorem]
    Every subset of $\reals^d$ is compact if and only if it is closed and bounded.
\end{theorem}

\begin{proof}
    Of course every compact set is closed in any Hausdorff space and bounded in any metric space, so we only consider the other implication.
    
    We first show that closed and bounded intervals are compact. Consider the interval $[a,b]$, and let $\calU$ be an open cover of $[a,b]$. Define the set
    %
    \begin{equation*}
        A
            = \set[\big]{x \in [a,b]}{\text{$[a,x]$ has a finite subcover in $\calU$}}.
    \end{equation*}
    %
    We clearly have $a \in A$ since a point is covered by a single set in $\calU$. If $s = \sup A$ then $a \leq s \leq b$. Suppose that $s < b$ and choose a set $U \in \calU$ with $s \in U$. There exist $r,t \in U$ such that $r < s < t$, and so $r \in A$. Let $\calU'$ denote a finite subcover of $[a,r]$ in $\calU$. Then $\calU' \union \{U\}$ is a finite subcover of $[a,t]$, contradicting the assumption that $s < b$. Hence $s = b$.

    Next, choose $V \in U$ with $b \in V$, and let $c \in V$ with $c < b$. Then $c \in A$, and adjoining $V$ to a finite subcover of $[a,c]$ yields a finite subcover of $[a,b]$, so $b \in A$. Thus $[a,b]$ is compact.

    Finally, let $K \subseteq \reals^d$ be closed and bounded. Since it is bounded it is contained in some cube $[-a,a]^d$. But this cube is a product of compact sets and hence compact, so $K$ is a closed subset of a compact set. The claim follows.
\end{proof}


If $A$ is a subset of a metric space $S$, recall that $A$ is \emph{totally bounded} if, for every $\epsilon > 0$, $A$ can be covered by finitely many open balls of radius $\epsilon$.

\begin{theorem}
    If $A$ is a subset of a metric space $(S,\rho)$, then the following are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:complete-totally-bounded} $A$ is complete and totally bounded.
        \item \label{enum:sequentially-compact} $A$ is sequentially compact.
        \item \label{enum:compact} $A$ is compact.
    \end{enumthm}
\end{theorem}

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:complete-totally-bounded} $\implies$ \subcref{enum:sequentially-compact}]
    Assume that $A$ is complete and totally bounded, and let $(x_n)_{n\in\naturals}$ be a sequence in $A$. Now $A$ can be covered by finitely many balls of radius $1$, at least one of which, say $B_1$, contains $x_n$ for infinitely many $n$, say for $n \in N_1 \subseteq \naturals$. Similarly, $A \intersect B_1$ may be covered by finitely many balls of radius $1/2$, and again there is a ball $B_2$ containing $x_n$ for infinitely many $n \in N_1$, say for $n \in N_2$. Continuing recursively we obtain a sequence of balls $B_i$ of radius $1/i$ and a decreasing sequence $(N_i)_{i\in\naturals}$ of infinite subsets of $\naturals$ such that $x_n \in B_i$ for $n \in N_i$.

    Next, choose a strictly increasing sequence $(n_i)_{i\in\naturals}$ of naturals numbers such that $n_i \in N_i$. Then $\rho(x_{n_i}, x_{n_j}) < 2/i$ for $i \leq j$, so $(x_{n_i})_{i\in\naturals}$ is a Cauchy sequence., and since $A$ is complete it has a limit in $A$.

    \item[\subcref{enum:sequentially-compact} $\implies$ \subcref{enum:complete-totally-bounded}]
    Assume that $A$ is sequentially compact. We first show that $A$ is complete, so let $(x_n)_{n\in\naturals}$ be a Cauchy sequence in $A$. This has a subsequence that converges to a point $x$ in $A$, so [lemma] implies that $(x_n)$ also converges to $x$.
    
    Now suppose that $A$ is not totally bounded, and let $\epsilon > 0$ be such that $A$ cannot be covered by finitely many $\epsilon$-balls. We construct a sequence $(x_n)_{n\in\naturals}$ in $A$ as follows: Choose any $x_1 \in A$, and given $x_1, \ldots, x_n$ choose $x_{n+1} \in A \setminus \bigunion_{i=1}^n B(x_i,\epsilon)$. Then $\rho(x_m,x_n) \geq \epsilon$ for all $m,n \in \naturals$ with $m \neq n$, so $(x_n)$ has no convergent subsequence.
    
    \item[\subcref{enum:complete-totally-bounded} \& \subcref{enum:sequentially-compact} $\implies$ \subcref{enum:compact}]
    Suppose that $A$ is complete, totally bounded and sequentially compact, and let $\calU$ be an open cover of $A$. It suffices to show that there some $\epsilon > 0$ such that any $\epsilon$-ball intersecting $A$ is contained in some $U \in \calU$, since $A$ can be covered by finitely many such balls.

    Assume towards a contradiction that for every $n \in \naturals$ there is a ball $B_n$ of radius $1/n$ intersecting $A$ such that $B_n$ is contained in no $U \in \calU$. Picking $x_n \in B_n$ for $n \in \naturals$, we may assume that the sequence $(x_n)_{n\in\naturals}$ converges to some $x \in A$ by passing to an appropriate subsequence. Then $x \in U$ for some $U \in \calU$, and since $U$ is open there is an $\epsilon > 0$ such that $B(x,\epsilon) \subseteq U$. Choosing $n \in \naturals$ large enough that $\rho(x_n,x) < \epsilon/2$ and $1/n < \epsilon/2$, we have $B_n \subseteq B(x,\epsilon) \subseteq U$, which is a contradiction.

    \item[\subcref{enum:compact} $\implies$ \subcref{enum:sequentially-compact}]
    We prove the contrapositive, so assume that $A$ is not sequentially compact, and let $(x_n)_{n\in\naturals}$ be a sequence in $A$ with no convergent subsequence. Every $x \in A$ is then contained in an open ball $B_x$ containing $x_n$ for only finitely many $n$. Thus $\{B_x\}_{x \in A}$ is an open cover of $A$ with no finite subcover, and $A$ is not compact.
\end{proofsec}
\end{proof}


% \chapter{Differentiation}

% \begin{definition}[Derivatives]
%     Let $I$ be an interval, and let $f \colon I \to \reals$. We say that $f$ is \emph{differentiable} at a point $a \in I$ if the limit
%     %
%     \begin{equation*}
%         \lim_{x \to a} \frac{f(x) - f(a)}{x-a}
%     \end{equation*}
%     %
%     exists. If so, the limit is called the \emph{derivative} of $f$ at $a$ and is denoted $f'(a)$ or $Df(a)$.
% \end{definition}

% \begin{lemma}
%     Let $f \colon I \to \reals$ and $a \in I$. Then the following are equivalent:
%     %
%     \begin{enumlem}
%         \item $f$ is differentiable at $a$.
        
%         \item There exists a function $\phi_a \colon I \to \reals$, continuous at $a$, such that
%         %
%         \begin{equation*}
%             f(x)
%                 = f(a) + \phi_a(x) (x-a)
%         \end{equation*}
%         %
%         for all $x \in I$.
        
%         \item There exists a number $A \in \reals$ and a function $\epsilon_a \colon U \to \reals$, where $U \subseteq \reals$ is an open neighbourhood of $0$, such that, for all $h \in \reals$ with $a + h \in I$,
%         %
%         \begin{equation*}
%             f(a+h)
%                 = f(a) + Ah + \epsilon_a(h),
%             \quad \text{and} \quad
%             \lim_{h \to 0} \frac{\epsilon_a(h)}{h} = 0. 
%         \end{equation*}
%     \end{enumlem}
%     %
%     In this case we have
% \end{lemma}


\chapter{Differentiation}

\section{Limits of functions} % [TODO move somewhere else?]

\newcommand{\calN}{\mathcal{N}}
\newcommand{\nhoods}[1]{\calN_{#1}}
\newcommand{\pnhoods}[1]{\calN'_{#1}}
\newcommand{\limitval}{\calL}
% [TODO: consider giving \nhoods an optional parameter for the topological space. E.g. \calN_space(point).]

Let $X$ be a topological space, and let $A \subseteq X$. A subset $N$ is a \emph{neighbourhood} of $A$ if there is an open set $U$ such that $A \subseteq U \subseteq N$. The set of neighbourhoods of $A$ is denoted $\nhoods{A}$ and is called the \emph{neighbourhood filter} of $A$ (since it is indeed a filter). If $A = \{x\}$ is a singleton we also write $\nhoods{x}$ for the neighbourhood filter of $x$. A \emph{punctured neighbourhood} of $x \in X$ is a set on the form $N \setminus \{x\}$, where $N \in \nhoods{x}$. The set of punctured neighbourhoods of $x$ is denoted $\pnhoods{x}$.

A point $a \in X$ is called an \emph{adherent point} of $A \subseteq X$ if every neighbourhood of $a$ intersects $A$, i.e. if $a \in \closure{A}$. Furthermore, $a$ is called a \emph{limit point} of $A$ if every \emph{punctured} neighbourhood of $a$ intersects $A$, or equivalently if $a$ is an adherent point of $A \setminus \{a\}$.

\begin{definition}[Limits of functions]
    \label{def:limit-of-function}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $L \subseteq A$ and $a \in X$ is an adherent point of $L$, then the triple $(f,L,a)$ is called a \emph{limit in $X$}. If $b \in Y$ is such that
    %
    \begin{equation*}
        \forall N \in \nhoods{b}\,
            \exists M \in \nhoods{a} \colon
            f(M \intersect L) \subseteq N,
    \end{equation*}
    %
    then we say that $b$ is a \emph{limit value} for the limit $(f,L,a)$. The set of such limit values is denoted $\limitval(f,L,a)$.

    If $\limitval(f,L,a)$ contains a single element $b$, then we also write
    %
    \begin{equation*}
        b
            = \lim_{a, L} f
            = \lim_{\substack{x \to a \\ x \in L}} f(x).
    \end{equation*}

    % be a map between topological spaces. If $a \in X$ is an adherent point of a subset $A \subseteq X$ and $b \in Y$, then we say that $f$ \emph{converges to $b$ in $Y$ at $a$ in $A$}, or that $f(x)$ \emph{converges to $b$ in $Y$ as $x$ converges to $a$ in $A$}, if

    % In this case, $b$ is called a \emph{limit of $f$ at $a$ in $A$} or a \emph{limit of $f(x)$ as $x$ converges to $a$ in $A$}.

    % If there is a unique such limit $b$, then we write
    % %
    % \begin{equation*}
    %     b
    %         = \lim_{a \in A} f
    %         = \lim_{\substack{x \to a \\ a \in A}} f(x).
    % \end{equation*}
\end{definition}

\begin{remark}
    \label{rem:limit-neighbourhoods}
    In \cref{def:limit-of-function} we did not specify whether the neighbourhoods of $a$ were neighbourhoods in $A$ or in $X$. In fact, this does not matter: The map $M \mapsto M \intersect A$ gives a one-to-one correspondence between neighbourhoods of $a$ in $X$ and $A$, and since $L$ is a subset of $A$ we have
    %
    \begin{equation*}
        f((M \intersect A) \intersect L) = f(M \intersect L).
    \end{equation*}
    %
    Hence we may use either neighbourhoods in $X$ or in $A$ when taking limits.
\end{remark}

\begin{proposition}[Uniqueness of limits]
    \label{prop:Hausdorff-limits-unique}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $Y$ is Hausdorff, then any limit $(f,L,a)$ in $X$ has at most one limit point.
\end{proposition}

\begin{proof}
    Let $b \in Y$ be a limit value for $(f,L,a)$, let $b' \neq b$, and let $N,N' \subseteq Y$ be disjoint neighbourhoods of $b$ and $b'$, respectively. Since $b$ is a limit value, there is a neighbourhood $M \in \nhoods{a}$ such that $f(M \intersect L) \subseteq N$. If $M'$ is any neighbourhood of $a$ then $M \intersect M'$ is also a neighbourhood of $a$, so it intersects $L$.\footnote{Notice that we here make crucial use of the requirement that $a$ be an adherent point of $L$.} But since
    %
    \begin{equation*}
        f((M \intersect M') \intersect L)
            \subseteq f(M \intersect L)
            \subseteq N,
    \end{equation*}
    %
    the set $f(M' \intersect L)$ intersects $N$, so it does not lie in $N'$. Hence $b'$ is not a limit value for $(f,L,a)$.
\end{proof}

% Overwrite \ball from topologycommands.sty
\renewcommand{\ball}[3][]{B_{#1}(#2,#3)}
\newcommand{\cball}[3][]{\overline{B}_{#1}(#2,#3)}
\newcommand{\pball}[3][]{B'_{#1}(#2,#3)}

\begin{example}
    \Cref{def:limit-of-function} provides a very general notion of limit, of which the familiar limiting processes are special cases:
    %
    \begin{enumexample}
        \item \label{enum:deleted-limit} Recall the standard definition of a limit of a function between topological spaces: If $a$ is a limit point of $A$, then we usually say that $b \in Y$ is a limit of $f(x)$ as $x \to a$ if
        %
        \begin{equation*}
            \forall N \in \nhoods{b}\,
                \exists M' \in \pnhoods{a} \colon
                f(M') \subseteq N.
        \end{equation*}
        %
        We recover this notion by considering the triple $(f,A \setminus \{a\},a)$: For notice that if $M$ is a neighbourhood of $a$, then
        %
        \begin{equation*}
            M \intersect (A \setminus \{a\})
                = (M \setminus \{a\}) \intersect A,
        \end{equation*}
        %
        and every punctured neighbourhood of $a$ is on the form $M \setminus \{a\}$. Here we recall from \cref{rem:limit-neighbourhoods} that it is immaterial whether we ue neighbourhoods of $a$ in $A$ or in $X$, so the intersection with $A$ makes no difference.

        Notice that it is crucial that $a$ is a limit point of $A$ and not just an adherent point, since $a$ is a limit point of $A$ if and only if $a$ is an adherent point of $A \setminus \{a\}$, which we require in our definition of limits.

        Below we shall take this notion of limit as standard, so if $Y$ is Hausdorff and hence limits are unique by \cref{prop:Hausdorff-limits-unique}, we use the shorthand notation
        %
        \begin{equation*}
            b
                = \lim_a f
                = \lim_{x \to a} f(x).
        \end{equation*}
        %
        If the set $A$ is not clear from context, we may disambiguate by writing \enquote{for $x \in A \setminus \{a\}$},  or something to that effect.

        \item Some authors distinguish between \emph{deleted} and \emph{non-deleted} limits. The notion of limits in \subcref{enum:deleted-limit} is that of deleted limits, since we only reture that $f(M') \subseteq N$ for a \emph{punctured} neighbourhood $M'$ of $a$. By contrast, in a \emph{non-deleted} limit we require that $f(M) \subseteq N$ for an ordinary neighbourhood $M$ of $a$. We may recover the notion of non-deleted limits by considering triples $(f,A,a)$.
        
        Non-deleted limits are relatively rare in the English-language literature (one notable exception is \textcite{taoanalysisI,taoanalysisII}, who allows for both types), but it seems to be the prevailing notion in the French-language literature.
        
        \item Consider next a function $f \colon A \to Y$, where $A \subseteq \reals$ contains an interval $(a,b)$. In this case we may consider the one-sided limits of $f(x)$ as $x$ approaches either $a$ from above or $b$ from below. These may be described in the above formalism by the triples $(f,(a,b),a)$ and $(f,(a,b),b)$, respectively (indeed, we may choose any subinterval of $(a,b)$ with $a$ or $b$ as a limit point, respectively, and the set of limit values is clearly independent of such a choice). If they exist, we denote the corresponding limit values by
        %
        \begin{equation*}
            \lim_{x \downarrow a} f(x)
            \quad \text{and} \quad
            \lim_{x \uparrow b} f(x),
        \end{equation*}
        %
        respectively.
    \end{enumexample}
\end{example}


\begin{lemma}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $L_1 \subseteq L_2 \subseteq A$ and $a \in X$ is an adherent point of both $L_1$ and $L_2$, then
    %
    \begin{equation*}
        \limitval(f,L_2,a)
            \subseteq \limitval(f,L_1,a).
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $b \in \limitval(f,L_2,a)$ and let $N \in \nhoods{b}$. Then there is an $M \in \nhoods{a}$ such that
    %
    \begin{equation*}
        f(M \intersect L_1)
            \subseteq f(M \intersect L_2)
            \subseteq N,
    \end{equation*}
    %
    showing that $b \in \limitval(f,L_1,a)$ as desired.
\end{proof}


\begin{proposition}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $a \in A$ is a limit point of $A$, then the following are equivalent:
    %
    \begin{enumprop}
        \item $f$ is continuous at $a$.
        \item $f(a) \in \limitval(f, A, a)$.
        \item $f(a) \in \limitval(f, A \setminus \{a\}, a)$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    Assume that $f$ is continuous at $a$, and let $N \in \nhoods{f(a)}$. By continuity there is an\footnote{Recall that we may use either neighbourhoods of $a$ in $A$ or in $X$, cf. \cref{rem:limit-neighbourhoods}.} $M \in \nhoods{a}$ such that $f(M) \subseteq N$, implying that $f(M \intersect A) \subseteq N$. Next, (ii) implies (iii) by [TODO ref lemma].
    
    Finally, if $f(a) \in \limitval(f, A \setminus \{a\}, a)$ then given $N \in \nhoods{f(a)}$ there is a neighbourhood $M \in \nhoods{a}$ in $A$ such that $f(M \setminus \{a\}) = f(M \intersect A \setminus \{a\}) \subseteq N$. Since $N$ contains $f(a)$ we have $f(M) \subseteq N$ so $f$ is continuous at $a$.
\end{proof}


\section{Differentiability}

% \begin{definition}[Differentiability on $\reals$]
%     Let $A \subseteq \reals$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals$. If the limit
%     %
%     \begin{equation*}
%         \lim_{x \to a} \frac{f(x) - f(a)}{x - a}
%     \end{equation*}
%     %
%     exists and equals $L \in \reals$, then we say that $f$ is \emph{differentiable} at $a$. The number $L$ is called the \emph{derivative} of $f$ at $a$ and is denoted $f'(a)$.
% \end{definition}
% %
% % \begin{remark}
% %     \label{rem:differentiability-reformulation}
% %     Recall that our standard notion of limit is that of \emph{deleted limits}, cf. \cref{enum:deleted-limit}. Hence the above is shorthand for the limit
% %     %
% %     \begin{equation*}
% %         \lim_{\substack{x \to a \\ a \in A \setminus \{a\}}} \frac{f(x) - f(a)}{x - a}.
% %     \end{equation*}
% %     %
% %     That is, we do not allow $x$ to equal $a$ (as is of course necessary since we divide by $x-a$).
    
% %     Notice also that we may reformulate the definition of differentiability of $f$ at $a$ by instead requiring that the limit
% %     %
% %     \begin{equation*}
% %         \lim_{\substack{h \to 0 \\ h \in (A-a) \setminus \{0\}}} \frac{f(a+h) - f(a)}{h}
% %     \end{equation*}
% %     %
% %     exist.
% % \end{remark}

% \begin{lemma}[Hadamard's lemma on $\reals$]
%     \label{lem:Hadamard-1D}
%     Let $A \subseteq \reals$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals$. The following are equivalent:
%     %
%     \begin{enumlem}
%         \item $f$ is differentiable at $a$.

%         \item There exists an $L(a) \in \reals$ and a function $\epsilon_a \colon A-a \to \reals$ such that
%         %
%         \begin{equation*}
%             \lim_{h \to 0} \frac{\epsilon_a(h)}{h} = 0,
%             \quad \text{and} \quad
%             f(a+h) - f(a)
%                 = L(a)h + \epsilon_a(h)
%         \end{equation*}
%         %
%         for all $h \in A-a$.

%         \item There exists a function $\phi = \phi_a \colon A \to \reals$, continuous at $a$, such that
%         %
%         \begin{equation*}
%             f(x) - f(a)
%                 = \phi_a(x) (x - a)
%         \end{equation*}
%         %
%         for all $x \in A$.
%     \end{enumlem}
%     %
%     If any of these conditions are satisfied, then
%     %
%     \begin{equation*}
%         f'(a) = \phi_a(a) = L(a),
%         \quad \text{and} \quad
%         \phi_a(x)
%             = f'(a) + \frac{\epsilon_a(x-a)}{x-a}
%     \end{equation*}
%     %
%     for all $x \in A \setminus \{a\}$.
% \end{lemma}
% %
% We will call the function $\phi = \phi_a$ an \emph{Hadamard function} for $f$ at $a$.

% \begin{proof}
% \begin{proofsec}
%     \item[(i) $\implies$ (ii)]
%     Let $L(a) = f'(a)$ and put
%     %
%     \begin{equation*}
%         \epsilon_a(h)
%             = f(a+h) - f(a) - f'(a)h
%     \end{equation*}
%     %
%     for $h \in A - a$. Then we have
%     %
%     \begin{equation*}
%         \frac{\epsilon_a(h)}{h}
%             = \frac{f(a+h) - f(a) - f'(a)h}{h}
%             = \frac{f(a+h) - f(a)}{h} - f'(a)
%             \xrightarrow[h \to 0]{} 0
%     \end{equation*}
%     %
%     as required.

%     \item[(ii) $\implies$ (iii)]
%     Define
%     %
%     \begin{equation*}
%         \phi_a(x) =
%         \begin{cases}
%             L(a) + \frac{1}{x-a} \epsilon_a(x-a),
%                 & x \in A \setminus \{a\}, \\
%             L(a),
%                 & x = a.
%         \end{cases}
%     \end{equation*}
%     %
%     Then $\phi_a$ is continuous at $a$, and furthermore
%     %
%     \begin{equation*}
%         \phi_a(x)(x-a)
%             = L(a)(x-a) + \epsilon_a(x-a)
%             = f(x) - f(a)
%     \end{equation*}
%     %
%     for $x \in A \setminus \{a\}$ as required.

%     \item[(iii) $\implies$ (i)]
%     Notice that
%     %
%     \begin{equation*}
%         \frac{f(x) - f(a)}{x-a}
%             = \phi_a(x-a)
%             \xrightarrow[x \to a]{} \phi_a(a),
%     \end{equation*}
%     %
%     so $f$ is differentiable at $a$.
% \end{proofsec}
% \end{proof}




\begin{definition}[Differentiability]
    \label{def:differentiability}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$. If there exists a linear map $L \in \calL(\reals^d, \reals^m)$ such that
    %
    \begin{equation}
        \label{eq:differentiability-definition}
        \lim_{x \to a} \frac{\norm{f(x) - f(a) - L(x-a)}}{\norm{x - a}}
            = 0,
    \end{equation}
    %
    then $f$ is said to be \emph{differentiable} at $a$. The map $L$ is called the \emph{derivative} of $f$ at $a$ and is denoted $f'(a)$. If $E \subseteq A$ and $f$ is differentiable at $a$ for all $a \in E$, then we say that $f$ is \emph{differentiable on $E$}. If $f$ is differentiable on $A$, then we simply say that $f$ is \emph{differentiable}.
    
    If $f$ is differentiable, then the map $f' \colon A to \calL(\reals^d,\reals^m)$ is called the \emph{derivative} of $f$.
\end{definition}
%
Note that for $f$ to be differentiable on $E$, every point in $E$ must be a limit point of $A$. In particular, for $f$ to be differentiable every point in $A$ must be a limit point of $A$. This is for instance the case when $A$ is open or when $A$ is an interval.


\begin{remarkbreak}[Differentiability on $\reals$]
    In the case where $d = 1$, the quotient in \cref{eq:differentiability-definition} goes to zero if and only if
    %
    \begin{equation*}
        \frac{f(x) - f(a)}{x - a} - L
            = \frac{f(x) - f(a) - L(x-a)}{x - a}
    \end{equation*}
    %
    goes to zero, i.e. the difference quotient converges to $L$. Furthermore, if also $m = 1$ then $L$ is multiplication by a scalar, so in this case \cref{def:differentiability} is equivalent to the usual definition of the derivative.
\end{remarkbreak}

[TODO uniqueness]


\begin{lemma}
    \label{lem:differentiability-coordinate-functions}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f = (f_1, \ldots, f_m) \colon A \to \reals^m$. Then $f$ is differentiable at $a$ if and only if each $f_i$ is differentiable at $a$, and $f'(a) = (f_1'(a), \ldots, f_m'(a))$.
\end{lemma}

\begin{proof}
    If $L = (L_1, \ldots, L_m) \in \calL(\reals^d,\reals^m)$, then notice that
    %
    \begin{equation*}
        f(x) - f(a) - L(x-a) =
        \begin{pmatrix}
            f_1(x) - f_1(a) - L_1(x-a) \\
            \vdots \\
            f_m(x) - f_m(a) - L_m(x-a)
        \end{pmatrix}.
    \end{equation*}
    %
    The claim then follows since the quotient in \cref{eq:differentiability-definition} converges to zero if and only if each quotient
    %
    \begin{equation*}
        \frac{\abs{f_i(x) - f_i(a) - L_i(x-a)}}{\norm{x - a}}
    \end{equation*}
    %
    converges to zero.
\end{proof}

\newcommand{\trans}{^{\top}}

\begin{lemma}[Hadamard's lemma]
    \label{lem:Hadamard-multiD}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$. The following are equivalent:
    %
    \begin{enumlem}
        \item $f$ is differentiable at $a$.

        \item There exists a linear map $L(a) \in \calL(\reals^d,\reals^m)$ and a function $\epsilon_a \colon A-a \to \reals^m$ such that
        %
        \begin{equation*}
            \lim_{h \to 0} \frac{\norm{\epsilon_a(h)}}{\norm{h}} = 0,
            \quad \text{and} \quad
            f(a+h) - f(a)
                = L(a)h + \epsilon_a(h)
        \end{equation*}
        %
        for all $h \in A-a$.

        \item There exists a function $\phi = \phi_a \colon A \to \calL(\reals^d,\reals^m)$, continuous at $a$, such that
        %
        \begin{equation*}
            f(x) - f(a)
                = \phi_a(x) (x - a)
        \end{equation*}
        %
        for all $x \in A$.
    \end{enumlem}
    %
    If any of these conditions are satisfied, then
    %
    \begin{equation*}
        f'(a) = \phi_a(a) = L(a),
        \quad \text{and} \quad
        \phi_a(x)
            = f'(a) + \frac{1}{\norm{x-a}^2} \epsilon_a(x-a)(x-a)\trans,
    \end{equation*}
    %
    for all $x \in A \setminus \{a\}$.
\end{lemma}
%
We will call the function $\phi = \phi_a$ an \emph{Hadamard function} for $f$ at $a$.

\begin{proof}
    \begin{proofsec}
        \item[(i) $\implies$ (ii)]
        Let $L(a) = f'(a)$ and put
        %
        \begin{equation*}
            \epsilon_a(h)
                = f(a+h) - f(a) - f'(a)h
        \end{equation*}
        %
        for $h \in A - a$. Then we have
        %
        \begin{equation*}
            \frac{\norm{\epsilon_a(h)}}{\norm{h}}
                = \frac{\norm{f(a+h) - f(a) - f'(a)h}}{\norm{h}}
                \xrightarrow[h \to 0]{} 0
        \end{equation*}
        %
        as required.
    
        \item[(ii) $\implies$ (iii)]
        Define
        %
        \begin{equation*}
            \phi_a(x) =
            \begin{cases}
                L(a) + \frac{1}{\norm{x-a}^2} \epsilon_a(x-a)(x-a)\trans,
                    & x \in A \setminus \{a\}, \\
                L(a),
                    & x = a.
            \end{cases}
        \end{equation*}
        %
        We first of all have
        %
        \begin{align*}
            \phi_a(x)(x-a)
                &= L(a)(x-a) + \epsilon_a(x-a) \frac{(x-a)\trans (x-a)}{\norm{x-a}^2} \\
                &= L(a)(x-a) + \epsilon_a(x-a) \\
                &= f(x) - f(a)
        \end{align*}
        %
        as required. Next notice that, since $h$ and $\epsilon_a(h)$ are (column) vectors, we have\footnote{More generally, let $v = (v_1, \ldots, v_d) \in \reals^d$ and $w = (w_1, \ldots, w_m) \in \reals^m$. Then $(wv\trans)_{ij} = w_i v_j$, so
        %
        \begin{equation*}
            \norm{wv\trans}^2
                = \sum_{i,j} \abs{w_i v_j}^2
                = \sum_{i=1}^m \abs{w_i}^2 \sum_{j=1}^d \abs{v_i}^2
                = \norm{w}^2 \norm{v}^2.
        \end{equation*}}
        %
        \begin{equation*}
            \frac{\norm{\epsilon_a(h) h\trans}}{\norm{h}^2}
                = \frac{\norm{\epsilon_a(h)} \, \norm{h}}{\norm{h}^2}
                = \frac{\norm{\epsilon_a(h)}}{\norm{h}}
                \xrightarrow[h \to 0]{} 0,
        \end{equation*}
        %
        so $\phi_a$ is continuous at $a$.
    
        \item[(iii) $\implies$ (i)]
        Notice that
        %
        \begin{align*}
            \frac{\norm{f(x) - f(a) - \phi_a(a)(x-a)}}{\norm{x-a}}
                &= \frac{\norm{\phi_a(x)(x-a) - \phi_a(a)(x-a)}}{\norm{x-a}} \\
                &\leq \frac{\norm{\phi_a(x) - \phi_a(a)} \, \norm{x-a}}{\norm{x-a}} \\
                &= \norm{\phi_a(x) - \phi_a(a)}
                \xrightarrow[x \to a]{} 0,
        \end{align*}
        %
        by continuity of $\phi_a$ at $a$, and continuity of the operator norm, so $f$ is differentiable at $a$.
    \end{proofsec}
\end{proof}


\begin{proposition}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$. If $f$ is differentiable at $a$, then $f$ is continuous at $a$.
\end{proposition}

\begin{proof}
    Let $\phi_a$ be an Hadamard function for $f$ at $a$, such that
    %
    \begin{equation}
        f(x)
            = f(a) + \phi_a(x)(x-a)
    \end{equation}
    %
    for $x \in A$. Since $\phi_a$ is continuous at $a$, so is $f$.
\end{proof}


\begin{theorem}[The chain rule]
    Let $A \subseteq \reals^d$ and $B \subseteq \reals^m$, and let $f \colon A \to \reals^m$ and $g \colon B \to \reals^p$ with $f(A) \subseteq B$. Assume that $a \in A$ is a limit point of $A$, that $f(a) \in B$ is a limit point of $B$, that $f$ is differentiable at $a$, and that $g$ is differentiable at $f(a)$. Then $g \circ f$ is differentiable at $a$ with
    %
    \begin{equation*}
        (g \circ f)'(a)
            = g'(f(a)) f'(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $\phi$ be an Hadamard function for $f$ at $a$, and let $\psi$ be an Hadamard function for $g$ at $f(a)$. Hence
    %
    \begin{equation*}
        f(x) - f(a)
            = \phi(x)(x - a)
        \quad \text{and} \quad
        g(y) - g(f(a))
            = \psi(y)(y - f(a))
    \end{equation*}
    %
    for all $x \in A$ and $y \in B$. Letting $y = f(x)$ we thus have
    %
    \begin{equation*}
        g(f(x)) - g(f(a))
            = \psi(f(x))(f(x) - f(a))
            = \psi(f(x))\phi(x)(x - a).
    \end{equation*}
    %
    Notice that the map $x \mapsto \psi(f(x))\phi(x)$ is continuous at $a$, so it is an Hadamard function for $g \circ f$ at $a$. Thus \cref{lem:Hadamard-multiD} implies that $g \circ f$ is differentiable at $a$ with
    %
    \begin{equation*}
        (g \circ f)'(a)
            = \psi(f(a)) \phi(a)
            = g'(f(a)) f'(a),
    \end{equation*}
    %
    as claimed.
\end{proof}


\section{Extrema and mean value theorems}

\begin{proposition}
    \label{prop:local-extrema-stationary}
    Let $f \colon (a,b) \to \reals$, and let $c \in (a,b)$. Assume that $f$ is differentiable at $c$ and attains a local extremum at $c$. Then $f'(c) = 0$.
\end{proposition}

\begin{proof}
    Assume for definiteness that $f$ has a local maximum at $c$, and choose $\delta > 0$ such that $f(x) \leq f(c)$ for $x \in (c - \delta, c + \delta)$. For $x \in (c - \delta, c)$ we have
    %
    \begin{equation*}
        \frac{f(x) - f(c)}{x - c} \geq 0,
    \end{equation*}
    %
    and letting $x \uparrow c$ we find that $f'(c) \geq 0$. By considering $x \in (c, c + \delta)$ we similarly find that $f'(c) \leq 0$ as desired.
\end{proof}


\begin{lemma}[Rolle's theorem]
    \label{lem:Rolle}
    Let $f \colon [a,b] \to \reals$ be a continuous function that is differentiable on $(a,b)$. If $f(a) = f(b)$, then there is a $c \in (a,b)$ such that $f'(c) = 0$.
\end{lemma}

\begin{proof}
    If $f$ is constant, then this is obvious. If $f$ is not constant, then since it is continuous it has a local extremum at some $c \in (a,b)$. By \cref{prop:local-extrema-stationary} we thus have $f'(c) = 0$ as desired.
\end{proof}


\begin{theorem}[The generalised mean value theorem]
    \label{thm:generalised-MVT}
    Let $f,g \colon [a,b] \to \reals$ be continuous functions that are differentiable on $(a,b)$. Then there exists a point $c \in (a,b)$ such that
    %
    \begin{equation*}
        \bigl( f(b) - f(a) \bigr) g'(c)
            = \bigl( g(b) - g(a) \bigr) f'(c).
    \end{equation*}
\end{theorem}

\begin{proof}
    Define $h \colon [a,b] \to \reals$ by $h(x) = (f(b) - f(a)) g(x) - (g(b) - g(a)) f(x)$, and notice that $h$ is continuous on $[a,b]$, differentiable on $(a,b)$, and that
    %
    \begin{equation*}
        h(a)
            = f(b)g(a) - g(b)f(a)
            = h(b).
    \end{equation*}
    %
    \Cref{lem:Rolle} thus implies that $h'(c) = 0$ for some $c \in (a,b)$. This proves the claim.
\end{proof}


\begin{corollary}[The mean value theorem]
    Let $f \colon [a,b] \to \reals$ be a continuous function that is differentiable on $(a,b)$. Then there is a $c \in (a,b)$ such that
    %
    \begin{equation*}
        f(b) - f(a)
            = f'(c) (b-a).
    \end{equation*}
\end{corollary}

\begin{proof}
    Let $g(x) = x$ in \cref{thm:generalised-MVT}.
\end{proof}

[TODO monotonicity]


\section{Directional and partial derivatives}

\begin{definition}[Directional and partial derivatives]
    Let $A \subseteq \reals^d$, and let $f \colon A \to \reals^m$. If $a$ is an interior point of $A$ and $v \in \reals^d$, then we say that $f$ has a \emph{directional derivative} at $a$ in the direction of $v$ if the limit
    %
    \begin{equation*}
        \lim_{t \to 0} \frac{f(a + tv) - f(a)}{t}
    \end{equation*}
    %
    exists. The value of this limit is denoted $D_v f(a)$ and is called the \emph{directional derivative} of $f$ at $a$ in the direction of $v$.

    If $v = e_j$, then we write $D_{e_j} f(a) = D_j f(a)$ and call $D_j f(a)$ the \emph{$j$-th partial derivative} of $f$ at $a$. If $D_j f(a)$ exists for all $j$, then $f$ is said to be \emph{partially differentiable} at $a$. If $E \subseteq A$ and $f$ is partially differentiable at $a$ for all $a \in E$, then we say that $f$ is \emph{partially differentiable on $E$}. If $f$ is partially differentiable on $A$, then we simply say that $f$ is \emph{partially differentiable}.

    If $f$ is partially differentiable on a set $E \subseteq A$, then the functions $D_i f \colon E \to \reals^m$ are called the \emph{partial derivatives} of $f$ (on $E$).
\end{definition}

\begin{proposition}
    If $f$ is differentiable at an inner point $a$
    \begin{enumprop}
        \item then the directional derivative $D_v f(a)$ exists for all $v \in \reals^d$, and
        %
        \begin{equation*}
            D_v f(a)
                = f'(a) v.
        \end{equation*}

        \item \label{enumprop:derivative-linear-combination-of-partials} $f$ is partially differentiable at $a$, and
        %
        \begin{equation*}
            f'(a)v
                = \sum_{j=1}^d v_j D_j f(a)
        \end{equation*}
        %
        for all $v = (v_1, \ldots v_d) \in \reals^d$. In particular, writing $f = (f_1, \ldots, f_m)$ the standard matrix representation $J_f(a)$ of $f'(a)$ is given by $J_f(a) = (D_j f_i(a))_{ij}$.
    \end{enumprop}
\end{proposition}

\begin{proof}
\begin{proofsec}
    \item[Proof of (i)]
    Let $U$ be an open neighbourhood of $a$ contained in $A$, and define the function $g \colon U \to \reals$ by $g(t) = f(a + tv)$. Then since the function $t \mapsto a+tv$ is differentiable at $0$ with derivative $v$, the chain rule implies that
    %
    \begin{equation*}
        \lim_{t \to 0} \frac{f(a + tv) - f(a)}{t}
            = g'(0)
            = f'(a)v.
    \end{equation*}

    \item[Proof of (ii)]
    Partial differentiability follows from (i). Writing $v = \sum_{j=1}^d v_j e_j$, it follows from (i) that
    %
    \begin{equation*}
        f'(a)v
            = \sum_{j=1}^d v_j f'(a)e_j
            = \sum_{j=1}^d v_j D_{e_j}(a)
            = \sum_{j=1}^d v_j D_j(a).
    \end{equation*}
\end{proofsec}
\end{proof}


\begin{theorem}[Criterion for differentiability]
    \label{thm:continuous-partials-implies-differentiable}
    Let $A \subseteq \reals^d$, let $a$ be an inner point of $A$, and $f \colon A \to \reals^m$. If $f$ is partially differentiable in a neighbourhood of $a$ and each partial derivative is continuous at $a$, then $f$ is differentiable at $a$.
\end{theorem}

\begin{proof}
    We may assume that $m = 1$ by \cref{lem:differentiability-coordinate-functions}. Let $U$ be an open neighbourhood of $a$ on which $f$ is partially differentiable. Let $h = (h_1, \ldots, h_d) \in \reals^d$, and for $k = 1, \ldots, d$ let $h^{(k)} = \sum_{j=1}^k h_j e_j$. Notice that $h^{(d)} = h$. Choose $h$ small enough such that $h^{(k)} \in U$ for all $k$. Next let $I_k \subseteq \reals$ be an open interval containing $0$ and $h_k$ such that the map
    %
    \begin{align*}
        g_k \colon I_k &\to \reals, \\
        t &\mapsto f(a + h^{(k-1)} + t e_k),
    \end{align*}
    %
    is differentiable. Applying the mean value theorem on $g_k$ yields an $s_k$ in the interval between $0$ and $h_k$ such that
    %
    \begin{equation*}
        g_k(h_k) - g_k(0)
            = g_k'(s_k) h_k
            = D_k f(a + h^{(k-1)} + s_k e_k) h_k.
    \end{equation*}
    %
    Next define a map $\phi_a \colon U \to \calL(\reals^d,\reals)$ by
    %
    \begin{equation*}
        \phi_a(a+h)x
            = \sum_{k=1}^d D_k f(a + h^{(k-1)} + s_k e_k) x_k
    \end{equation*}
    %
    for $h \in U - a$ and $x = (x_1, \ldots, x_d) \in \reals^d$. This is clearly well-defined, and since the partial derivatives of $f$ are continuous at $a$, so is $\phi_a$ (notice that $s_k$ also goes to zero when $h$ goes to zero).
    
    Finally notice that
    %
    \begin{align*}
        f(a+h) - f(a)
            &= \sum_{k=1}^d \bigl( f(a + h^{(k)}) - f(a + h^{(k-1)}) \bigr) \\
            &= \sum_{k=1}^d \bigl( g_k(h_k) - g_k(0)) \bigr) \\
            &= \sum_{k=1}^d D_k f(a + h^{(k-1)} + s_k e_k) h_k \\
            &= \phi_a(a+h) h.
    \end{align*}
    %
    But then $\phi_a$ is an Hadamard function for $f$ at $a$, so $f$ is differentiable at $a$ by \cref{lem:Hadamard-multiD}.
\end{proof}


\section{Continuous differentiability}

Since $\calL(\reals^d,\reals^m)$ is a finite-dimensional vector space, it has a unique vector space topology [TODO ref to my notes on topological vector spaces]. Hence it makes sense to talk about the continuity of maps into or out of $\calL(\reals^d,\reals^m)$. In particular, if the derivative $f'$ of $f$ is continuous, then we say that $f$ is \emph{continuously differentiable}. The subset of $C(A,\reals^m)$ containing the continuously differentiable functions is denoted $C^1(A,\reals^m)$.

\begin{corollary}
    Let $U \subseteq \reals^d$ be open and let $f \colon U \to \reals^m$. Then $f$ is continuously differentiable if and only if $f$ is partially differentiable and all of its partial derivatives are continuous.
\end{corollary}

\begin{proof}
    If $f$ is partially differentiable, then $D_j f$ equals the function $a \mapsto f'(a) e_j$ by \cref{enumprop:derivative-linear-combination-of-partials}. If $f$ is differentiable then it is partially differentiable, and if $f'$ is continuous then so are the $D_j f$.

    Conversely, if $f$ is partially differentiable with continuous partial derivatives, then \cref{thm:continuous-partials-implies-differentiable} implies that $f$ is differentiable. Continuity of $f'$ now follows since its coordinate functions are continuous.
\end{proof}


\section{Higher-order derivatives}

\begin{theorem}[Clairaut's Theorem]
    Let $A \subseteq \reals^d$ be open, let $a$ be an inner point of $U$, and let $f \colon A \to \reals^m$. For $i,j \in \{1, \ldots, d\}$, if $D_j D_i f$ and $D_i D_j f$ exist in a neighbourhood of $a$ and are continuous at $a$, then
    %
    \begin{equation*}
        D_j D_i f(a)
            = D_i D_j f(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    By permuting indices, we may assume that $i = 1$ and $j = 2$, and indeed that $d = 2$. Furthermore, by \cref{lem:differentiability-coordinate-functions} it suffices to prove the theorem for $m = 1$.

    Let $U$ be an open convex neighbourhood of $a$ on which $D_j D_i f$ and $D_i D_j f$ exist, and let $x \in U$. Writing $a = (a_1,a_2)$ and $x = (x_1,x_2)$, define
    %
    \begin{equation*}
        r(x)
            = \frac{f(x) - f(a_1,x_2) - f(x_1,a_2) + f(a)}{(x_1 - a_1)(x_2 - a_2)}
    \end{equation*}
    %
    for $x \neq a$. Let $I$ be an open interval containing $a_1$ and $x_1$, and define $g \colon I \to \reals$ by $g(t) = f(t,x_2) - f(t,a_2)$. Notice that $g$ is well-defined by convexity of $U$. Then
    %
    \begin{equation*}
        r(x)
            = \frac{g(x_1) - g(a_1)}{(x_1 - a_1)(x_2 - a_2)}.
    \end{equation*}
    %
    Since $g$ is differentiable on $I$, the mean value theorem yields an $s_1 \in \reals$ between $a_1$ and $x_1$ such that
    %
    \begin{equation*}
        r_(x)
            = \frac{g'(s_1)}{x_2 - a_2}
            = \frac{D_1 f(s_1,x_2) - D_1 f(s_1,a_2)}{x_2 - a_2}.
    \end{equation*}
    %
    Similarly, let $J$ be an open interval containing $a_2$ and $x_2$, and define $h \colon J \to \reals$ by $h(t) = D_1 f(s_1,t)$. Again the mean value theorem yields an $s_2 \in \reals$ between $x_2$ and $a_2$ such that
    %
    \begin{equation*}
        r(x)
            = h'(s_2)
            = D_2 D_1 f(s_1, s_2).
    \end{equation*}
    %
    Let $s = (s_1,s_2)$. Since $\norm{s - a} \leq \norm{x - a}$, when $x \to a$ we also have $s \to a$. Continuity of $D_2 D_1 f$ at $a$ thus implies that
    %
    \begin{equation*}
        \lim_{x \to a} r(x)
            = \lim_{s \to a} D_2 D_1 f(s)
            = D_2 D_1 f(a).
    \end{equation*}
    %
    Finally notice that the above argument is symmetric in the indices $1$ and $2$, so we similarly find that $\lim_{x \to a} r(x) = D_1 D_2 f(a)$.
\end{proof}


\chapter{Integration}

\section{Functions of bounded variation}

\newcommand{\boundedvar}[1]{\mathit{BV}[#1]}
\newcommand{\integrable}[2][]{\calR_{#1}[#2]}

A \emph{partition} of an interval $[a,b]$ is a collection $P = \{x_0, \ldots, x_n \}$ of real numbers such that
%
\begin{equation*}
    a = x_0 < \cdots < x_n = b.
\end{equation*}
%
In turn, a \emph{tagged partition} of $[a,b]$ is a pair $(P,T)$ where $P$ is a partition of $[a,b]$ and $T = \{t_1, \ldots, t_n\}$ is a multiset of numbers such that $t_i \in [x_{i-1}, x_i]$ for all $i = 1, \ldots, n$. Let $\calP'[a,b]$ denote the set of tagged partitions of $[a,b]$. We define a direction on $\calP'[a,b]$ by $(P,T) \preceq (P',T')$ if $P \subseteq P'$. Notice that $T$ and $T'$ do not appear in this definition. This also induces a direction on the set $\calP[a,b]$ of all (non-tagged) partitions of $[a,b]$.

Given a partition $P = \{x_0, \ldots, x_n \}$ of $[a,b]$ and a function $f \colon [a,b] \to \reals$ we write $\Delta f_i = f(x_i) - f(x_{i-1})$ for $i = 1, \ldots, n$. We furthermore write $\Delta x_i = x_i - x_{i-1}$. Furthermore, define
%
\begin{equation*}
    \norm{P}
        = \max_{1 \leq i \leq n} \Delta x_i
    \quad \text{and} \quad
    \Sigma_f(P)
        = \sum_{i=1}^n \abs{\Delta f_i}.
\end{equation*}
%
The number $\norm{P}$ is called the \emph{norm} of $P$. Notice that the map $P \mapsto \norm{P}$ is decreasing, while the map $P \mapsto \Sigma_f(P)$ is increasing.

\begin{definition}[Total variation]
    Consider a function $f \colon [a,b] \to \reals$. The \emph{total variation} of $f$ on $[a,b]$ is the number
    %
    \begin{equation*}
        V_f(a,b)
            = \sup_{P \in \calP[a,b]} \Sigma_f(P).
    \end{equation*}
    %
    If $V_f(a,b) < \infty$, then we say that $f$ is of \emph{bounded variation} on $[a,b]$. The set of all functions that are of bounded variation on $[a,b]$ is denoted $\boundedvar{a,b}$.
\end{definition}
%
If $f$ is of bounded variation on $[a,b]$, then it is clear that $f$ is also of bounded variation on any subinterval of $[a,b]$. If $c \in (a,b)$ it is also easy to show that
%
\begin{equation}
    \label{eq:total-variation-additive}
    V_f(a,b)
        = V_f(a,c) + V_f(c,b).
\end{equation}
%
If $g \colon [a,b] \to \reals$ is another function of bounded variation on $[a,b]$ and $c \in \reals$, then it is clear from the definition that $f + g$ and $cf$ are also of bounded variation. It is also simple to show that the product $fg$ is of bounded variation. Hence $\boundedvar{a,b}$ is an $\reals$-algebra.

Also note that monotonic functions are of bounded variation on any compact interval.

\begin{lemma}
    \label{lem:total-variation-increasing}
    Let $f \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_f(a,x)$ for $x \in (a, b]$ and $V(a) = 0$. Then the functions $V$ and $V - f$ are increasing on $[a,b]$.
\end{lemma}

\begin{proof}
    The function $V$ is clearly increasing, so consider the function $D = V-f$. Let $x,y \in [a,b]$ with $x < y$, and notice that $f(y) - f(x) \leq V_f(x,y)$. Recalling \cref{eq:total-variation-additive} it follows that
    %
    \begin{equation*}
        D(y) - D(x)
            = V(y) - V(x) - (f(y) - f(x))
            = V_f(y,x) - (f(y) - f(x))
            \geq 0.
    \end{equation*}
\end{proof}


\begin{proposition}
    \label{prop:BV-difference-of-increasing-functions}
    A function $f \colon [a,b] \to \reals$ is of bounded variation if and only if it is the difference of two (strictly) increasing functions.
\end{proposition}

\begin{proof}
    By \cref{lem:total-variation-increasing} we can write $f$ as the difference of two increasing functions as $f = V - (V - f)$. Adding a strictly increasing function to both $V$ and $V - f$ yields the claim.
\end{proof}


\begin{proposition}
    \label{prop:total-variation-continuity}
    Let $f \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_f(a,x)$ for $x \in (a, b]$ and $V(a) = 0$. If $c \in [a,b]$, then $f$ is continuous at $c$ if and only if $V$ is continuous at $c$.
\end{proposition}

\begin{proof}
    We prove the claim in the case where $c$ is an inner point of $[a,b]$. First assume that $V$ is continuous at $c$. Since $V$ is monotonic by \cref{lem:total-variation-increasing}, the left- and right-hand limits $V(c-)$ and $V(c+)$ exist. For $x \in (c,b]$ we have
    %
    \begin{equation*}
        \abs{f(x) - f(c)}
            \leq V(x) - V(c)
            \xrightarrow[x \downarrow c]{}
            V(c+) - V(c)
            = 0,
    \end{equation*}
    %
    so $f$ is right-continuous at $c$. Similarly for left-continuity.

    We prove the converse, so assume that $f$ is continuous at $c$ and let $\epsilon > 0$. There exists a $\delta > 0$ such that $0 < \abs{x - c} < \delta$ implies $\abs{f(x) - f(c)} < \epsilon$. Further choose a partition $P = \{x_0, \ldots, x_n\}$ of $[c,b]$ such that
    %
    \begin{equation*}
        V_f(c,b) - \epsilon
            < \sum_{i=1}^n \abs{\Delta f_k}.
    \end{equation*}
    %
    This inequality is conserved when adding points to $P$, so we may assume that $x_1 - x_0 < \delta$, implying that $\abs{\Delta f_1} < \epsilon$. The inequality above then becomes
    %
    \begin{equation*}
        V_f(c,b) - \epsilon
            < \epsilon + \sum_{i=2}^n \abs{\Delta f_k}
            \leq \epsilon + V_f(x_1,b),
    \end{equation*}
    %
    so that $V_f(c,b) - V_f(x_1,b) < 2\epsilon$. This implies that
    %
    \begin{align*}
        0
            &\leq V(x_1) - V(c)
             = V_f(a,x_1) - V_f(a,c) \\
            &= V_f(c,x_1)
             = V_f(c,b) - V_f(x_1,b)
             < 2\epsilon,
    \end{align*}
    %
    showing that $V(c+) = V(c)$. A similar argument yields $V(c-) = V(c)$, so $V$ is continuous at $c$.
\end{proof}


\section{Integration}

Next consider two functions $f, \alpha \colon [a,b] \to \reals$. For each tagged partition $(P,T)$ of $[a,b]$ we define the \emph{Riemann--Stieltjes sum}
%
\begin{equation*}
    S_{f,\alpha}(P,T)
        = \sum_{i=1}^n f(t_i) \Delta\alpha_i.
\end{equation*}
%
This induces a net $S_{f,\alpha} \colon \calP'[a,b] \to \reals$.


\begin{definition}[Riemann--Stieltjes integral]
    Let $f,\alpha \colon [a,b] \to \reals$ be bounded functions. We say that $f$ is \emph{Riemann-integrable} with respect to $\alpha$ (or simply \emph{$\alpha$-integrable}) on $[a,b]$ if the net $S_{f,\alpha}$ has a limit $A \in \reals$. In this case $A$ is called the \emph{Riemann--Stieltjes integral} of $f$ with respect to $\alpha$ on $[a,b]$ and is denoted
    %
    \begin{equation*}
        \int_a^b f \dif \alpha
        \quad \text{or} \quad
        \int_a^b f(x) \dif \alpha(x).
    \end{equation*}
    %
    We denote the set of $\alpha$-integrable functions on $[a,b]$ by $\integrable[\alpha]{a,b}$.
\end{definition}
%
We call $f$ the \emph{integrand} and $\alpha$ the \emph{integrator}. In the case where $\alpha(x) = x$, we use the notations
%
\begin{equation*}
    S_f,
    \quad
    \int_a^b f
    \quad \text{and} \quad
    \int_a^b f(x) \dif x.
\end{equation*}
%
The sums $S_f$ are then simply called \emph{Riemann sums} and the integral the \emph{Riemann integral} of $f$ on $[a,b]$. With this choice of $\alpha$, an $\alpha$-integrable function is called \emph{Riemann integrable} on $[a,b]$, and the set of such functions is denoted $\integrable{a,b}$.

\begin{remark}
    In the ordinary Riemann integral it is not necessary to assume that $f$ is bounded, since integrability in this case implies boundedness. We claim that this assumption can be lifted whenever the integrator $\alpha$ is injective. In fact, we only need to assume that there is a partition $Q$ of $[a,b]$ such that $\alpha$ is injective on each subinterval of $Q$.
    
    If the net $S_{f,\alpha}$ has a limit $A \in \reals$, then there is a tagged partition $(P,T)$ of $[a,b]$ such that
    %
    \begin{equation*}
        \abs{A - S_{f,\alpha}(P',T')} < 1,
        \quad \text{implying that} \quad
        \abs{S_{f,\alpha}(P',T')} < M \defn \abs{A} + 1,
    \end{equation*}
    %
    whenever $(P,T) \preceq (P',T')$. By replacing $P$ with $P \union Q$ we may assume that $\alpha$ is injective on each subinterval of $P$, since $\alpha$ is clearly also injective on each subinterval of $P \union Q$.
    
    Write $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Let $x \in [x_{k-1},x_k]$ and consider the multiset $T_x = \{t_1, \ldots, t_{k-1}, x, t_{k+1}, \ldots, t_n\}$ and the corresponding tagged partition $(P,T_x)$. We then have
    %
    \begin{equation*}
        f(x) \Delta \alpha_k
            = S_{f,\alpha}(P,T_x) - \sum_{i \neq k} f(t_i) \Delta \alpha_i.
    \end{equation*}
    %
    Since $\alpha$ is injective on $[x_{k-1},x_k]$ we have $\Delta\alpha_k \neq 0$, so the above implies that
    %
    \begin{equation*}
        \abs{f(x)}
            = \frac{1}{\abs{\Delta \alpha_k}} \abs[\bigg]{ S_{f,\alpha}(P,T_x) - \sum_{i \neq k} f(t_i) \Delta \alpha_i }
            < \frac{1}{\abs{\Delta \alpha_k}} \biggl( M + \abs[\bigg]{ \sum_{i \neq k} f(t_i) \Delta \alpha_i } \biggr),
    \end{equation*}
    %
    where the inequality follows since $(P,T) \preceq (P,T_x)$ for all $x \in [x_{k-1},x_k]$. The right-hand side is thus as upper bound for $\abs{f}$ on $[x_{k-1},x_k]$, and there are finitely many such intervals, so $f$ is bounded on $[a,b]$ as claimed.
\end{remark}

Below we fix an interval $[a,b]$ and (bounded) integrators $\alpha$ and $\beta$ on it.

\begin{proposition}[Linearity of the integral]
    \label{prop:integral-linearity}
    Let $f,g \in \integrable[\alpha]{a,b}$ and $c_1, c_2 \in \reals$. Then:
    %
    \begin{enumprop}
        \item $c_1 f + c_2 g$ is $\alpha$-integrable on $[a,b]$ and
        %
        \begin{equation*}
            \int_a^b (c_1 f + c_2 g) \dif\alpha
                = c_1 \int_a^b f \dif\alpha + c_2 \int_a^b g \dif\alpha.
        \end{equation*}
        %
        In particular, $\integrable[\alpha]{a,b}$ is a vector space.

        \item $f$ is $(c_1 \alpha + c_2 \beta)$-integrable on $[a,b]$ and
        %
        \begin{equation*}
            \int_a^b f \dif(c_1 \alpha + c_2 \beta)
                = c_1 \int_a^b f \dif\alpha + c_2 \int_a^b f \dif\beta.
        \end{equation*}
    \end{enumprop}
\end{proposition}

\begin{proof}
    This follows immediately from the bilinearity of the map $(f,\alpha) \mapsto S_{f,\alpha}$ along with basic properties of nets.
\end{proof}


\begin{proposition}
    \label{prop:integral-dividing-interval}
    Consider $f, \alpha \colon [a,b] \to \reals$ and let $c \in (a,b)$. If two of the three integrals in \cref{eq:integral-dividing-interval} exist, then so does the third and we have
    %
    \begin{equation}
        \label{eq:integral-dividing-interval}
        \int_a^c f \dif\alpha + \int_c^b f \dif\alpha
            = \int_a^b f \dif\alpha.
    \end{equation}
\end{proposition}

\begin{proof}
    Let $(P,T)$ be a tagged partition of $[a,b]$ such that $c \in P$, and let $P_1 = P \intersect [a,c]$ and $P_2 = P \intersect [c,b]$, and $T_1 = T \intersect [a,c]$ and $T_2 = T \intersect [c,b]$. Then
    %
    \begin{equation}
        \label{eq:Riemann-sums-dividing-interval}
        S_{f,\alpha}(P,T)
            = S_{f,\alpha}(P_1,T_1) + S_{f,\alpha}(P_2,T_2).
    \end{equation}
    %
    If two of the three integrals in \cref{eq:integral-dividing-interval} exist, then two of the nets in \cref{eq:Riemann-sums-dividing-interval} converge to the respective integrals. The third net then also converges to the relevant integral: Notice that we assume without loss of generality that $c \in P$, since adding $c$ to a partition simply yields a finer partition. Also notice that any partition $P'$ of e.g. $[a,c]$ is on the form $P' = P \intersect [a,c]$ for some partition $P$ of $[a,b]$.
\end{proof}


\begin{proposition}[Integration by parts]
    \label{prop:integration-by-parts}
    Given functions $f,\alpha \colon [a,b] \to \reals$, assume that $f \in \integrable[\alpha]{a,b}$. Then $\alpha \in \integrable[f]{a,b}$ and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha + \int_a^b \alpha \dif f
            = f(b)\alpha(b) - f(a)\alpha(a).
    \end{equation*}
    %
    In particular, exchanging $f$ and $\alpha$ we have
    %
    \begin{equation*}
        f \in \integrable[\alpha]{a,b}
            \quad \text{if and only if} \quad
            \alpha \in \integrable[f]{a,b}.
    \end{equation*}
\end{proposition}

\begin{proof}
    Let $(P,T)$ be a tagged partition of $[a,b]$ with $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Writing $A = f(b)\alpha(b) - f(a)\alpha(a)$, notice that
    %
    \begin{equation*}
        A
            = \sum_{i=1}^n f(x_i) \alpha(x_i) - \sum_{i=1}^n f(x_{i-1}) \alpha(x_{i-1}),
    \end{equation*}
    %
    and that
    %
    \begin{equation*}
        S_{\alpha,f}(P,T)
            = \sum_{i=1}^n \alpha(t_i) \Delta f_i
            = \sum_{i=1}^n f(x_i) \alpha(t_i) - \sum_{i=1}^n f(x_{i-1}) \alpha(t_i).
    \end{equation*}
    %
    Hence we have
    %
    \begin{align*}
        A - S_{\alpha,f}(P,T)
            &= \sum_{i=1}^n f(x_i) (\alpha(x_i) - \alpha(t_i)) + \sum_{i=1}^n f(x_{i-1}) (\alpha(t_i) - \alpha(x_{i-1})) \\
            &= S_{f,\alpha}(P \union T, P'),
    \end{align*}
    %
    where $P'$ is obtained from $P$ by duplicating\footnote{Recall that if $(P,T)$ is a tagged partition, then $T$ is a \emph{multiset}.} appropriate elements such that each subinterval of $P \union T$ contains the corresponding element from $P'$. Notice that if $P$ and $T$ have any elements in common, these are not duplicated in the union $P \union T$. However, in this case the corresponding terms in the sum above vanish, so the last equality does in fact hold. Since $P \union T$ is finer than $P$, the claim follows by taking the limit of $S_{\alpha,f}$.
\end{proof}


\begin{proposition}[Change of variables in Riemann--Stieljes integrals]
    Let $f \in \integrable[\alpha]{a,b}$, and let $\phi \colon I \to [a,b]$ be a monotonic (or equivalently continuous) bijection where $I$ is an interval with endpoints $c$ and $d$. Assume that $a = \phi(c)$ and $b = \phi(d)$. Then $f \circ \phi \in \integrable[\alpha \circ \phi]{c,d}$ and
    %
    \begin{equation*}
        \int_a^b f \dif \alpha
            = \int_c^d f \circ \phi \dif(\alpha \circ \phi).
    \end{equation*}
\end{proposition}

\begin{proof}
    Since $\phi$ is bijective it is strictly monotonic, so it induces an order isomorphism $\calP'(I) \to \calP'[a,b]$ given by $(P, T) \mapsto (\phi(P), \phi(T))$.

    Now let $(P,T) \in \calP'(I)$ with $P = \{y_0, \ldots, y_n\}$ and $T = \{s_1, \ldots, s_n\}$. Assume for definiteness that $\phi$ is increasing so that $I = [c,d]$, and write $x_i = \phi(y_i)$ and $t_i = \phi(s_i)$. Then we have
    %
    \begin{equation*}
        S_{f \circ \phi, \alpha \circ \phi}(P,T)
            = \sum_{i=1}^n (f \circ \phi)(s_i) \Delta(\alpha \circ \phi)_i
            = \sum_{i=1}^n f \circ \phi(t_i) \Delta \alpha_i
            = S_{f,\alpha}(\phi(P),\phi(T)).
    \end{equation*}
    %
    Since the map $(P,T) \mapsto (\phi(P),\phi(T))$ is an order isomorphism, each side above converges to the corresponding integral, proving the claim.
\end{proof}


\begin{proposition}[Reduction to a Riemann integral]
    Let $\alpha \in C^1[a,b]$ and $f \in \integrable[\alpha]{a,b}$. Then $f \alpha' \in \integrable{a,b}$, and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \int_a^b f \alpha'.
    \end{equation*}
\end{proposition}

\begin{proof}
    Consider the Riemann(--Stieltjes) sums
    %
    \begin{equation*}
        S_{f\alpha'}(P,T)
            = \sum_{i=1}^n f(t_i) \alpha'(t_i) \Delta x_i
        \quad \text{and} \quad
        S_{f,\alpha}(P,T)
            = \sum_{i=1}^n f(t_i) \Delta \alpha_i.
    \end{equation*}
    %
    By the mean value theorem we can write $\Delta \alpha_i = \alpha'(s_i) \Delta x_i$ for appropriate $s_i \in (x_{i-1}, x_i)$. It follows that
    %
    \begin{equation*}
        S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T)
            = \sum_{i=1}^n f(t_i) (\alpha'(s_i) - \alpha'(t_i)) \Delta x_i.
    \end{equation*}
    %
    By uniform continuity of $\alpha'$, given $\epsilon > 0$ there exists a $\delta > 0$ such that $\abs{x-y} < \delta$ implies $\abs{\alpha'(x) - \alpha'(y)} < \epsilon$ for all $x,y \in [a,b]$. Choosing a tagged partition $(P,T)$ with $\norm{P} < \delta$ we thus have
    %
    \begin{equation*}
        \abs{S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T)}
            \leq \norm{f}_{\sup} \epsilon (b-a).
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, the left-hand side converges to zero. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ \int_a^b f \dif\alpha - S_{f\alpha'}(P,T) }
            \leq \abs[\bigg]{ \int_a^b f \dif\alpha - S_{f,\alpha}(P,T) } + \abs{ S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T) },
    \end{equation*}
    %
    which converges to zero. Hence $S_{f\alpha'}(P,T)$ converges, so $f\alpha' \in \integrable{a,b}$, and its integral equals the $\alpha$-integral of $f$ as claimed.
\end{proof}


\section{Increasing integrators}

\begin{definition}
    Let $f, \alpha \colon [a,b] \to \reals$ be bounded functions, and assume that $\alpha$ is increasing. Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$, and let
    %
    \begin{align*}
        M_i(f)
            &= \sup \set[\big]{f(x)}{x \in [x_{i-1}, x_i]}, \\
        m_i(f)
            &= \inf \set[\big]{f(x)}{x \in [x_{i-1}, x_i]}.
    \end{align*}
    %
    The numbers
    %
    \begin{equation*}
        U_{f,\alpha}(P)
            = \sum_{i=1}^n M_i(f) \Delta \alpha_i
        \quad \text{and} \quad
        L_{f,\alpha}(P)
            = \sum_{i=1}^n m_i(f) \Delta \alpha_i
    \end{equation*}
    %
    are called the \emph{upper and lower Stieltjes sums} of $f$ with respect to $\alpha$ for the partition $P$.
\end{definition}
%
Since $\alpha$ is increasing we have $\Delta\alpha_i \geq 0$, so it is immediate that
%
\begin{equation*}
    L_{f,\alpha}(P)
        \leq S_{f,\alpha}(P,T)
        \leq U_{f,\alpha}(P)
\end{equation*}
%
for any tagged partition $(P,T)$ of $[a,b]$. It is also clear that, if $P \subseteq P'$, then
%
\begin{equation}
    \label{eq:upper-lower-sum-decreasing-increasing}
    U_{f,\alpha}(P) \geq U_{f,\alpha}(P')
    \quad \text{and} \quad
    L_{f,\alpha}(P) \leq L_{f,\alpha}(P'),
\end{equation}
%
and that for any pair of partitions $P_1$ and $P_2$ we have
%
\begin{equation}
    \label{eq:upper-lower-sum-inequality}
    L_{f,\alpha}(P_1) \leq U_{f,\alpha}(P_2).
\end{equation}


% https://tex.stackexchange.com/questions/44237/lower-and-upper-riemann-integrals
% Have changed lowint and the first one in upint -- need to adjust the rest in upint if I want to use them, also make versions of lowint if I want to use those
\def\upint{\mathchoice%
    {\mkern10mu\overline{\vphantom{\intop}\mkern10mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern2mu\underline{\vphantom{\intop}\mkern10mu}\mkern-12mu\int}

\begin{definition}
    Let $f, \alpha \colon [a,b] \to \reals$ be bounded functions with $\alpha$ increasing. Then the numbers
    %
    \begin{equation*}
        \upint_a^b f \dif\alpha
            = \inf \set[\big]{U_{f,\alpha}(P)}{P \in \calP[a,b]}
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \lowint_a^b f \dif\alpha
            = \sup \set[\big]{L_{f,\alpha}(P)}{P \in \calP[a,b]}
    \end{equation*}
    %
    are called the \emph{upper and lower Stieltjes integrals} of $f$ with respect to $\alpha$ on $[a,b]$.
\end{definition}
%
We also use the notations $\overline{I}(f,\alpha)$ and $\underline{I}(f,\alpha)$ for the upper and lower integrals, respectively, when the interval $[a,b]$ is understood. It follows immediately from the definition and \cref{eq:upper-lower-sum-inequality} that $\underline{I}(f,\alpha) \leq \overline{I}(f,\alpha)$. 


\begin{theorem}[Riemann's condition]
    Let $f,\alpha \colon [a,b] \to \reals$ be bounded functions with $\alpha$ increasing. Then the following conditions are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:integrability} $f \in \integrable[\alpha]{a,b}$.
        \item \label{enum:Riemanns-condition} $f$ satisfies \emph{Riemann's condition} with respect to $\alpha$ on $[a,b]$: For every $\epsilon > 0$ there exists a partition $P$ of $[a,b]$ such that
        %
        \begin{equation}
            \label{eq:Riemanns-condition}
            U_{f,\alpha}(P) - L_{f,\alpha}(P) < \epsilon.
        \end{equation}
        \item \label{enum:upper-lower-integrals-equal} $\underline{I}(f,\alpha) = \overline{I}(f,\alpha)$.
    \end{enumthm}
    %
    In this case we have
    %
    \begin{equation*}
        \lowint_a^b f \dif\alpha
            = \int_a^b f \dif\alpha
            = \upint_a^b f \dif\alpha.
    \end{equation*}
\end{theorem}

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:integrability} $\implies$ \subcref{enum:Riemanns-condition}]
    Let $\epsilon > 0$, and choose a partition $P = \{x_0, \ldots, x_n\}$ of $[a,b]$ such that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{i=1}^n f(t_i) \Delta\alpha_i - \int_a^b f \dif\alpha }
        < \epsilon
    \end{equation*}
    %
    for all $t_i \in [x_{i-1},x_i]$. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i }
        < 2 \epsilon
    \end{equation*}
    %
    for all $t_i, t_i' \in [x_{i-1},x_i]$. For any $\delta > 0$ there exist $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i) - f(t_i')
        > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    From this it follows that
    %
    \begin{align*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P)
        &= \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i \\
        &< \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i + \delta (\alpha(b) - \alpha(a)) \\
        &< 3\epsilon
    \end{align*}
    %
    for an appropriate choice of $\delta$. Since $\epsilon$ was arbitrary, this proves \subcref{enum:Riemanns-condition}.
    
    \item[\subcref{enum:Riemanns-condition} $\implies$ \subcref{enum:upper-lower-integrals-equal}]
    If $P$ is any partition of $[a,b]$ we have
    %
    \begin{equation*}
        L_{f,\alpha}(P)
        \leq \lowint_a^b f \dif\alpha
        \leq \upint_a^b f \dif\alpha
        \leq U_{f,\alpha}(P).
    \end{equation*}
    %
    Thus \cref{eq:Riemanns-condition} implies that $0 \leq \overline{I}(f,\alpha) - \underline{I}(f,\alpha) < \epsilon$ for every $\epsilon > 0$, proving \subcref{enum:upper-lower-integrals-equal}.
    
    \item[\subcref{enum:upper-lower-integrals-equal} $\implies$ \subcref{enum:integrability}]
    Let $\epsilon > 0$. There exists a partition $P$ of $[a,b]$ such that
    %
    \begin{equation*}
        \underline{I}(f,\alpha) - \epsilon
        < L_{f,\alpha}(P)
        \leq S_{f,\alpha}(P,T)
        \leq U_{f,\alpha}(P)
        < \overline{I}(f,\alpha) + \epsilon
    \end{equation*}
    %
    for any choice of points $T$ such that $(P,T)$ is a tagged partition. Denoting the common value of $\underline{I}(f,\alpha)$ and $\overline{I}(f,\alpha)$ by $A$, this shows that $\abs{S_{f,\alpha}(P',T') - A} < \epsilon$ for all tagged partitions $(P',T')$ with $P \subseteq P'$. Hence $f \in \integrable[\alpha]{a,b}$, and the integral of $f$ with respect to $\alpha$ equals $A$.
\end{proofsec}
\end{proof}


\section{Mean value theorems}

\begin{proposition}[The first mean value theorem]
    \label{prop:mean-value-1}
    Let $\alpha \colon [a,b] \to \reals$ be increasing, and let $f \in \integrable[\alpha]{a,b}$. Let $m = \inf_{x \in [a,b]} f(x)$ and $M = \sup_{x \in [a,b]} f(x)$. Then there exists a $c \in \reals$ with $m \leq c \leq M$ such that
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = c \int_a^b \dif\alpha
            = c (\alpha(y) - \alpha(x)).
    \end{equation*}
\end{proposition}

\begin{proof}
    If $\alpha(a) = \alpha(b)$ then both sides are zero, so assume that $\alpha(a) < \alpha(b)$. From the inequalities
    %
    \begin{equation*}
        m (\alpha(b) - \alpha(a))
            \leq \int_a^b f \dif\alpha
            \leq M (\alpha(b) - \alpha(a))
    \end{equation*}
    %
    follow that
    %
    \begin{equation*}
        m 
            \leq \frac{1}{\alpha(b) - \alpha(a)} \int_a^b f \dif\alpha
            \leq M,
    \end{equation*}
    %
    so defining $c$ as the middle term, the claim follows.
\end{proof}

% [TODO MVT2?]


\section{Integrators of bounded variation}

\begin{theorem}
    \label{thm:alpha-integrable-implies-V-integrable}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_\alpha(a,x)$ for $x \in (a,b]$ and $V(a) = 0$. Then $\integrable[\alpha]{a,b} \subseteq \integrable[V]{a,b}$.
\end{theorem}

\begin{proof}
    Let $f \in \integrable[\alpha]{a,b}$, and choose $M > 0$ such that $\abs{f} \leq M$. Choose a partition $P = \{x_0, \ldots, x_n\}$ of $[a,b]$ such that $V(b) < \sum_{i=1}^n \abs{\Delta\alpha_i} + \epsilon$. Then
    %
    \begin{align*}
        \sum_{i=1}^n (M_i(f) - m_i(f)) ( \Delta V_i - \abs{\Delta\alpha_i})
            &\leq 2M \sum_{i=1}^n (\Delta V_i - \abs{\Delta\alpha_i}) \\
            &= 2M \biggl( V(b) - \sum_{i=1}^n \abs{\Delta\alpha_i} \biggr) \\
            &< 2M \epsilon.
    \end{align*}
    %
    Also choose $P$ such that $\abs{ \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i } < \epsilon$ for all $t_i, t_i' \in [x_{i-1}, x_i]$. Next let $\delta > 0$. For $i = 1, \ldots, n$, if $\Delta\alpha_i \geq 0$ choose $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i) - f(t_i')
            > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    If instead $\Delta\alpha_i < 0$, choose $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i') - f(t_i)
            > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    It follows that
    %
    \begin{equation*}
        \sum_{i=1}^n (M_i(f) - m_i(f)) \abs{\Delta\alpha_i}
            < \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i
              + \delta V(b)
            < 2 \epsilon
    \end{equation*}
    %
    for an appropriate choice of $\delta$. Combining these inequalities yields
    %
    \begin{equation*}
        U_{f,V}(P) - L_{f,V}(P)
            = \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta V_i
            < 2(M+1)\epsilon,
    \end{equation*}
    %
    and since $\epsilon$ was arbitrary, this shows that $f \in \integrable[V]{a,b}$.
\end{proof}
%
Since $\alpha = V - (V - \alpha)$ and both $V$ and $V - \alpha$ are increasing, this allows us to reduce questions about integrators of bounded variation to questions about monotonic integrators. In particular it lets us use Riemann's condition to prove integrability with respect to integrators of bounded variation.


\begin{corollary}
    \label{cor:integrable-on-subinterval}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Then $f \in \integrable[\alpha]{c,d}$ for every subinterval $[c,d]$ of $[a,b]$.
\end{corollary}

\begin{proof}
    By \cref{thm:alpha-integrable-implies-V-integrable} and \cref{prop:integral-linearity}, it suffices to prove the claim when $\alpha$ is increasing. Furthermore, by \cref{prop:integral-dividing-interval} it suffices to show that $f$ is $\alpha$-integrable on $[a,x]$ for all $x \in (a,b]$. Given $\epsilon > 0$, by \cref{enum:Riemanns-condition} there is a partition $P$ of $[a,b]$ such that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P) < \epsilon.
    \end{equation*}
    %
    By \cref{eq:upper-lower-sum-decreasing-increasing}, adjoining $x$ to $P$ preserves the inequality. Writing $P = \{x_0, \ldots, x_n\}$ we may thus assume that $x = x_k$ for some $k \in \{1, \ldots, n\}$. Letting $P' = P \intersect [a,x]$ we thus have
    %
    \begin{align*}
        U_{f,\alpha}(P') - L_{f,\alpha}(P')
            &= \sum_{i=1}^k (M_i(f) - m_i(f)) \Delta\alpha_i
             \leq \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i \\
            &= U_{f,\alpha}(P) - L_{f,\alpha}(P)
             < \epsilon,
    \end{align*}
    %
    where the first inequality follows since every term in the second sum is non-negative. Thus $f$ is integrable on $[a,x]$.
\end{proof}


\begin{proposition}
    Let $f,\alpha \colon [a,b] \to \reals$ be functions with $f$ continuous and $\alpha$ of bounded variation. Then $f$ is $\alpha$-integrable, and $\alpha$ is $f$-integrable.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $\epsilon > 0$. Uniform continuity of $f$ furnishes a $\delta < 0$ such that $\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} < \epsilon$ for $x,y \in [a,b]$. Let $P = \{x_0,\ldots,x_n\}$ be a partition with $\norm{P} < \delta$. Then $M_i(f) - m_i(f) \leq \epsilon$, implying that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P)
            = \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i
            \leq \epsilon (\alpha(b) - \alpha(a)),
    \end{equation*}
    %
    and since $\epsilon$ was arbitrary, it follows from Riemann's condition that $f \in \integrable[\alpha]{a,b}$. The final claim follows from \cref{prop:integration-by-parts}.
\end{proof}

\fleuronbreak

For functions $f \colon [a,b] \to \reals$ and $g \colon f([a,b]) \to \reals$, when is the composition $g \circ f$ integrable? In the Lebesgue theory all that is required is that $f$ and $g$ are measurable, but as far as I know, no such result is available for the Riemann integral. However, in the case where $g$ is continuous, \cref{prop:continuity-integrability} below shows that $g \circ f$ is indeed integrable. We first prove this and then prove a couple of important corollaries.

\begin{proposition}
    \label{prop:continuity-integrability}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Choose $m,M \in \reals$ such that $m \leq f \leq M$. If $\phi \colon [m,M] \to \reals$ is continuous, then $\phi \circ f \in \integrable[\alpha]{a,b}$.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Put $g = \phi \circ f$ and let $\epsilon > 0$. Uniform continuity of $\phi$ yields a $\delta > 0$ such that $\abs{s-t} < \delta$ implies $\abs{\phi(s) - \phi(t)} < \epsilon$ for $s,t \in [m,M]$. Also choose $\delta$ such that $\delta \leq \epsilon$. Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$ such that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P) < \delta^2.
    \end{equation*}
    %
    Let $A$ consist of those numbers $i \in \{1, \ldots, n\}$ such that $M_i(f) - m_i(f) < \delta$, and let $B$ consist of the remaining $i$. For $i \in A$ we then have $M_i(g) - m_i(g) \leq \epsilon$.

    Let $K > 0$ be such that $\abs{\phi} \leq K$. For $i \in B$ we then have $M_i(g) - m_i(g) \leq 2K$. Furthermore, we have
    %
    \begin{equation*}
        \sum_{i \in B} \Delta\alpha_i
            \leq \frac{1}{\delta} \sum_{i \in B} (M_i(f) - m_i(f)) \Delta\alpha_i
            \leq \frac{1}{\delta} \bigl( U_{f,\alpha}(P) - L_{f,\alpha}(P) \bigr)
            < \delta.
    \end{equation*}
    %
    It thus follows that
    %
    \begin{align*}
        U_{g, \alpha}(P) - L_{g, \alpha}(P)
            &= \sum_{i \in A} (M_i(g) - m_i(g)) \Delta\alpha_i
               + \sum_{i \in B} (M_i(g) - m_i(g)) \Delta\alpha_i \\
            &\leq \epsilon (\alpha(b) - \alpha(a))
               + 2K \delta \\
            &\leq (\alpha(b) - \alpha(a) + 2K) \epsilon.
    \end{align*}
    %
    Since $\epsilon$ was arbitrary, it follows that $g \in \integrable[\alpha]{a,b}$.
\end{proof}


For any function $f \colon X \to \reals$, recall that the \emph{positive and negative parts} of $f$ are the functions $f^+ = f \join 0$ and $f^- = -(f \meet 0)$ respectively, and that $f = f^+ - f^-$. We notice that $f^+$ and $f^-$ are both non-negative.

\begin{corollary}
    \label{cor:positive-negative-part}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Then a function $f \colon [a,b] \to \reals$ is $\alpha$-integrable if and only if $f^+$ and $f^-$ are, in which case
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \int_a^b f^+ \dif\alpha - \int_a^b f^- \dif\alpha.
    \end{equation*}
\end{corollary}

\begin{proof}
    This follows immediately from \cref{prop:continuity-integrability}, since the maps $x \mapsto x \join 0$ and $x \mapsto -(x \meet 0)$ are continuous.
\end{proof}


\begin{corollary}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f,g \in \integrable[\alpha]{a,b}$. Then the functions $\abs{f}$ and $fg$ are also $\alpha$-integrable. In particular, $\integrable[\alpha]{a,b}$ is an $\reals$-algebra.
    
    If $\alpha$ is increasing we also have
    %
    \begin{equation}
        \label{eq:integral-triangle-inequality}
        \abs[\bigg]{ \int_a^b f \dif\alpha }
            \leq \int_a^b \abs{f}\dif\alpha.
    \end{equation}
\end{corollary}
%
Recall from \cref{prop:integral-linearity} that $\integrable[\alpha]{a,b}$ is always a vector space. 

\begin{proof}
    Integrability of $\abs{f}$ follows from \cref{prop:continuity-integrability} since $x \mapsto \abs{x}$ is continuous. The inequality \cref{eq:integral-triangle-inequality} follows since $f \leq \abs{f}$, and since the $\alpha$-integral is increasing when $\alpha$ is.

    For the product $fg$, notice that
    %
    \begin{equation*}
        2fg = (f+g)^2 - f^2 - g^2,
    \end{equation*}
    %
    and that the function $x \mapsto x^2$ is continuous.
\end{proof}


\section{The fundamental theorems of calculus}

\begin{theorem}[The first fundamental theorem of calculus]
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Define a function $F \colon [a,b] \to \reals$ by
    %
    \begin{equation*}
        F(x)
            = \int_a^x f \dif\alpha.
    \end{equation*}
    %
    Then the following hold:
    %
    \begin{enumthm}
        \item \label{enum:integral-is-BV} $F$ is of bounded variation.
        \item \label{enum:continuity-of-integral} Every point of continuity of $\alpha$ is also a point of continuity of $F$.
        \item \label{enum:derivative-of-integral} Assume that $\alpha$ is increasing. If $f$ is continuous and $\alpha$ differentiable at $x \in (a,b)$, then $F$ is differentiable at $x$ with $F'(x) = f(x) \alpha'(x)$.
    \end{enumthm}
\end{theorem}
%
Notice that the integral above exists by \cref{cor:integrable-on-subinterval}, and so do the integrals in the proof below.

% [TODO (ii) requires that alpha continuous implies V continuous!]

\begin{proof}
\begin{proofsec}
    \item[Proof of \subcref{enum:integral-is-BV}]
    Since $\boundedvar{a,b}$ is a vector space, we may assume that $\alpha$ is increasing. By \cref{cor:positive-negative-part} we may further assume that $f$ is positive. Then $F$ is increasing, hence of bounded variation.\footnote{E.g. Apostol [TODO ref] attempt to prove this using \cref{prop:mean-value-1}, but his argument is not, as far as I can tell, correct as stated. Hence we use a different argument, more in the spirit of Lebesgue.}
    
    \item[Proof of \subcref{enum:continuity-of-integral}]
    By \cref{prop:total-variation-continuity} we may assume that $\alpha$ is increasing. Let $x,y \in [a,b]$ with $x \neq y$, and let $I$ denote the closed interval between $x$ and $y$. \Cref{prop:mean-value-1} now furnishes a $c_{xy} \in \reals$ with
    %
    \begin{equation*}
        \inf_{t \in [a,b]} f(t)
            \leq \inf_{t \in I} f(t)
            \leq c_{xy}
            \leq \sup_{t \in I} f(t)
            \leq \sup_{t \in [a,b]} f(t),
    \end{equation*}
    %
    such that
    %
    \begin{equation}
        \label{eq:fundamental-theorem-difference}
        F(y) - F(x)
            = \int_x^y f \dif\alpha
            = c_{xy} (\alpha(y) - \alpha(x)).
    \end{equation}
    %
    Assume now that $\alpha$ is continuous at $x$. Since $y \mapsto c_{xy}$ is bounded on $[a,b]$ (since $f$ is), letting $y \to x$ thus yields continuity of $F$ at $x$.

    \item[Proof of \subcref{enum:derivative-of-integral}]
    If $f$ is continuous at $x$, then $c_{xy} \to f(x)$ as $y \to x$, since $c_{xy}$ is bounded by the supremum and infimum of $f$ in a neighbourhood of $x$. Now \cref{eq:fundamental-theorem-difference} implies that
    %
    \begin{equation*}
        \frac{F(y) - F(x)}{y-x}
            = c_{xy} \frac{\alpha(y) - \alpha(x)}{y-x}
            \xrightarrow[y \to x]{}
            f(x) \alpha'(x),
    \end{equation*}
    %
    as desired.
\end{proofsec}
\end{proof}


\begin{remark}
    In the case $\alpha(x) = x$, \subcref{enum:derivative-of-integral} has a proof that does not use \cref{prop:mean-value-1}: Simply note that
    %
    \begin{equation*}
        \frac{F(y)-F(x)}{y-x} - f(x)
            = \frac{1}{y-x} \int_x^y (f(t) - f(x)) \dif t,
    \end{equation*}
    %
    and notice that the integrand can be made less than any $\epsilon > 0$ if $\abs{t-x} < \delta$ for an appropriate $\delta > 0$. I am not sure that this proof can be generalised.
\end{remark}


\begin{theorem}[The second fundamental theorem of calculus]
    Let $f \in \integrable{a,b}$. If there exists a continuous function $F \colon [a,b] \to \reals$ that is differentiable on $(a,b)$ with $F' = f$, then
    %
    \begin{equation*}
        \int_a^b f
            = F(b) - F(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$. The mean value theorem furnishes points $t_i \in (x_{i-1}, x_i)$ such that $\Delta F_i = F'(t_i) \Delta x_i = f(t_i) \Delta x_i$. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ F(b) - F(a) - \int_a^b f }
            = \abs[\bigg]{ \sum_{i=1}^n f(t_i)\Delta x_i - \int_a^b f }
            < \epsilon
    \end{equation*}
    %
    if $P$ is fine enough. Since $\epsilon$ was arbitrary, this proves the theorem.
\end{proof}


\section{Limit and continuity theorems}


\begin{proposition}
    \label{thm:integral-continuity}
    Let $f \colon [a,b] \times [c,d] \to \reals$ be continuous, and let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Then the function $F \colon [c,d] \to \reals$ given by
    %
    \begin{equation*}
        F(y)
            = \int_a^b f(x,y) \dif\alpha(x)
    \end{equation*}
    %
    is continuous.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. By uniform continuity of $f$, given $\epsilon > 0$ there is a $\delta > 0$ such that $\norm{z - z'} < \delta$ implies $\abs{f(z) - f(z')} < \epsilon$ for $z,z' \in [a,b] \times [c,d]$. Given $y,y' \in [c,d]$ with $\abs{y - y'} < \delta$ we thus have
    %
    \begin{equation*}
        \abs{F(y) - F(y')}
            \leq \int_a^b \abs{f(x,y) - f(x,y')} \dif\alpha(x)
            \leq \epsilon(\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, this shows that $F$ is continuous.
\end{proof}


\begin{proposition}
    Let $f \colon [a,b] \times [c,d] \to \reals$ be bounded, and let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Assume that $f(\,\cdot\,,y) \in \integrable[\alpha]{a,b}$ for all $y \in [c,d]$, that $f(x,\,\cdot\,)$ is continuous on $[c,d]$ and differentiable on $(c,d)$ for all $x \in [a,b]$, and that $D_2 f$ is continuous on $[a,b] \times (c,d)$. Then the function $F \colon [c,d] \to \reals$ given by
    %
    \begin{equation*}
        F(y)
            = \int_a^b f(x,y) \dif\alpha(x)
    \end{equation*}
    %
    is differentiable on $(c,d)$ and
    %
    \begin{equation*}
        F'(y)
            = \int_a^b D_2 f(x,y) \dif\alpha(x).
    \end{equation*}
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $y,y_0 \in (c,d)$ with $y \neq y_0$. By the mean value theorem we have
    %
    \begin{equation*}
        \frac{F(y) - F(y_0)}{y - y_0}
            = \int_a^b \frac{f(x,y) - f(x,y_0)}{y - y_0} \dif\alpha(x)
            = \int_a^b D_2 f(x, y_x) \dif\alpha(x)
    \end{equation*}
    %
    for some $y_x \in (c,d)$ lying between $y$ and $y_0$, depending on $x$. Let $I \subseteq (c,d)$ be a non-trivial compact interval containing $y$. Then $D_2 f$ is uniformly continuous on $[a,b] \times I$, so given $\epsilon > 0$ there is a $\delta > 0$ such that $\norm{z-z'} < \delta$ implies $\abs{D_2 f(z) - D_2 f(z')} < \epsilon$ for $z,z' \in [a,b] \times I$. For $y,y_0 \in I$ with $\abs{y-y_0} < \delta$ we also have $\abs{y_x-y_0} < \delta$ for all $x \in [a,b]$, and so
    %
    \begin{align*}
        \abs[\bigg]{ \int_a^b D_2 f(x, y_x) \dif\alpha(x)
            - \int_a^b D_2 f(x, y_0) \dif\alpha(x) }
            &\leq \int_a^b \abs{ D_2 f(x, y_x) - D_2 f(x, y_0) } \dif\alpha(x) \\
            &\leq \epsilon (\alpha(b) - \alpha(a)).
    \end{align*}
    %
    Since $\epsilon$ was arbitrary, this shows that $F$ is differentiable at $y_0$ with derivative as claimed.
\end{proof}


\begin{proposition}
    Let $\alpha$ be of bounded variation on $[a,b]$, and let $(f_n)_{n\in\naturals}$ be a sequence of $\alpha$-integrable functions on $[a,b]$ that converge uniformly to a function $f$. Then $f$ is also $\alpha$-integrable on $[a,b]$, and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \lim_{n\to\infty} \int_a^b f_n \dif\alpha.
    \end{equation*}
    %
    In particular, $\integrable[\alpha]{a,b}$ is a closed subspace of $C[a,b]$ equipped with the uniform norm.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $\epsilon_n = \norm{f_n - f}_\infty$ such that
    %
    \begin{equation*}
        f_n - \epsilon_n
            \leq f
            \leq f_n + \epsilon_n
    \end{equation*}
    %
    for $n \in \naturals$. It follows that
    %
    \begin{equation*}
        \int_a^b (f_n - \epsilon_n) \dif\alpha
            \leq \lowint_a^b f \dif\alpha
            \leq \upint_a^b f \dif\alpha
            \leq \int_a^b (f_n + \epsilon_n) \dif\alpha,
    \end{equation*}
    %
    and hence,
    %
    \begin{equation*}
        0
            \leq \upint_a^b f \dif\alpha - \lowint_a^b f \dif\alpha
            \leq 2 \epsilon_n (\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    Thus the upper and lower integrals of $f$ are equal, so $f$ is $\alpha$-integrable. Finally we have
    %
    \begin{equation*}
        \abs[\bigg]{ \int_a^b f_n \dif\alpha - \int_a^b f \dif\alpha }
            \leq \int_a^b \abs{f_n - f} \dif\alpha
            \leq \epsilon_n (\alpha(b) - \alpha(a)),
    \end{equation*}
    %
    proving the claim.
\end{proof}


\section{Line integrals}

\newcommand{\partition}{\mathcal{P}}
\newcommand{\grad}{\nabla}

\noindent Recall that a \emph{path} in a topological space $X$ is a continuous map $\gamma \colon [a,b] \to X$, and that $\gamma$ is \emph{closed} if $\gamma(a) = \gamma(b)$. A subset $\Gamma \subseteq X$ is called a \emph{curve} in $X$ if there is a path $\alpha$ in $X$ whose image is $\Gamma$. The image of a path $\gamma$ is called its \emph{trace} and is denoted $\gamma^*$.

\begin{definition}[Equivalence of paths]
	Let $\alpha \colon [a,b] \to X$ and $\beta \colon [c,d] \to X$ be paths in a topological space $X$. If there is an increasing homeomorphism $\phi \colon [c,d] \to [a,b]$ such that $\beta = \alpha \circ \phi$, then $\alpha$ and $\beta$ are said to be \emph{properly equivalent}.

	If $\alpha$ and $\beta$ are closed paths with $\alpha(a) \neq \beta(c)$, then we also say that they are properly equivalent if there is a point $e \in (c,d)$ such that $\alpha$ and $\gamma$ are properly equivalent in the above sense, where $\gamma \colon [e, d-c+e] \to X$ is given by
	%
	\begin{equation*}
		\gamma(t) =
		\begin{cases}
			\beta(t), & t \in [e,d], \\
			\beta(t-d+c), & t \in [d,d-c+e].
		\end{cases}
	\end{equation*}
	
	If the map $\phi$ above is decreasing, then we say that $\alpha$ and $\beta$ are \emph{improperly equivalent}. The paths $\alpha$ and $\beta$ are \emph{equivalent} if they are either properly or improperly equivalent.
\end{definition}
%
Note that the condition that $\phi$ be an increasing (decreasing) homeomorphism is equivalent to it being continuous, strictly increasing (decreasing) and surjective. Also note that equivalent paths trace out the same curve in $X$.

\begin{definition}[Line integrals] % TODO: Assume f is bounded, as for regular integrals?
	Let $\gamma \colon [a,b] \to \setR^d$ be a path, and let $f \colon \gamma^* \to \setR^d$ be a vector field. Given a tagged partition $(P,T)$ of $[a,b]$ then, with notation as above, we form the sums
    %
    \begin{equation*}
        S_{f,\gamma}(P,T)
            = \sum_{i=1}^n f(\gamma(t_i)) \cdot (\gamma(x_i) - \gamma(x_{i-1})).
    \end{equation*}
    %
    Define the \emph{line integral} of $f$ with respect to $\gamma$ as the limit of the net $S_{f,\gamma}$, if the limit exists. We denote this integral by $\int f \cdot \dif\gamma$.
\end{definition}
%
Notice that if $\alpha$ and $\beta$ are properly equivalent paths, then
%
\begin{equation*}
	\int f \cdot \dif\alpha = \int f \cdot \dif\beta.
\end{equation*}
%
If $\alpha$ and $\beta$ are instead improperly equivalent, then the two integrals are equal but with opposite signs.


\begin{proposition}
	Let $\gamma \colon [a,b] \to \setR^d$ be a path, and let $f \colon \gamma^* \to \setR^d$ be a bounded function. Then
	%
	\begin{equation*}
		\int f \cdot \dif\gamma
			= \sum_{k=1}^d \int_a^b f_k \circ \gamma \dif\gamma_k
	\end{equation*}
	%
	whenever each Riemann--Stieltjes integral on the right exists. If in addition $\gamma$ is piecewise $C^1$, then
	%
	\begin{equation*}
		\int f \cdot \dif\gamma
			= \int_a^b f(\gamma(t)) \cdot \gamma'(t) \dif t.
	\end{equation*}
\end{proposition}

\begin{proof}
	Notice that
	%
	\begin{equation*}
		S_{f,\gamma}(P,T)
			= \sum_{k=1}^d \sum_{i=1}^n f_k(\gamma(t_i)) (\gamma_k(t_i) - \gamma_k(t_{i-1})).
	\end{equation*}
	%
	Since the inner sums on the right-hand side approximate the Riemann--Stieltjes integrals $\int_a^b f_k \circ \gamma \dif\gamma_k$, the first claim follows by taking limits. The second claim follows by [reference].
\end{proof}


\begin{theorem}[Integral of a gradient]
	Let $U \subseteq \setR^d$ be open, and let $\phi \in C^1(U)$. For every pair of points $x,y \in U$ and every piecewise $C^1$ path $\gamma \colon [a,b] \to U$ with $\gamma(a) = x$ and $\gamma(b) = y$ we have
	%
	\begin{equation*}
		\int \grad\phi \cdot \dif\gamma
			= \phi(y) - \phi(x).
	\end{equation*}
\end{theorem}
%
If $f = \grad\phi$, then $\phi$ is called a \emph{potential function} for $f$.

\begin{proof}
	Let $a = t_0 < \cdots < t_n = b$ be a partition of $[a,b]$ such that $\gamma'$ is continuous on each subinterval. By the chain rule,
	%
	\begin{equation*}
		(\phi \circ \gamma)'(t)
			= \grad\phi(\gamma(t)) \cdot \gamma'(t)
	\end{equation*}
	%
	on each open subinterval $(t_{i-1}, t_i)$. By [reference],
	%
	\begin{align*}
		\int \grad\phi \cdot \dif\gamma
			&= \sum_{i=1}^n \int_{t_{i-1}}^{t_i} \grad\phi(\gamma(t)) \cdot \gamma'(t) \dif t
			= \sum_{i=1}^n \int_{t_{i-1}}^{t_i} (\phi \circ \gamma)'(t) \dif t \\
			&= \phi(\gamma(b)) - \phi(\gamma(a))
			= \phi(y) - \phi(x),
	\end{align*}
	%
	as desired.
\end{proof}



\begin{theorem}
	Let $U \subseteq \setR^d$ be an open, connected set, and let $f \colon U \to \setR^d$ be a continuous function. Fix a point $x_0 \in U$. For each $x \in U$ and each pair of polygonal paths $\alpha, \beta \colon [a,b] \to U$ joining $x_0$ and $x$, assume that
	%
	\begin{equation*}
		\int f \cdot \dif\alpha
			= \int f \cdot \dif\beta.
	\end{equation*}
	%
	Then there exists a function $\phi \in C^1(U)$ such that $f = \grad\phi$.
\end{theorem}
%
Notice that since $U$ is connected, such polygonal paths exist between any pair of points.

\begin{proof}
	Let $x \in U$, and let $\alpha \colon [a,b] \to U$ be a polygonal curve joining $x_0$ and $x$. Define
	%
	\begin{equation*}
		\phi(x) = \int f \cdot \dif\alpha.
	\end{equation*}
	%
	By hypothesis, the number $\phi(x)$ does not depend on the particular choice of $\alpha$. We show that each partial derivative $D_k \phi(x)$ exists and equals $f_k(x)$.

	Let $B(x,\delta) \subseteq U$ for some $\delta > 0$, and let $\lambda \in [-\delta/2, \delta/2]$. Define a path $\gamma \colon [0,1] \to B(x,\delta)$ by $\gamma(t) = (1-t)x + t(x + \lambda e_k)$, where $e_k$ is the $k$th standard basis vector. Then
	%
	\begin{equation*}
		\phi(x + \lambda e_k) - \phi(x)
			= \int f \cdot \dif\gamma.
	\end{equation*}
	%
	Furthermore, $\gamma_k'(t) = \lambda$ and $\gamma_i'(t) = 0$ for $i \neq k$. Thus $\gamma$ is $C^1$, and so
	%
	\begin{align*}
		\phi(x + \lambda e_k) - \phi(x)
			&= \sum_{i=1}^d \int_0^1 f_i(\gamma(t)) \gamma_i'(t) \dif t \\
			&= \lambda \int_0^1 f_k(\gamma(t)) \dif t
			 = \lambda \int_0^1 g(t,\lambda) \dif t,
	\end{align*}
	%
	where $g(t,\lambda) = f_k((1-t)x + t(x + \lambda e_k))$. Since $g$ is continuous on $[0,1] \times [-\delta/2, \delta/2]$, \cref{thm:integral-continuity} implies that
	%
	\begin{equation*}
		\lim_{\lambda \to 0} \int_0^1 g(t,\lambda) \dif t
			= \int_0^1 g(t,0) \dif t
			= \int_0^1 f_k(x) \dif t
			= f_k(x),
	\end{equation*}
	%
	proving that $D_k \phi(x) = f_k(x)$. Thus $\grad\phi(x) = f(x)$ for all $x \in U$, and $\phi \in C^1(U)$ since $f$ is continuous.
\end{proof}


\chapter{Convergence}

It is well-known that a Cauchy sequence $(x_n)_{n\in\naturals}$ in a metric space $S$ converges to some $x \in S$ if and only if $(x_n)$ has a \emph{subsequence} that converges to $x$.

In this section we highlight a similar feature of convergence in measure and convergence in mean: If $(f_n)_{n\in\naturals}$ is a Cauchy sequence in measure, and if there is a subsequence that converges \emph{pointwise a.e.} to some function $f$, then $(f_n)$ also converges to $f$ in measure. Similarly for convergence in mean.

Furthermore, Markov's inequality implies that convergence in mean is stronger than convergence in measure. In particular, a sequence that is Cauchy in mean is also Cauchy in measure. Hence when we show that convergence in measure and in mean are complete, it suffices to show that being Cauchy in measure implies the existence of a pointwise a.e. convergent subsequence.

\begin{definition}[Convergence in measure]
    Let $(X,\calE,\mu)$ be a measure space. Let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$, and let further $f \in \measurable(\calE)$. We say that $(f_n)$ \emph{converges to $f$ in $\mu$-measure} if for every $\epsilon > 0$,
    %
    \begin{equation*}
        \lim_{n\to\infty} \mu\bigl( \{ \abs{f_n - f} > \epsilon \}\bigr) = 0.
    \end{equation*}
    %
    Furthermore, $(f_n)$ is called a \emph{Cauchy sequence in $\mu$-measure} if, for every $\epsilon > 0$,
    %
    \begin{equation*}
        \lim_{m,n\to\infty} \mu\bigl( \{ \abs{f_m - f_n} > \epsilon \} \bigr) = 0.
    \end{equation*}
\end{definition}

We prove that convergence in $\mu$-measure is complete.

\begin{lemma}
    \label{thm:convergence-in-measure-lemma}
    Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$. If there exists a sequence $(\epsilon_n)_{n\in\naturals}$ of strictly positive numbers such that
    %
    \begin{equation}
        \label{eq:convergence-in-measure-complete-lemma}
        \sum_{n=1}^\infty \epsilon_n < \infty
        \quad \text{and} \quad
        \sum_{n=1}^\infty \mu \bigl( \{ \abs{f_{n+1} - f_n} > \epsilon_n \} \bigr) < \infty,
    \end{equation}
    %
    then there exists a function $f \in \measurable(\calE)$ such that $(f_n)$ converges to $f$ both $\mu$-a.e. and in $\mu$-measure.
\end{lemma}

\begin{proof}
    For $n \in \naturals$, denote the set $\{ \abs{f_{n+1} - f_n} > \epsilon_n \}$ by $E_n$, and define sets $F_k = \bigunion_{n=k}^\infty E_n$ and $F = \bigintersect_{k\in\naturals} F_k$. Notice that $F = \limsup_{n\to\infty} E_n$, so the first Borel--Cantelli lemma implies that $\mu(F) = 0$.
    
    For $m \geq n$ we find that
    %
    \begin{equation}
        \label{eq:convergence-in-measure-Cauchy}
        \abs{ f_m - f_n }
            \leq \sum_{i=n}^{m-1} \abs{f_{i+1} - f_i}
            \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}.
    \end{equation}
    %
    If furthermore $x \in F_k^c$ and $m \geq n \geq k$, then
    %
    \begin{equation*}
        \abs{ f_m(x) - f_n(x) }
            \leq \sum_{i=n}^\infty \epsilon_i.
    \end{equation*}
    %
    The right-hand side converges to zero as $n \to \infty$, which shows that $(f_n(x))$ is a Cauchy sequence in $\reals$ for $x \in F_k^c$, hence for $x \in F^c$. Letting $f = \lim_{n\to\infty} f_n \indicator{F^c}$ we thus find that $(f_n)$ converges to $f \in \measurable(\calE)$ $\mu$-a.e.

    Next we show that $f_n \to f$ in $\mu$-measure as $n \to \infty$. Letting $m \to \infty$ in \cref{eq:convergence-in-measure-Cauchy} we find that
    %
    \begin{equation*}
        \abs{ f_n - f }
        \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}
    \end{equation*}
    %
    $\mu$-a.e. Now let $\epsilon > 0$, and choose an $N \in \naturals$ such that $\sum_{i=N}^\infty \epsilon_i \leq \epsilon$. For $n \geq N$, $\abs{f_n - f} > \epsilon$ then implies that
    %
    \begin{equation*}
        \sum_{i=n}^\infty \epsilon_i
            \leq \epsilon
            < \abs{f_n - f}
            \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}
    \end{equation*}
    %
    $\mu$-a.e., which in turn implies that $\epsilon_i < \abs{f_{i+1} - f_i}$ $\mu$-a.e. for some $i \geq n$. Hence it follows that
    %
    \begin{equation*}
        \mu\bigl( \{ \abs{f_n - f} > \epsilon \}\bigr)
            \leq \mu \Bigl( \bigunion_{i=n}^\infty E_i \Bigr)
            \leq \sum_{i=n}^\infty \mu(E_i),
    \end{equation*}
    %
    which converges to zero by \cref{eq:convergence-in-measure-complete-lemma}.
\end{proof}


\begin{theorem}[Completeness of convergence in measure]
    Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in $\mu$-measure. Then there exists a function $f \in \measurable(\calE)$ such that $f_n \to f$ in $\mu$-measure. Furthermore, $(f_n)$ has a subsequence that converges to $f$ $\mu$-a.e.
\end{theorem}

\begin{proof}
    We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in $\mu$-measure. If $(f_n)$ has a subsequence that converges $\mu$-a.e. to function $f \in \measurable(\calE)$, then $(f_n)$ also converges to $f$ in $\mu$-measure.
    \end{displaytheorem}
    %
    Let $(f_{n_k})$ be such a subsequence. For $\epsilon > 0$ we then have
    %
    \begin{equation*}
        \{ \abs{f_n - f} > \epsilon \}
            \subseteq \{ \abs{f_n - f_{n_k}} > \epsilon/2 \} \union \{ \abs{f_{n_k} - f} > \epsilon/2 \},
    \end{equation*}
    %
    and the measures of the sets on the right-hand side go to zero as $n \to \infty$ (since $n_k \geq n$). This proves the lemma.

    To prove the theorem, choose a subsequence $(g_k) = (f_{n_k})$ such that
    %
    \begin{equation*}
        \mu \bigl( \{ \abs{g_{k+1} - g_k} > 2^{-k} \} \bigr)
            \leq 2^{-k}.
    \end{equation*}
    %
    \cref{thm:convergence-in-measure-lemma} then implies the existence of a function $f \in \measurable(\calE)$ such that $g_k \to f$ both $\mu$-a.e. and in $\mu$-measure. The claim then follows from the above lemma.
\end{proof}


\begin{definition}[Convergence in mean]
    Let $(X,\calE,\mu)$ be a measure space. Let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$, and let further $f \in \measurable(\calE)$. If $p \in (0,\infty)$ we say that $(f_n)$ \emph{converges to $f$ in the $\mu$-$p$-th mean} if
    %
    \begin{equation*}
        \lim_{n\to\infty} \int_X \abs{f_n - f}^p \dif\mu = 0.
    \end{equation*}
    %
    Furthermore, $(f_n)$ is called a \emph{Cauchy sequence in the $\mu$-$p$-th mean} if
    %
    \begin{equation*}
        \lim_{m,n\to\infty} \int_X \abs{f_m - f_n}^p \dif\mu = 0.
    \end{equation*}
\end{definition}

\begin{remark}
    If $(f_n)$ converges to $f$ in the $\mu$-$p$-th mean, then $f_n - f \in \calL^p(\mu)$ for $n \geq N$ for some $N \in \naturals$. Furthermore, if $f \in \calL^p(\mu)$, then $f_n = (f_n - f) + f \in \calL^p(\mu)$ for $n \geq N$.
    
    On the other hand, if $f_n \in \calL^p(\mu)$ for large enough $n$, then $f = (f - f_n) + f_n \in \calL^p(\mu)$. In particular, $\calL^p(\mu)$ is a closed subspace of $\measurable(\calE)$.
\end{remark}


\begin{theorem}[Completeness of convergence in mean]
    Let $(X,\calE,\mu)$ be a measure space, let $p \in (0,\infty)$, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in the $\mu$-$p$-th mean. Then there exists a function $f \in \measurable(\calE)$ such that $f_n \to f$ in the $\mu$-$p$-th mean. Furthermore, $(f_n)$ has a subsequence that converges to $f$ $\mu$-a.e.

    In particular, $\calL^p(\mu)$ is complete.
\end{theorem}

\begin{proof}
    We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $(X,\calE,\mu)$ be a measure space, let $p \in (0,1)$, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in the $\mu$-$p$-th mean. If $(f_n)$ has a subsequence that converges $\mu$-a.e. to function $f \in \measurable(\calE)$, then $(f_n)$ also converges to $f$ in the $\mu$-$p$-th mean.
    \end{displaytheorem}
    %
    Let $(f_{n_k})_{k\in\naturals}$ be a subsequence that converges $\mu$-a.e. to $f$. For $n \in \naturals$, Fatou's lemma implies that
    %
    \begin{align*}
        \int_X \abs{f_n - f}^p \dif\mu
            &= \int_X \liminf_{k\to\infty} \abs{f_n - f_{n_k}}^p \dif\mu
             \leq \liminf_{k\to\infty} \int_X \abs{f_n - f_{n_k}}^p \dif\mu \\
            &\leq \sup_{m \geq n} \int_X \abs{f_n - f_m}^p \dif\mu,
    \end{align*}
    %
    which converges to zero as $n \to \infty$ as desired.

    We now prove the theorem. Markov's inequality implies that $(f_n)$ is also a Cauchy sequence in $\mu$-measure, so [reference] yields a function $f \in \measurable(\calE)$ such that $f_n \to f$ in $\mu$-measure, and such that $(f_n)$ has a subsequence $(f_{n_k})_{k\in\naturals}$ that converges to $f$ $\mu$-a.e. The lemma then implies that $(f_n)$ converges to $f$ in the $\mu$-$p$-th mean.

    For the last claim, if $f_n \in \calL^p(\mu)$ for all $n \in \naturals$, then since $\calL^p(\mu)$ is closed we also have $f \in \calL^p(\mu)$.
\end{proof}


% \subsection{Completeness part 2}

% Let $X$ be a topological vector space. If $(x_n)_{n\in\naturals}$ is a sequence in $X$, then the series $\sum_{n=1}^\infty x_n$ is said to \emph{converge} to $x \in X$ if $\sum_{i=1}^n x_i \to x$ as $n \to \infty$. If $X$ is equipped with a pseudometric $\rho$, then the above series is said to be \emph{absolutely convergent} if $\sum_{n=1}^\infty \rho(x_n,0) < \infty$. If $\rho$ is invariant, then notice that
% %
% \begin{equation*}
%     \rho(x+y, 0)
%         = \rho(x, -y)
%         \leq \rho(x, 0) + \rho(0, -y)
%         = \rho(x, 0) + \rho(y, 0)
% \end{equation*}
% %
% for all $x,y \in X$. The most important case is of course when $\rho$ is induced from a seminorm $\norm{\,\cdot\,}$, in which case $\norm{x} = \rho(x,0)$ for $x \in X$.

% \begin{lemma}
%     Let $X$ be a vector space equipped with an invariant pseudometric $\rho$. Then $X$ is complete if and only if every absolutely convergent series in $X$ converges.
% \end{lemma}

% \begin{proof}
%     First assume that $X$ is complete, and consider a sequence $(x_n)_{n\in\naturals}$ in $X$ with $\sum_{n=1}^\infty \rho(x_n,0) < \infty$. Letting $s_n = \sum_{i=1}^n x_i$, if $m < n$ then
%     %
%     \begin{equation*}
%         \rho(s_m, s_n)
%             = \rho(s_n - s_m, 0)
%             \leq \sum_{i = m+1}^n \rho(x_i, 0)
%             \to 0
%     \end{equation*}
%     %
%     as $m,n \to \infty$. Thus $(s_n)$ is Cauchy and hence convergent.

%     Conversely, assume that every absolutely convergent sequence in $X$ converges, and let $(x_n)_{n\in\naturals}$ be a Cauchy sequence in $X$. Choose a strictly increasing sequence $(n_k)_{k\in\naturals}$ of natural numbers such that $\rho(x_m,x_n) < 2^{-k}$ for $m,n \geq n_k$. Let $y_1 = x_{n_1}$ and $y_k = x_{n_k} - x_{n_{k-1}}$ for $k > 1$. Then $\sum_{i=1}^k y_i = x_k$, and
%     %
%     \begin{equation*}
%         \sum_{k=1}^\infty \rho(y_k, 0)
%             \leq \rho(y_1, 0) + \sum_{k=1}^\infty 2^{-k}
%             = \rho(y_1, 0) + 1
%             < \infty.
%     \end{equation*}
%     %
%     Thus $\lim_{k\to\infty} x_{n_k} = \sum_{k=1}^\infty y_k$ exists, and so $(x_n)$ is a Cauchy sequence with a convergent subsequence, and hence is itself convergent.
% \end{proof}


% \subsection{Completeness part 3}


\chapter{Portmanteau theorems}

If $P$ is a Borel probability measure on a topological space $X$, then a subset $A \subseteq X$ is called a \emph{$P$-continuity set} if $P(\boundary A) = 0$.

Recall that a topological space $X$ is called a \emph{perfect space} or a \emph{$G_\delta$ space} if every closed subset of $X$ is a $G_\delta$ set, i.e. a countable intersection of open sets. (Equivalently, if every open subset is an $F_\sigma$ set, a countable union of closed sets.) Furthermore, if $A$ and $B$ are closed subsets of $X$, a \emph{Urysohn function} for the pair $(A,B)$ is a continuous function $f \colon X \to [0,1]$ with $f(A) = 1$ and $f(B) = 0$.\footnote{The ordering of $A$ and $B$ is only relevant insofar as it is relevant on which set the Urysohn function in question vanishes. If $f \colon X \to [0,1]$ is a Urysohn function for the pair $(A,B)$, then the function $1-f$ is a Urysohn function for the pair $(B,A)$.} By Urysohn's lemma, a topological space is normal (i.e. every pair of disjoint closed sets can be separated by disjoint open sets) if and only if there is a Urysohn function for every pair of disjoint closed subsets. Finally, a (not necessarily Hausdorff) normal $G_\delta$ space is called a \emph{perfectly normal space}. If it is also Hausdorff it also called a \emph{$T_6$-space} or a \emph{perfectly $T_4$ space}.

It is easy to show that (pseudo-)metrisable spaces are perfectly normal. Another notable class of perfectly normal (Hausdorff) spaces are the CW complexes. [TODO: Reference, proof somewhere else maybe?]

\begin{theorem}
    Let $(P_n)_{n\in\naturals}$ and $P$ be probability measures on a perfectly normal space $X$. Then the following conditions are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:portmanteau-weak-convergence} $P_n \wto P$.

        \item \label{enum:portmanteau-closed} $\limsup_{n\to\infty} P_n(F) \leq P(F)$ for all closed $F \subseteq X$.

        \item \label{enum:portmanteau-open} $\liminf_{n\to\infty} P_n(G) \geq P(G)$ for all open $G \subseteq X$.

        \item \label{enum:portmanteau-continuity-sets} $P_n(A) \to P(A)$ for all $P$-continuity sets $A \subseteq X$.
    \end{enumthm}
\end{theorem}


\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:portmanteau-weak-convergence} $\implies$ \subcref{enum:portmanteau-closed}]
    Let $F$ be a closed subset of $X$. Since $X$ is perfect, there is a decreasing sequence $(F_k)_{k\in\naturals}$ of open sets such that $F = \bigintersect_{k\in\naturals} F_k$. Furthermore, since $X$ is normal there is for each $k \in \naturals$ a Urysohn function $g_k$ for the pair $(F,F_k^c)$. We clearly have $\indicator{F} \leq g_k \leq \indicator{F_k}$ so
    %
    \begin{equation*}
        \limsup_{n\to\infty} P_n(F)
            \leq \limsup_{n\to\infty} \int_X g_k \dif P_n
            = \int_X g_k \dif P
            \leq P(F_k).
    \end{equation*}
    %
    Since $F_k \downarrow F$ as $k \to \infty$, continuity of $P$ implies the claim.

    \item[\subcref{enum:portmanteau-closed} $\Leftrightarrow$ \subcref{enum:portmanteau-open}]
    This follows easily by taking complements.

    \item[\subcref{enum:portmanteau-closed} \& \subcref{enum:portmanteau-open} $\implies$ \subcref{enum:portmanteau-continuity-sets}]
    For $A \subseteq X$ we have
    %
    \begin{align*}
        P(\interior{A})
            &\leq \liminf_{n\to\infty} P_n(\interior{A})
             \leq \liminf_{n\to\infty} P_n(A) \\
            &\leq \limsup_{n\to\infty} P_n(A)
             \leq \limsup_{n\to\infty} P_n(\closure{A})
             \leq P(\closure{A}).
    \end{align*}
    %
    If $A$ is a $P$-continuity set then $P(\interior{A}) = P(\closure{A})$, which implies \subcref{enum:portmanteau-continuity-sets}.

    \item[\subcref{enum:portmanteau-continuity-sets} $\implies$ \subcref{enum:portmanteau-weak-convergence}]
    Given $f \in C_b(X)$, by linearity we may assume that $0 \leq f \leq 1$. Then
    %
    \begin{equation*}
        \int_X f \dif P
            = \int_0^\infty P( f \geq t ) \dif t
            = \int_0^1 P( f \geq t ) \dif t,
    \end{equation*}
    %
    and similarly for $P_n$. Since $f$ is continuous, we have $\boundary \{ f \geq t \} \subseteq \{ f = t \}$. Now notice that $\{ f = t \}$ is a $P$-null set except for countably many $t \in (0,1)$, since $P$ is a finite measure.\footnote{Indeed, $f$ is a random variable whose discrete support is precisely this set of $t$s. But the discrete support of any random variable is countable.} Hence $\{ f \geq t \}$ is a $P$-continuity set for all but countably many $t$. It then follows from \subcref{enum:portmanteau-continuity-sets} and the dominated convergence theorem that
    %
    \begin{equation*}
        \int_X f \dif P_n
            = \int_0^1 P_n( f \geq t ) \dif t
            \xrightarrow[n\to\infty]{} \int_0^1 P( f \geq t ) \dif t
            = \int_X f \dif P
    \end{equation*}
    %
    as claimed.
\end{proofsec}
\end{proof}


\begin{remark}
    In the case that $X$ is pseudometrisable, we may pick a pseudometric $\rho$. The sets $F_k$ can then be given explicitly by
    %
    \begin{equation*}
        F_k
            = \set[\bigg]{x \in S}{\rho(x,F) < \frac{1}{k}}.
    \end{equation*}
    %
    Furthermore, we may take the Urysohn functions $g_k$ to be given by $g_k(x) = (1 - k\rho(x,F)) \join 0$. Notice that these are Lipschitz since
    %
    \begin{align*}
        \abs{ g_k(x) - g_k(y) }
            &= \abs[\big]{ (1 - k\rho(x,F)) \join 0 - (1 - k\rho(y,F)) \join 0 } \\
            &\leq \abs{ k\rho(x,F) - k\rho(y,F) }
             \leq k \rho(x,y)
    \end{align*}
    %
    for all $x,y \in X$. In particular they are uniformly continuous. Hence weak convergence in a metrisable space may be characterised by the bounded, uniformly continuous functions, or even the bounded Lipschitz functions. Notice also that this is independent of the metric.
\end{remark}


















\chapter{Dynkin systems and monotone classes}

\begin{definition}[Dynkin systems, $\pi$-systems]
    Let $X$ be a set. A collection $\calD$ of subsets of $X$ is a \emph{Dynkin system} in $X$ if it has the following properties:
    %
    \begin{enumdef}
        \item $X \in \calD$,
        \item $B \setminus A \in \calD$ whenever $A,B \in \calD$ and $A \subseteq B$, and
        \item $\bigunion_{n\in\naturals} A_n \in \calD$ for any increasing sequence $(A_n)_{n\in\naturals}$ of sets in $\calD$.
    \end{enumdef}
    %
    Furthermore, a collection $\calS$ of subsets of $X$ is called a \emph{$\pi$-system} if it is closed under intersections.
\end{definition}
%
It is easy to show that if $\calD$ is both a Dynkin system in $X$ and a $\pi$-system, then it is in fact a $\sigma$-algebra in $X$.

\begin{definition}[Monotone classes, set algebras]
    Let $X$ be a set. A collection $\calM$ of subsets of $X$ is a \emph{monotone class} if it is closed under countable increasing unions and countable decreasing intersections.

    Furthermore, a collection $\calA$ of subsets of $X$ is called a \emph{set algebra} in $X$ if it is closed under finite unions and complements.
\end{definition}
%
We note that a set algebra $\calA$ in $X$ is automatically closed under finite intersections, and that it also contains both $\emptyset$ and $X$. It is easy to show that if $\calM$ is both a monotone class and a set algebra in $X$, then it is in fact a $\sigma$-algebra in $X$.

Notice that we have two pairs of properties that ensure that a collection of sets is a $\sigma$-algebra. On the one hand we should think of Dynkin systems and monotone classes as being analogous, and similarly for $\pi$-systems and set algebras on the other. The latter pair of properties are algebraic, while the first pair are somehow analytic (or continuous), in that they involve infinitely many operations.

It turns out that if $\calS$ is a $\pi$-system, then the Dynkin system $\delta(\calS)$ generated by $\calS$ is also a $\pi$-system. Similarly, if $\calA$ is a set algebra, then the monotone class $M(\calA)$ generated by $\calA$ is also a set algebra. We have the following two results whose proofs are basically identical:

\begin{theorem}[Dynkin's lemma]
    Let $\calS$ be a $\pi$-system in a set $X$. Then $\delta(\calS)$ is also a $\pi$-system, and in particular
    %
    \begin{equation}
        \label{eq:Dynkins-lemma}
        \delta(\calS) = \sigma(\calS).
    \end{equation}
\end{theorem}

\begin{proof}
    The identity \cref{eq:Dynkins-lemma} follows if $\delta(\calS)$ is a $\pi$-system: For then it is also a $\sigma$-algebra, and then $\sigma(\calS) \subseteq \delta(\calS)$.

    For $A \in \delta(\calS)$ define
    %
    \begin{equation*}
        \calD_A
            = \set{B \in \delta(\calS)}{A \intersect B \in \delta(\calS)}.
    \end{equation*}
    %
    This is easily seen to be a Dynkin system. Also notice that $B \in \calD_A$ if and only if $A \in \calD_B$ for all $A,B \in \delta(\calS)$. Furthermore, if $A \in \calS$ then $\calS \subseteq \calD_A$, and so $\delta(\calS) \subseteq \calD_A$. In other words,
    %
    \begin{equation*}
        B \in \calD_A
        \quad
        \text{for $A \in \calS$ and $B \in \delta(\calS)$}.
    \end{equation*}
    %
    By symmetry we then have
    %
    \begin{equation*}
        A \in \calD_B
        \quad
        \text{for $A \in \calS$ and $B \in \delta(\calS)$},
    \end{equation*}
    %
    and since $\calD_B$ is a Dynkin system it follows that $\delta(\calS) \subseteq \calD_B$. Hence $\delta(\calS)$ is a $\pi$-system as desired.
\end{proof}


\begin{theorem}[The monotone class lemma]
    Let $\calA$ be a set algebra in a set $X$. Then $M(\calA)$ is also a set algebra, and in particular
    %
    \begin{equation}
        \label{eq:monotone-class-lemma}
        M(\calA) = \sigma(\calA).
    \end{equation}
\end{theorem}

\begin{proof}
    The identity \cref{eq:monotone-class-lemma} follows if $M(\calA)$ is a set algebra: For then it is also a $\sigma$-algebra, and then $\sigma(\calA) \subseteq M(\calA)$.

    For $A \in M(\calA)$ define
    %
    \begin{equation*}
        \calM_A
            = \set{B \in M(\calA)}{\text{$A \setminus B$, $B \setminus A$, and $A \intersect B$ are in $M(\calA)$}}.
    \end{equation*}
    %
    This is easily seen to be a monotone class. Also notice that $B \in \calM_A$ if and only if $A \in \calM_B$ for all $A,B \in M(\calA)$. Furthermore, if $A \in \calA$ then $\calA \subseteq \calM_A$, and so $M(\calA) \subseteq \calM_A$. In other words,
    %
    \begin{equation*}
        B \in \calM_A
        \quad
        \text{for $A \in \calA$ and $B \in M(\calA)$}.
    \end{equation*}
    %
    By symmetry we then have
    %
    \begin{equation*}
        A \in \calM_B
        \quad
        \text{for $A \in \calA$ and $B \in M(\calA)$},
    \end{equation*}
    %
    and since $\calM_B$ is a monotone class it follows that $M(\calA) \subseteq \calM_B$. Hence $M(\calA)$ is a set algebra as desired.
\end{proof}


\chapter{Complex analysis}

[Should it be here?]

\newcommand{\diam}{\operatorname{diam}}
\newcommand{\hol}{\mathcal{H}}

If $V \subseteq \complex$ is open and $f \colon V \to \complex$ is differentiable at every point in $V$, then we say that $f$ is \emph{holomorphic} on $V$. The set of functions holomorphic on $V$ is denoted $\hol(V)$.

\begin{theorem}[The Cauchy--Goursat Lemma]
    If $f \in \hol(V)$, then
    %
    \begin{equation*}
        \int_{\boundary T} f(x) \dif z = 0
    \end{equation*}
    %
    for every triangle $T \subseteq V$.
\end{theorem}

\begin{proof}
    Notice that any triangle $T$ can be subdivided into four smaller triangles $T^1, \ldots, T^4$ whose corners are the corners and midpoints of the sides of $T$. We then clearly have
    %
    \begin{equation*}
        \int_{\boundary T} g(z) \dif z
            = \sum_{i=1}^4 \int_{\boundary T^i} g(z) \dif z
    \end{equation*}
    %
    for all $g \in C(T)$.

    Let $T_0 \subseteq V$ be a triangle, and consider the integral
    %
    \begin{equation*}
        I
            = \int_{\boundary T_0} f(z) \dif z.
    \end{equation*}
    %
    By the above considerations we have
    %
    \begin{equation*}
        \abs{I}
            \leq 4 \abs[\bigg]{ \int_{\boundary T_0^i} f(z) \dif z }
    \end{equation*}
    %
    for at least one value of $i$. For this value of $i$ let $T_1 = T_0^i$. Continuing this process yields a sequence $(T_n)_{n\in\naturals}$ of triangles such that
    %
    \begin{equation*}
        \abs{I}
            \leq 4^n \abs[\bigg]{ \int_{\boundary T_n} f(z) \dif z }
    \end{equation*}
    %
    for $n \in \naturals_0$.

    Furthermore, each of the four triangles in a subdivision of a triangle $T$ have side lengths half of those of $T$, so
    %
    \begin{equation*}
        \diam T_n
            = 2^{-n} \diam T_0
    \end{equation*}
    %
    for $n \in \naturals_0$. Thus there exists a point $z_0 \in \complex$ such that $\bigintersect_{n\in\naturals_0} T_n = \{ z_0 \}$ since $(T_n)$ is a sequence of closed sets whose diameters tend to zero. It follows that
    %
    \begin{equation*}
        \sup_{z \in \boundary T_n} \abs{z - z_0}
            \leq 2^{-n} \diam T_0.
    \end{equation*}
    %
    Given $\epsilon > 0$ there exists an $r > 0$ such that
    %
    \begin{equation*}
        \abs{ f(z) - f(z_0) - f'(z_0)(z - z_0) }
            \leq \epsilon \abs{z - z_0}
    \end{equation*}
    %
    for $z \in B(z_0,r)$, and there further exists an $N \in \naturals$ such that $n \geq N$ implies $T_n \subseteq B(z_0,r)$. Now notice that the function $z \mapsto f(z_0) + f'(z_0)(z - z_0)$ has an antiderivative, so its integral along $\boundary T_n$ is zero. Denoting the length of the curve $\boundary T_n$ by $L_n$ we have $L_n \leq 2 \diam T_n$. Hence,
    %
    \begin{align*}
        \abs[\bigg]{ \int_{\boundary T_n} f(z) \dif z }
            &\leq \int_{\boundary T_n} \abs{ f(z) - f(z_0) - f'(z_0)(z - z_0) } \dif z \\
            &\leq L_n \epsilon \sup_{z \in \boundary T_n} \abs{z - z_0} \\
            &\leq 2^{-2n+1} \epsilon (\diam T_0)^2
    \end{align*}
    %
    for $n \geq N$, and so
    %
    \begin{equation*}
        \abs{I}
            \leq 2 \epsilon (\diam T_0)^2.
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, it follows that $I = 0$ as desired.
\end{proof}


\chapter{The extended real line}

\newcommand{\exreals}{\closure{\reals}}

Let $+\infty$ (or simply $\infty$) and $-\infty$ denote elements disjoint from $\reals$. The \emph{extended real line}, as a set, is then the union $\exreals = \reals \union \{\pm\infty\}$. We extend the ordering on $\reals$ to $\exreals$ by declaring that $-\infty < a$ for all $a \neq -\infty$ and that $\infty > a$ for all $a \neq \infty$. This clearly makes $\exreals$ into a totally ordered set. We further equip it with order topology, i.e. the topology generated by open rays $\set{x \in \exreals}{a < x}$ and $\set{x \in \exreals}{x < b}$ for all $a,b \in \exreals$. One easily sees that this is a Hausdorff topology (in fact it is $T_5$, which all order topologies are), so in particular singletons are closed and $\reals$ is open.

Since $\exreals$ is a topological space we can consider the Borel $\sigma$-algebra $\borel(\exreals)$. Before characterising this we recall two elementary results:
%
\begin{enumerate}
    \item If $(X,\calT)$ is a topological space and $A \subseteq X$ is any subspace, then $\borel(A) = \borel(X)_A$. That is, the Borel $\sigma$-algebra generated by the subspace topology on $A$ agrees with the restriction of the Borel $\sigma$-algebra on $X$ to $A$.

    \item If $(X,\calE)$ is a measurable space and $A \in \calE$, then
    %
    \begin{equation*}
        \calE
            = \set{E \union F}{E \in \calE_A, F \in \calE_{A^c}}.
    \end{equation*}
    %
    The inclusion \enquote{$\supseteq$} is obvious, and the other inclusion follows since if $B \in \calE$, then
    %
    \begin{equation*}
        B
            = (B \intersect A) \union (B \intersect A^c),
    \end{equation*}
    %
    and $B \intersect A \in \calE_A$ and $B \intersect A^c \in \calE_{A^c}$ by the definition of the subspace $\sigma$-algebra.
\end{enumerate}
%
This easily implies the following result:

\begin{proposition}
    The Borel $\sigma$-algebra on $\exreals$ is given by
    %
    \begin{equation*}
        \borel(\exreals)
            = \set[\big]{A \union S}{A \in \borel(\reals), S \subseteq \{\pm\infty\}}
            = \set{B \subseteq \exreals}{B \intersect \reals \in \borel(\reals)}.
    \end{equation*}
\end{proposition}

\begin{proof}
    To prove the first equality we only need to verify that $\borel(\{\pm\infty\}) = 2^{\{\pm\infty\}}$. But this is obvious since both sets $\{\infty\}$ and $\{-\infty\}$ are closed. The second equality easily follows from the first. (Note that we cannot simply use the fact that we can characterise $\borel(\reals)$ in terms of $\borel(\exreals)$, which we can do since $\reals$ is a subset of $\exreals$, since we are trying to do the exact opposite.)
\end{proof}

\nocite{*}

\printbibliography


\end{document}

% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Miscellaneous analysis notes}
\newcommand\docauthor{Danny NygÃ¥rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{latex-sty/topologycommands}
% \usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{fit, patterns}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\setF}{\mathbb{F}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\borel}{\mathcal{B}}
\newcommand{\measurable}{\mathcal{M}}
\newcommand{\wto}{\Rightarrow}
\DeclarePairedDelimiter{\net}{\langle}{\rangle}
\newcommand{\strucS}{\mathfrak{S}}
\DeclarePairedDelimiter{\gen}{\langle}{\rangle} % Generating set
\newcommand{\frakL}{\mathfrak{L}}

\newenvironment{displaytheorem}{%
	\begin{displayquote}\itshape%
}{%
	\end{displayquote}%
}

% Break
\usepackage{adforn}
\newcommand\fleuronbreak{\fancybreak{\textcolor{linkcolor}{\adfhangingflatleafleft}}}


\begin{document}

\maketitle

\chapter{The exponential function}

\begin{theorem}
    Given $b > 0$ there exists a unique continuous function $E_b \colon \reals \to \reals$ such that $E_b(m/n) = (b^m)^{1/n}$ for all $m,n \in \ints$ with $n > 0$.
\end{theorem}

\begin{proof}
    We first assume that $b > 1$. The proof of the theorem in this case is by the following stages:
    %
    \begin{enumerate}
        \item Given $m,n,p,q \in \ints$ with $n,q > 0$ and $r = m/n = p/q$, then
        %
        \begin{equation*}
            (b^m)^{1/n}
                = (b^p)^{1/q}.
        \end{equation*}
        %
        Thus defining $E_b(r) = (b^m)^{1/n}$ makes sense.

        \item If $r,s \in \rationals$, then $E_b(r+s) = E_b(r) E_b(s)$. In particular, $E_b$ is increasing on $\rationals$.
        
        \item The map $E_b$ is continuous on $\rationals$.
        
        \item For $x \in \reals$, let
        %
        \begin{equation*}
            L_x
                = \set{E_b(r)}{t \in \rationals, t \leq x}.
        \end{equation*}
        %
        Then $E_b(r) = \sup L_r$ for $r \in \rationals$, so it makes sense to define $E_b(x) = \sup L_x$ for all $x \in \reals$. Hence $E_b$ is increasing and therefore continuous on $\reals$.

        \item For $x,y \in \reals$ we have $E_b(x+y) = E_b(x) E_b(y)$.
    \end{enumerate}
    %
    (1) Notice that $mq = pn$, so
    %
    \begin{equation*}
        \bigl[ (b^m)^{1/n} \bigr]^{pn}
            = (b^m)^p
            = (b^p)^m
            = \bigl[ (b^p)^{1/q} \bigr]^{mq}
            = \bigl[ (b^p)^{1/q} \bigr]^{pn}.
    \end{equation*}
    %
    The (positive) $pn$th root of this number is unique, so $(b^m)^{1/n} = (b^p)^{1/q}$.

    (2) Write $r = m/n$ and $s = p/q$ for appropriate $m,n,p,q \in \ints$ with $n,q > 0$. Then
    %
    \begin{equation*}
        [ (b^m)^{1/n} (b^p)^{1/q} ]^{nq}
            = b^{mq} b^{pn}
            = b^{mq + pn}.
    \end{equation*}
    %
    Taking the $nq$th root implies that
    %
    \begin{equation*}
        E_b(r) E_b(s)
            = (b^m)^{1/n} (b^p)^{1/q}
            = (b^{mq+pn})^{1/nq}
            = E_b(r + s),
    \end{equation*}
    %
    where we in the last equality use that
    %
    \begin{equation*}
        r + s
            = \frac{m}{n} + \frac{p}{q}
            = \frac{mq + pn}{nq}.
    \end{equation*}
    %
    To see that $E_b$ is increasing on $\rationals$, notice that the assumption that $b > 1$ implies that $E_b(s) > 1$ for $s > 0$. If also $r \in \rationals$, then
    %
    \begin{equation*}
        E_b(r + s)
            = E_b(r) E_b(s)
            \geq E_b(r),
    \end{equation*}
    %
    so $E_b$ is increasing.
    
    (3) We begin by showing that $E_b$ is continuous from above at $0$. Since $E_b$ is monotonic and $E_b(0) = 1$, it suffices to show that $\lim_{n\to\infty} E_b(r_n) = 1$ for some sequence $(r_n)_{n\in\naturals}$ in $\rationals$ that decreases to $0$. We claim that the sequence given by $r_n = 1/n$ has this property. Clearly $1/n \downarrow 0$, so assume that $E_b(1/n)$ did not converge to $1$. Then there would be some $\epsilon > 0$ such that $E_b(1/n) \geq 1 + \epsilon$, i.e. $b \geq (1 + \epsilon)^n$, for all $n \in \naturals$. But by [Bernoulli's inequality] this is impossible, so we must have $E_b(1/n) \to 1$. A similar argument shows that $E_b$ is continuous from below at $0$, using the fact that $E_b(-1/n) = (1/b)^n$.

    Finally let $r,h \in \rationals$, and notice that
    %
    \begin{equation*}
        E_b(r+h)
            = E_b(r) E_b(h)
            \to E_b(r) E_b(0)
            = E_b(r)
    \end{equation*}
    %
    as $n \to \infty$. Thus $E_b$ is also continuous at $r$.

    (4) We clearly have $E_b(r) \leq \sup L_r$. For the opposite inequality, notice that $E_b(r)$ is an upper bound for $L_r$ since $E_b$ is increasing on $\rationals$. Hence also $\sup L_r \leq E_b(r)$.

    If $x \leq y$ then $L_x \subseteq L_y$, and hence $E_b(x) \leq E_b(y)$. Thus $E_b$ is monotonic on $\reals$. But then since $E_b$ continuous on a dense subset of $\reals$, it is clearly also continuous on $\reals$.

    (5) Let $(r_n)$ and $(s_n)$ be sequences in $\rationals$ with limits $x$ and $y$ respectively. Then $r_n+s_n$ converges to $x+y$, so
    %
    \begin{equation*}
        E_b(x+y)
            = \lim_{n\to\infty} E_b(r_n + s_n)
            = \lim_{n\to\infty} E_b(r_n) E_b(s_n)
            = E_b(x) E_b(y).
    \end{equation*}
\end{proof}


\chapter{Introduction}

\begin{definition}
    Let $X$ be a set. A \emph{sequence} in $X$ is a map $a \colon \naturals \to X$. For $n \in \naturals$, we usually write $a_n$ for $a(n)$ and denote $a$ by $(a_n)_{n\in\naturals}$ or simply $(a_n)$.
\end{definition}


\begin{definition}
    Let $(S,\rho)$ be a metric space, and let $(a_n)_{n\in\naturals}$ be a sequence in $S$. We say that $(a_n)$ \emph{converges} to a point $a \in S$ if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $n \geq N$ implies that $\rho(a_n,a) < \epsilon$. In this case we call $a$ the \emph{limit} of $(a_n)$ and write $a_n \to a$ as $n \to \infty$, and we say that $(a_n)$ is \emph{convergent}.

    Furthermore, $(a_n)$ is called a \emph{Cauchy sequence} if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $m,n \geq N$ implies that $\rho(a_m,a_n) < \epsilon$. If every Cauchy sequence in $S$ is convergent, then $S$ is said to be \emph{complete}.
\end{definition}
%
Notice that limits of sequences in metric spaces are unique. It is also clear that convergent sequences are Cauchy, and that Cauchy sequences are bounded: We say that a sequence $(a_n)_{n\in\naturals}$ in a metric space is \emph{bounded} if the set $\set{a_n}{n\in\naturals}$ is bounded.

If $(X,\leq)$ is a poset, a sequence $(a_n)_{n\in\naturals}$ in $X$ is \emph{increasing} (\emph{decreasing}) if $m \leq n$ implies $a_m \leq a_n$ ($a_m \geq a_n$) for all $m,n \in \naturals$. If $m < n$ implies $a_m < a_n$ ($a_m > a_n$), then $(a_n)$ is \emph{strictly} increasing (decreasing). A sequence that is either (strictly) increasing or (strictly) decreasing is called \emph{(strictly) monotonic}.

Given a sequence $(a_n)_{n\in\naturals}$ in a set $X$ and a strictly increasing sequence $(n_k)_{k\in\naturals}$ in $\naturals$, the sequence $(a_{n_k})_{k\in\naturals}$ is called a \emph{subsequence} of $(a_n)$. In particular, every sequence is a subsequence of itself.

\begin{lemma}
    Let $(a_n)_{n\in\naturals}$ be a sequence in a metric space $(S,\rho)$. If $(a_n)$ is both Cauchy and has a convergent subsequence, then $(a_n)$ itself is convergent.
\end{lemma}

\begin{proof}
    Let $(a_{n_k})_{k\in\naturals}$ be a convergent subsequence of $(a_n)$, and let $\epsilon > 0$. Choose $N_1, N_2 \in \naturals$ such that
    %
    \begin{equation*}
        m,n \geq N_1
        \quad \implies \quad
        \rho(a_m, a_n) < \frac{\epsilon}{2}
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        k \geq N_2
        \quad \implies \quad
        \rho(a_{n_k}, a) < \frac{\epsilon}{2},
    \end{equation*}
    %
    where $a \in S$ is the limit of $(a_{n_k})$. For $n \geq N_1 \join N_2$ we thus have
    %
    \begin{equation*}
        \rho(a_n, a)
            \leq \rho(a_n, a_m) + \rho(a_m, a)
            < \frac{\epsilon}{2} + \frac{\epsilon}{2}
            = \epsilon,
    \end{equation*}
    %
    showing that $a_n \to a$ as $n \to \infty$.
\end{proof}


\section{Sequences of real numbers}

\begin{proposition}
    Let $(a_n)_{n\in\naturals}$ be a monotonic sequence in $\reals$. Then $(a_n)$ is convergent if and only if it is bounded, in which case it converges to $\sup_{n\in\naturals} a_n$ if it is increasing and $\inf_{n\in\naturals} a_n$ if it is decreasing.
\end{proposition}

\begin{proof}
    If $(a_n)$ is convergent then it is bounded, so assume that it is bounded and let $\epsilon > 0$. For definiteness we assume that it is increasing and let $s = \sup_{n\in\naturals} a_n$. By definition of $s$ there exists an $N \in \naturals$ such that $s - a_N < \epsilon$. Since $(a_n)$ is increasing and $s$ is an upper bound of the sequence, we thus have
    %
    \begin{equation*}
        0 \leq s - a_n < \epsilon
    \end{equation*}
    %
    for all $n \geq N$, proving that $a_n \to s$.
\end{proof}

\begin{lemma}
    Every sequence in $\reals$ has a monotonic subsequence.
\end{lemma}

\begin{proof}
    Let $(a_n)_{n\in\naturals}$ be a sequence in $\reals$. We say that $n \in \naturals$ is a \emph{peak} if $a_n \geq a_m$ for all $m \geq n$. If $(a_n)$ has infinitely many peaks, the subsequence consisting of these constitute a decreasing subsequence.

    Hence we assume that $(a_n)$ only has finitely many peaks. We construct an increasing sequence $(n_k)_{n\in\naturals}$ in $\naturals$ as follows: Let $n_1 \in \naturals$ be such that all peaks are strictly less than $n_1$, and assume that $n_1, \ldots, n_{k-1}$ have been chosen such that $a_1 \leq \cdots \leq a_{n_{k-1}}$. Since $a_{n_{k-1}}$ is not a peak there is an $n' > n_{k-1}$ such that $a_{n_{k-1}} < a_{n'}$. Letting $n_k = n'$ we obtain an increasing subsequence $(a_{n_k})$ of $(a_n)$, proving the claim.
\end{proof}

\begin{theorem}[The Bolzano--Weierstrass theorem]
    Every subset of $\reals^d$ is sequentially compact if and only if it is closed and bounded.
\end{theorem}
%
We recall that a topological space $X$ is \emph{sequentially compact} if every sequence in $X$ has a convergent subsequence.

\begin{proof}
    We begin with the case $d = 1$. Let $A \subseteq \reals$ be closed and bounded, and let $(a_n)_{n\in\naturals}$ be a sequence in $A$. Let $(a_{n_k})$ be a monotonic subsequence of $(a_n)$, and notice that $(a_{n_k})$ is convergent since it is bounded.

    The case for general $d$ follows by induction in $d$, by noticing that a sequence in $\reals^d$ converges if and only if each coordinate sequence converges.

    For the converse, let $A \subseteq \reals^d$ be sequentially compact. If $A$ were not bounded we could choose $a_n \in A \intersect B(0,n)$ for all $n \in \naturals$, yielding a sequence $(a_n)$ with no convergent subsequence. Furthermore, if $(a_n)$ is a sequence in $A$ converging to a point $a \in \reals^d$, it is in particular a Cauchy sequence. Since it has a subsequence converging to a point $a' \in A$, and by [lemma] we must have $a = a'$. Thus $A$ is also closed.
\end{proof}


\begin{theorem}[Completeness of $\reals^d$]
    The Euclidean space $\reals^d$ is complete.
\end{theorem}

\begin{proof}
    Let $(a_n)_{n\in\naturals}$ be a Cauchy sequence in $\reals^d$. Hence it is bounded, and so it has a convergent subsequence by the Bolzano--Weierstrass theorem. But then $(a_n)$ itself converges by [lemma], so $\reals^d$ is complete.
\end{proof}


\begin{theorem}[The Heine--Borel theorem]
    Every subset of $\reals^d$ is compact if and only if it is closed and bounded.
\end{theorem}

\begin{proof}
    Of course every compact set is closed in any Hausdorff space and bounded in any metric space, so we only consider the other implication.
    
    We first show that closed and bounded intervals are compact. Consider the interval $[a,b]$, and let $\calU$ be an open cover of $[a,b]$. Define the set
    %
    \begin{equation*}
        A
            = \set[\big]{x \in [a,b]}{\text{$[a,x]$ has a finite subcover in $\calU$}}.
    \end{equation*}
    %
    We clearly have $a \in A$ since a point is covered by a single set in $\calU$. If $s = \sup A$ then $a \leq s \leq b$. Suppose that $s < b$ and choose a set $U \in \calU$ with $s \in U$. There exist $r,t \in U$ such that $r < s < t$, and so $r \in A$. Let $\calU'$ denote a finite subcover of $[a,r]$ in $\calU$. Then $\calU' \union \{U\}$ is a finite subcover of $[a,t]$, contradicting the assumption that $s < b$. Hence $s = b$.

    Next, choose $V \in U$ with $b \in V$, and let $c \in V$ with $c < b$. Then $c \in A$, and adjoining $V$ to a finite subcover of $[a,c]$ yields a finite subcover of $[a,b]$, so $b \in A$. Thus $[a,b]$ is compact.

    Finally, let $K \subseteq \reals^d$ be closed and bounded. Since it is bounded it is contained in some cube $[-a,a]^d$. But this cube is a product of compact sets and hence compact, so $K$ is a closed subset of a compact set. The claim follows.
\end{proof}


If $A$ is a subset of a metric space $S$, recall that $A$ is \emph{totally bounded} if, for every $\epsilon > 0$, $A$ can be covered by finitely many open balls of radius $\epsilon$.

\begin{theorem}
    If $A$ is a subset of a metric space $(S,\rho)$, then the following are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:complete-totally-bounded} $A$ is complete and totally bounded.
        \item \label{enum:sequentially-compact} $A$ is sequentially compact.
        \item \label{enum:compact} $A$ is compact.
    \end{enumthm}
\end{theorem}

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:complete-totally-bounded} $\implies$ \subcref{enum:sequentially-compact}]
    Assume that $A$ is complete and totally bounded, and let $(x_n)_{n\in\naturals}$ be a sequence in $A$. Now $A$ can be covered by finitely many balls of radius $1$, at least one of which, say $B_1$, contains $x_n$ for infinitely many $n$, say for $n \in N_1 \subseteq \naturals$. Similarly, $A \intersect B_1$ may be covered by finitely many balls of radius $1/2$, and again there is a ball $B_2$ containing $x_n$ for infinitely many $n \in N_1$, say for $n \in N_2$. Continuing recursively we obtain a sequence of balls $B_i$ of radius $1/i$ and a decreasing sequence $(N_i)_{i\in\naturals}$ of infinite subsets of $\naturals$ such that $x_n \in B_i$ for $n \in N_i$.

    Next, choose a strictly increasing sequence $(n_i)_{i\in\naturals}$ of naturals numbers such that $n_i \in N_i$. Then $\rho(x_{n_i}, x_{n_j}) < 2/i$ for $i \leq j$, so $(x_{n_i})_{i\in\naturals}$ is a Cauchy sequence., and since $A$ is complete it has a limit in $A$.

    \item[\subcref{enum:sequentially-compact} $\implies$ \subcref{enum:complete-totally-bounded}]
    Assume that $A$ is sequentially compact. We first show that $A$ is complete, so let $(x_n)_{n\in\naturals}$ be a Cauchy sequence in $A$. This has a subsequence that converges to a point $x$ in $A$, so [lemma] implies that $(x_n)$ also converges to $x$.
    
    Now suppose that $A$ is not totally bounded, and let $\epsilon > 0$ be such that $A$ cannot be covered by finitely many $\epsilon$-balls. We construct a sequence $(x_n)_{n\in\naturals}$ in $A$ as follows: Choose any $x_1 \in A$, and given $x_1, \ldots, x_n$ choose $x_{n+1} \in A \setminus \bigunion_{i=1}^n B(x_i,\epsilon)$. Then $\rho(x_m,x_n) \geq \epsilon$ for all $m,n \in \naturals$ with $m \neq n$, so $(x_n)$ has no convergent subsequence.
    
    \item[\subcref{enum:complete-totally-bounded} \& \subcref{enum:sequentially-compact} $\implies$ \subcref{enum:compact}]
    Suppose that $A$ is complete, totally bounded and sequentially compact, and let $\calU$ be an open cover of $A$. It suffices to show that there some $\epsilon > 0$ such that any $\epsilon$-ball intersecting $A$ is contained in some $U \in \calU$, since $A$ can be covered by finitely many such balls.

    Assume towards a contradiction that for every $n \in \naturals$ there is a ball $B_n$ of radius $1/n$ intersecting $A$ such that $B_n$ is contained in no $U \in \calU$. Picking $x_n \in B_n$ for $n \in \naturals$, we may assume that the sequence $(x_n)_{n\in\naturals}$ converges to some $x \in A$ by passing to an appropriate subsequence. Then $x \in U$ for some $U \in \calU$, and since $U$ is open there is an $\epsilon > 0$ such that $B(x,\epsilon) \subseteq U$. Choosing $n \in \naturals$ large enough that $\rho(x_n,x) < \epsilon/2$ and $1/n < \epsilon/2$, we have $B_n \subseteq B(x,\epsilon) \subseteq U$, which is a contradiction.

    \item[\subcref{enum:compact} $\implies$ \subcref{enum:sequentially-compact}]
    We prove the contrapositive, so assume that $A$ is not sequentially compact, and let $(x_n)_{n\in\naturals}$ be a sequence in $A$ with no convergent subsequence. Every $x \in A$ is then contained in an open ball $B_x$ containing $x_n$ for only finitely many $n$. Thus $\{B_x\}_{x \in A}$ is an open cover of $A$ with no finite subcover, and $A$ is not compact.
\end{proofsec}
\end{proof}


% \chapter{Differentiation}

% \begin{definition}[Derivatives]
%     Let $I$ be an interval, and let $f \colon I \to \reals$. We say that $f$ is \emph{differentiable} at a point $a \in I$ if the limit
%     %
%     \begin{equation*}
%         \lim_{x \to a} \frac{f(x) - f(a)}{x-a}
%     \end{equation*}
%     %
%     exists. If so, the limit is called the \emph{derivative} of $f$ at $a$ and is denoted $f'(a)$ or $Df(a)$.
% \end{definition}

% \begin{lemma}
%     Let $f \colon I \to \reals$ and $a \in I$. Then the following are equivalent:
%     %
%     \begin{enumlem}
%         \item $f$ is differentiable at $a$.
        
%         \item There exists a function $\phi_a \colon I \to \reals$, continuous at $a$, such that
%         %
%         \begin{equation*}
%             f(x)
%                 = f(a) + \phi_a(x) (x-a)
%         \end{equation*}
%         %
%         for all $x \in I$.
        
%         \item There exists a number $A \in \reals$ and a function $\epsilon_a \colon U \to \reals$, where $U \subseteq \reals$ is an open neighbourhood of $0$, such that, for all $h \in \reals$ with $a + h \in I$,
%         %
%         \begin{equation*}
%             f(a+h)
%                 = f(a) + Ah + \epsilon_a(h),
%             \quad \text{and} \quad
%             \lim_{h \to 0} \frac{\epsilon_a(h)}{h} = 0. 
%         \end{equation*}
%     \end{enumlem}
%     %
%     In this case we have
% \end{lemma}


\chapter{Differentiation}

\begin{definition}[Differentiability]
    Let $A \subseteq \reals$, let $a \in A$ be a limit point\footnotemark{} of $A$, and let $f \colon A \to \reals$. If the limit
    %
    \begin{equation*}
        \lim_{x \to a} \frac{f(x) - f(a)}{x - a}
    \end{equation*}
    %
    exists and equals $L$, then we say that $f$ is \emph{differentiable} at $a$. The number $L$ is called the \emph{derivative} of $f$ at $a$ and is denoted $f'(a)$.
\end{definition}
\footnotetext{Recall that $a$ is a limit point of $A$ if every punctured neighbourhood of $a$ intersects $A$.}
%
\begin{remark}
    \label{rem:differentiability-reformulation}
    Notice that we may reformulate the definition of differentiability of $f$ at $a$ by instead requiring that the limit
    %
    \begin{equation*}
        \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}
    \end{equation*}
    %
    exists, when $h \in A-a$.
\end{remark}

\begin{lemma}[Hadamard's lemma]
    \label{lem:Hadamard-1D}
    Let $A \subseteq \reals$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals$. The following are equivalent:
    %
    \begin{enumlem}
        \item $f$ is differentiable at $a$.

        \item There exists a function $\phi = \phi_a \colon A \to \reals$, continuous at $a$, such that
        %
        \begin{equation}
            \label{eq:Hadamard-1D-phi}
            f(x) - f(a)
                = \phi_a(x) (x - a)
        \end{equation}
        %
        for all $x \in A$.

        \item There exists an $L(a) \in \reals$ and a function $\epsilon_a \colon A-a \to \reals$ such that
        %
        \begin{equation*}
            \lim_{h \to 0} \frac{\epsilon_a(h)}{h} = 0
            \quad \text{and} \quad
            f(a+h) - f(a)
                = L(a)h + \epsilon_a(h)
        \end{equation*}
        %
        for all $h \in A-a$.
    \end{enumlem}
    %
    If any of these conditions are satisfied, then
    %
    \begin{equation*}
        f'(a) = \phi_a(a) = L(a),
        \quad \text{and} \quad
        \phi_a(x)
            = \frac{\epsilon_a(x-a)}{x-a}
    \end{equation*}
    %
    for all $x \in A \setminus \{a\}$.
\end{lemma}
%
We will call the function $\phi = \phi_a$ an \emph{Hadamard function} for $f$ at $a$.

\begin{proof}
\begin{proofsec}
    \item[(i) $\implies$ (ii)]
    Define
    %
    \begin{equation*}
        \phi_a(x) =
        \begin{cases}
            \frac{f(x) - f(a)}{x - a}, & x \in A \setminus \{a\}, \\
            f'(a), & x = a.
        \end{cases}
    \end{equation*}
    %
    Then $\phi_a$ is continuous at $a$ and satisfies \cref{eq:Hadamard-1D-phi} for all $x \in A$.

    \item[(ii) $\implies$ (iii)]
    Let $L(a) = \phi_a(a)$ and
    %
    \begin{equation*}
        \epsilon_a(h)
            = \frac{\phi_a(a+h) + \phi_a(a)}{h}.
    \end{equation*}

    \item[(iii) $\implies$ (i)]
    This is clear from \cref{rem:differentiability-reformulation}.
\end{proofsec}
\end{proof}

[TODO diff implies continuous]

\begin{theorem}[The chain rule]
    Let $A,B \subseteq \reals$, and let $f \colon A \to \reals$ and $g \colon B \to \reals$ with $f(A) \subseteq B$. Let $a \in A$ be a limit point of $A$, and assume that $f(a) \in B$ is a limit point of $B$, that $f$ is differentiable at $a$ and that $g$ is differentiable at $f(a)$. Then $g \circ f$ is differentiable at $a$ with
    %
    \begin{equation*}
        (g \circ f)'(a)
            = g'(f(a)) f'(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $\phi$ be an Hadamard function for $f$ at $a$, and let $\psi$ be an Hadamard function for $g$ at $f(a)$. Hence
    %
    \begin{equation*}
        f(x) - f(a)
            = \phi(x)(x - a)
        \quad \text{and} \quad
        g(y) - g(f(a))
            = \psi(y)(y - f(a))
    \end{equation*}
    %
    for all $x \in A$ and $y \in B$. Letting $y = f(x)$ we thus have
    %
    \begin{equation*}
        g(f(x)) - g(f(a))
            = \psi(f(x))(f(x) - f(a))
            = \psi(f(x))\phi(x)(x - a).
    \end{equation*}
    %
    Notice that the map $x \mapsto \psi(f(x))\phi(x)$ is continuous at $a$, so it is an Hadamard function for $g \circ f$ at $a$. Thus \cref{lem:Hadamard-1D} implies that $g \circ f$ is differentiable at $a$ with
    %
    \begin{equation*}
        (g \circ f)'(a)
            = \psi(f(a)) \phi(a)
            = g'(f(a)) f'(a),
    \end{equation*}
    %
    as claimed.
\end{proof}


\begin{proposition}
    \label{prop:local-extrema-stationary}
    Let $f \colon (a,b) \to \reals$, and let $c \in (a,b)$. Assume that $f$ is differentiable at $c$ and attains a local extremum at $c$. Then $f'(c) = 0$.
\end{proposition}

\begin{proof}
    Assume for definiteness that $f$ has a local maximum at $c$, and choose $\delta > 0$ such that $f(x) \leq f(c)$ for $x \in (c - \delta, c + \delta)$. For $x \in (c - \delta, c)$ we have
    %
    \begin{equation*}
        \frac{f(x) - f(c)}{x - c} \geq 0,
    \end{equation*}
    %
    and letting $x \uparrow c$ we find that $f'(c) \geq 0$. By considering $x \in (c, c + \delta)$ we similarly find that $f'(c) \leq 0$ as desired.
\end{proof}


\begin{lemma}[Rolle's theorem]
    \label{lem:Rolle}
    Let $f \colon [a,b] \to \reals$ be a continuous function that is differentiable on $(a,b)$. If $f(a) = f(b)$, then there is a $c \in (a,b)$ such that $f'(c) = 0$.
\end{lemma}

\begin{proof}
    If $f$ is constant, then this is obvious. If $f$ is not constant, then since it is continuous it has a local extremum at some $c \in (a,b)$. By \cref{prop:local-extrema-stationary} we thus have $f'(c) = 0$ as desired.
\end{proof}

\begin{theorem}[The generalised mean value theorem]
    \label{thm:generalised-MVT}
    Let $f,g \colon [a,b] \to \reals$ be continuous functions that are differentiable on $(a,b)$. Then there exists a point $c \in (a,b)$ such that
    %
    \begin{equation*}
        \bigl( f(b) - f(a) \bigr) g'(c)
            = \bigl( g(b) - g(a) \bigr) f'(c).
    \end{equation*}
\end{theorem}

\begin{proof}
    Define $h \colon [a,b] \to \reals$ by $h(x) = (f(b) - f(a)) g(x) - (g(b) - g(a)) f(x)$, and notice that $h$ is continuous on $[a,b]$, differentiable on $(a,b)$, and that
    %
    \begin{equation*}
        h(a)
            = f(b)g(a) - g(b)f(a)
            = h(b).
    \end{equation*}
    %
    \Cref{lem:Rolle} thus implies that $h'(c) = 0$ for some $c \in (a,b)$. This proves the claim.
\end{proof}


\begin{corollary}[The mean value theorem]
    Let $f \colon [a,b] \to \reals$ be a continuous function that is differentiable on $(a,b)$. Then there is a $c \in (a,b)$ such that
    %
    \begin{equation*}
        f(b) - f(a)
            = (b-a) f'(c).
    \end{equation*}
\end{corollary}

\begin{proof}
    Let $g(x) = x$ in \cref{thm:generalised-MVT}.
\end{proof}

[TODO monotonicity]


\chapter{Integration}

\section{Functions of bounded variation}

\newcommand{\boundedvar}[1]{\mathit{BV}[#1]}
\newcommand{\integrable}[2][]{\calR_{#1}[#2]}

A \emph{partition} of an interval $[a,b]$ is a collection $P = \{x_0, \ldots, x_n \}$ of real numbers such that
%
\begin{equation*}
    a = x_0 < \cdots < x_n = b.
\end{equation*}
%
In turn, a \emph{tagged partition} of $[a,b]$ is a pair $(P,T)$ where $P$ is a partition of $[a,b]$ and $T = \{t_1, \ldots, t_n\}$ is a multiset of numbers such that $t_i \in [x_{i-1}, x_i]$ for all $i = 1, \ldots, n$. Let $\calP'[a,b]$ denote the set of tagged partitions of $[a,b]$. We define a direction on $\calP'[a,b]$ by $(P,T) \preceq (P',T')$ if $P \subseteq P'$. Notice that $T$ and $T'$ do not appear in this definition. This also induces a direction on the set $\calP[a,b]$ of all (non-tagged) partitions of $[a,b]$.

Given a partition $P = \{x_0, \ldots, x_n \}$ of $[a,b]$ and a function $f \colon [a,b] \to \reals$ we write $\Delta f_i = f(x_i) - f(x_{i-1})$ for $i = 1, \ldots, n$. We furthermore write $\Delta x_i = x_i - x_{i-1}$. Furthermore, define
%
\begin{equation*}
    \norm{P}
        = \max_{1 \leq i \leq n} \Delta x_i
    \quad \text{and} \quad
    \Sigma_f(P)
        = \sum_{i=1}^n \abs{\Delta f_i}.
\end{equation*}
%
The number $\norm{P}$ is called the \emph{norm} of $P$. Notice that the map $P \mapsto \norm{P}$ is decreasing, while the map $P \mapsto \Sigma_f(P)$ is increasing.

\begin{definition}[Total variation]
    Consider a function $f \colon [a,b] \to \reals$. The \emph{total variation} of $f$ on $[a,b]$ is the number
    %
    \begin{equation*}
        V_f(a,b)
            = \sup_{P \in \calP[a,b]} \Sigma_f(P).
    \end{equation*}
    %
    If $V_f(a,b) < \infty$, then we say that $f$ is of \emph{bounded variation} on $[a,b]$. The set of all functions that are of bounded variation on $[a,b]$ is denoted $\boundedvar{a,b}$.
\end{definition}
%
If $f$ is of bounded variation on $[a,b]$, then it is clear that $f$ is also of bounded variation on any subinterval of $[a,b]$. If $c \in (a,b)$ it is also easy to show that
%
\begin{equation}
    \label{eq:total-variation-additive}
    V_f(a,b)
        = V_f(a,c) + V_f(c,b).
\end{equation}
%
If $g \colon [a,b] \to \reals$ is another function of bounded variation on $[a,b]$ and $c \in \reals$, then it is clear from the definition that $f + g$ and $cf$ are also of bounded variation. It is also simple to show that the product $fg$ is of bounded variation. Hence $\boundedvar{a,b}$ is an $\reals$-algebra.

Also note that monotonic functions are of bounded variation on any compact interval.

\begin{lemma}
    \label{lem:total-variation-increasing}
    Let $f \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_f(a,x)$ for $x \in (a, b]$ and $V(a) = 0$. Then the functions $V$ and $V - f$ are increasing on $[a,b]$.
\end{lemma}

\begin{proof}
    The function $V$ is clearly increasing, so consider the function $D = V-f$. Let $x,y \in [a,b]$ with $x < y$, and notice that $f(y) - f(x) \leq V_f(x,y)$. Recalling \cref{eq:total-variation-additive} it follows that
    %
    \begin{equation*}
        D(y) - D(x)
            = V(y) - V(x) - (f(y) - f(x))
            = V_f(y,x) - (f(y) - f(x))
            \geq 0.
    \end{equation*}
\end{proof}


\begin{proposition}
    \label{prop:BV-difference-of-increasing-functions}
    A function $f \colon [a,b] \to \reals$ is of bounded variation if and only if it is the difference of two (strictly) increasing functions.
\end{proposition}

\begin{proof}
    By \cref{lem:total-variation-increasing} we can write $f$ as the difference of two increasing functions as $f = V - (V - f)$. Adding a strictly increasing function to both $V$ and $V - f$ yields the claim.
\end{proof}


\begin{proposition}
    \label{prop:total-variation-continuity}
    Let $f \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_f(a,x)$ for $x \in (a, b]$ and $V(a) = 0$. If $c \in [a,b]$, then $f$ is continuous at $c$ if and only if $V$ is continuous at $c$.
\end{proposition}

\begin{proof}
    We prove the claim in the case where $c$ is an inner point of $[a,b]$. First assume that $V$ is continuous at $c$. Since $V$ is monotonic by \cref{lem:total-variation-increasing}, the left- and right-hand limits $V(c-)$ and $V(c+)$ exist. For $x \in (c,b]$ we have
    %
    \begin{equation*}
        \abs{f(x) - f(c)}
            \leq V(x) - V(c)
            \xrightarrow[x \downarrow c]{}
            V(c+) - V(c)
            = 0,
    \end{equation*}
    %
    so $f$ is right-continuous at $c$. Similarly for left-continuity.

    We prove the converse, so assume that $f$ is continuous at $c$ and let $\epsilon > 0$. There exists a $\delta > 0$ such that $0 < \abs{x - c} < \delta$ implies $\abs{f(x) - f(c)} < \epsilon$. Further choose a partition $P = \{x_0, \ldots, x_n\}$ of $[c,b]$ such that
    %
    \begin{equation*}
        V_f(c,b) - \epsilon
            < \sum_{i=1}^n \abs{\Delta f_k}.
    \end{equation*}
    %
    This inequality is conserved when adding points to $P$, so we may assume that $x_1 - x_0 < \delta$, implying that $\abs{\Delta f_1} < \epsilon$. The inequality above then becomes
    %
    \begin{equation*}
        V_f(c,b) - \epsilon
            < \epsilon + \sum_{i=2}^n \abs{\Delta f_k}
            \leq \epsilon + V_f(x_1,b),
    \end{equation*}
    %
    so that $V_f(c,b) - V_f(x_1,b) < 2\epsilon$. This implies that
    %
    \begin{align*}
        0
            &\leq V(x_1) - V(c)
             = V_f(a,x_1) - V_f(a,c) \\
            &= V_f(c,x_1)
             = V_f(c,b) - V_f(x_1,b)
             < 2\epsilon,
    \end{align*}
    %
    showing that $V(c+) = V(c)$. A similar argument yields $V(c-) = V(c)$, so $V$ is continuous at $c$.
\end{proof}


\section{Integration}

Next consider two functions $f, \alpha \colon [a,b] \to \reals$. For each tagged partition $(P,T)$ of $[a,b]$ we define the \emph{Riemann--Stieltjes sum}
%
\begin{equation*}
    S_{f,\alpha}(P,T)
        = \sum_{i=1}^n f(t_i) \Delta\alpha_i.
\end{equation*}
%
This induces a net $S_{f,\alpha} \colon \calP'[a,b] \to \reals$.


\begin{definition}[Riemann--Stieltjes integral]
    Let $f,\alpha \colon [a,b] \to \reals$ be bounded functions. We say that $f$ is \emph{Riemann-integrable} with respect to $\alpha$ (or simply \emph{$\alpha$-integrable}) on $[a,b]$ if the net $S_{f,\alpha}$ has a limit $A \in \reals$. In this case $A$ is called the \emph{Riemann--Stieltjes integral} of $f$ with respect to $\alpha$ on $[a,b]$ and is denoted
    %
    \begin{equation*}
        \int_a^b f \dif \alpha
        \quad \text{or} \quad
        \int_a^b f(x) \dif \alpha(x).
    \end{equation*}
    %
    We denote the set of $\alpha$-integrable functions on $[a,b]$ by $\integrable[\alpha]{a,b}$.
\end{definition}
%
We call $f$ the \emph{integrand} and $\alpha$ the \emph{integrator}. In the case where $\alpha(x) = x$, we use the notations
%
\begin{equation*}
    S_f,
    \quad
    \int_a^b f
    \quad \text{and} \quad
    \int_a^b f(x) \dif x.
\end{equation*}
%
The sums $S_f$ are then simply called \emph{Riemann sums} and the integral the \emph{Riemann integral} of $f$ on $[a,b]$. With this choice of $\alpha$, an $\alpha$-integrable function is called \emph{Riemann integrable} on $[a,b]$, and the set of such functions is denoted $\integrable{a,b}$.

\begin{remark}
    In the ordinary Riemann integral it is not necessary to assume that $f$ is bounded, since integrability in this case implies boundedness. We claim that this assumption can be lifted whenever the integrator $\alpha$ is injective. In fact, we only need to assume that there is a partition $Q$ of $[a,b]$ such that $\alpha$ is injective on each subinterval of $Q$.
    
    If the net $S_{f,\alpha}$ has a limit $A \in \reals$, then there is a tagged partition $(P,T)$ of $[a,b]$ such that
    %
    \begin{equation*}
        \abs{A - S_{f,\alpha}(P',T')} < 1,
        \quad \text{implying that} \quad
        \abs{S_{f,\alpha}(P',T')} < M \defn \abs{A} + 1,
    \end{equation*}
    %
    whenever $(P,T) \preceq (P',T')$. By replacing $P$ with $P \union Q$ we may assume that $\alpha$ is injective on each subinterval of $P$, since $\alpha$ is clearly also injective on each subinterval of $P \union Q$.
    
    Write $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Let $x \in [x_{k-1},x_k]$ and consider the multiset $T_x = \{t_1, \ldots, t_{k-1}, x, t_{k+1}, \ldots, t_n\}$ and the corresponding tagged partition $(P,T_x)$. We then have
    %
    \begin{equation*}
        f(x) \Delta \alpha_k
            = S_{f,\alpha}(P,T_x) - \sum_{i \neq k} f(t_i) \Delta \alpha_i.
    \end{equation*}
    %
    Since $\alpha$ is injective on $[x_{k-1},x_k]$ we have $\Delta\alpha_k \neq 0$, so the above implies that
    %
    \begin{equation*}
        \abs{f(x)}
            = \frac{1}{\abs{\Delta \alpha_k}} \abs[\bigg]{ S_{f,\alpha}(P,T_x) - \sum_{i \neq k} f(t_i) \Delta \alpha_i }
            < \frac{1}{\abs{\Delta \alpha_k}} \biggl( M + \abs[\bigg]{ \sum_{i \neq k} f(t_i) \Delta \alpha_i } \biggr),
    \end{equation*}
    %
    where the inequality follows since $(P,T) \preceq (P,T_x)$ for all $x \in [x_{k-1},x_k]$. The right-hand side is thus as upper bound for $\abs{f}$ on $[x_{k-1},x_k]$, and there are finitely many such intervals, so $f$ is bounded on $[a,b]$ as claimed.
\end{remark}

Below we fix an interval $[a,b]$ and (bounded) integrators $\alpha$ and $\beta$ on it.

\begin{proposition}[Linearity of the integral]
    \label{prop:integral-linearity}
    Let $f,g \in \integrable[\alpha]{a,b}$ and $c_1, c_2 \in \reals$. Then:
    %
    \begin{enumprop}
        \item $c_1 f + c_2 g$ is $\alpha$-integrable on $[a,b]$ and
        %
        \begin{equation*}
            \int_a^b (c_1 f + c_2 g) \dif\alpha
                = c_1 \int_a^b f \dif\alpha + c_2 \int_a^b g \dif\alpha.
        \end{equation*}
        %
        In particular, $\integrable[\alpha]{a,b}$ is a vector space.

        \item $f$ is $(c_1 \alpha + c_2 \beta)$-integrable on $[a,b]$ and
        %
        \begin{equation*}
            \int_a^b f \dif(c_1 \alpha + c_2 \beta)
                = c_1 \int_a^b f \dif\alpha + c_2 \int_a^b f \dif\beta.
        \end{equation*}
    \end{enumprop}
\end{proposition}

\begin{proof}
    This follows immediately from the bilinearity of the map $(f,\alpha) \mapsto S_{f,\alpha}$ along with basic properties of nets.
\end{proof}


\begin{proposition}
    \label{prop:integral-dividing-interval}
    Consider $f, \alpha \colon [a,b] \to \reals$ and let $c \in (a,b)$. If two of the three integrals in \cref{eq:integral-dividing-interval} exist, then so does the third and we have
    %
    \begin{equation}
        \label{eq:integral-dividing-interval}
        \int_a^c f \dif\alpha + \int_c^b f \dif\alpha
            = \int_a^b f \dif\alpha.
    \end{equation}
\end{proposition}

\begin{proof}
    Let $(P,T)$ be a tagged partition of $[a,b]$ such that $c \in P$, and let $P_1 = P \intersect [a,c]$ and $P_2 = P \intersect [c,b]$, and $T_1 = T \intersect [a,c]$ and $T_2 = T \intersect [c,b]$. Then
    %
    \begin{equation}
        \label{eq:Riemann-sums-dividing-interval}
        S_{f,\alpha}(P,T)
            = S_{f,\alpha}(P_1,T_1) + S_{f,\alpha}(P_2,T_2).
    \end{equation}
    %
    If two of the three integrals in \cref{eq:integral-dividing-interval} exist, then two of the nets in \cref{eq:Riemann-sums-dividing-interval} converge to the respective integrals. The third net then also converges to the relevant integral: Notice that we assume without loss of generality that $c \in P$, since adding $c$ to a partition simply yields a finer partition. Also notice that any partition $P'$ of e.g. $[a,c]$ is on the form $P' = P \intersect [a,c]$ for some partition $P$ of $[a,b]$.
\end{proof}


\begin{proposition}[Integration by parts]
    \label{prop:integration-by-parts}
    Given functions $f,\alpha \colon [a,b] \to \reals$, assume that $f \in \integrable[\alpha]{a,b}$. Then $\alpha \in \integrable[f]{a,b}$ and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha + \int_a^b \alpha \dif f
            = f(b)\alpha(b) - f(a)\alpha(a).
    \end{equation*}
    %
    In particular, exchanging $f$ and $\alpha$ we have
    %
    \begin{equation*}
        f \in \integrable[\alpha]{a,b}
            \quad \text{if and only if} \quad
            \alpha \in \integrable[f]{a,b}.
    \end{equation*}
\end{proposition}

\begin{proof}
    Let $(P,T)$ be a tagged partition of $[a,b]$ with $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Writing $A = f(b)\alpha(b) - f(a)\alpha(a)$, notice that
    %
    \begin{equation*}
        A
            = \sum_{i=1}^n f(x_i) \alpha(x_i) - \sum_{i=1}^n f(x_{i-1}) \alpha(x_{i-1}),
    \end{equation*}
    %
    and that
    %
    \begin{equation*}
        S_{\alpha,f}(P,T)
            = \sum_{i=1}^n \alpha(t_i) \Delta f_i
            = \sum_{i=1}^n f(x_i) \alpha(t_i) - \sum_{i=1}^n f(x_{i-1}) \alpha(t_i).
    \end{equation*}
    %
    Hence we have
    %
    \begin{align*}
        A - S_{\alpha,f}(P,T)
            &= \sum_{i=1}^n f(x_i) (\alpha(x_i) - \alpha(t_i)) + \sum_{i=1}^n f(x_{i-1}) (\alpha(t_i) - \alpha(x_{i-1})) \\
            &= S_{f,\alpha}(P \union T, P'),
    \end{align*}
    %
    where $P'$ is obtained from $P$ by duplicating\footnote{Recall that if $(P,T)$ is a tagged partition, then $T$ is a \emph{multiset}.} appropriate elements such that each subinterval of $P \union T$ contains the corresponding element from $P'$. Notice that if $P$ and $T$ have any elements in common, these are not duplicated in the union $P \union T$. However, in this case the corresponding terms in the sum above vanish, so the last equality does in fact hold. Since $P \union T$ is finer than $P$, the claim follows by taking the limit of $S_{\alpha,f}$.
\end{proof}


\begin{proposition}[Change of variables in Riemann--Stieljes integrals]
    Let $f \in \integrable[\alpha]{a,b}$, and let $\phi \colon I \to [a,b]$ be a monotonic (or equivalently continuous) bijection where $I$ is an interval with endpoints $c$ and $d$. Assume that $a = \phi(c)$ and $b = \phi(d)$. Then $f \circ \phi \in \integrable[\alpha \circ \phi]{c,d}$ and
    %
    \begin{equation*}
        \int_a^b f \dif \alpha
            = \int_c^d f \circ \phi \dif(\alpha \circ \phi).
    \end{equation*}
\end{proposition}

\begin{proof}
    Since $\phi$ is bijective it is strictly monotonic, so it induces an order isomorphism $\calP'(I) \to \calP'[a,b]$ given by $(P, T) \mapsto (\phi(P), \phi(T))$.

    Now let $(P,T) \in \calP'(I)$ with $P = \{y_0, \ldots, y_n\}$ and $T = \{s_1, \ldots, s_n\}$. Assume for definiteness that $\phi$ is increasing so that $I = [c,d]$, and write $x_i = \phi(y_i)$ and $t_i = \phi(s_i)$. Then we have
    %
    \begin{equation*}
        S_{f \circ \phi, \alpha \circ \phi}(P,T)
            = \sum_{i=1}^n (f \circ \phi)(s_i) \Delta(\alpha \circ \phi)_i
            = \sum_{i=1}^n f \circ \phi(t_i) \Delta \alpha_i
            = S_{f,\alpha}(\phi(P),\phi(T)).
    \end{equation*}
    %
    Since the map $(P,T) \mapsto (\phi(P),\phi(T))$ is an order isomorphism, each side above converges to the corresponding integral, proving the claim.
\end{proof}


\begin{proposition}[Reduction to a Riemann integral]
    Let $\alpha \in C^1[a,b]$ and $f \in \integrable[\alpha]{a,b}$. Then $f \alpha' \in \integrable{a,b}$, and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \int_a^b f \alpha'.
    \end{equation*}
\end{proposition}

\begin{proof}
    Consider the Riemann(--Stieltjes) sums
    %
    \begin{equation*}
        S_{f\alpha'}(P,T)
            = \sum_{i=1}^n f(t_i) \alpha'(t_i) \Delta x_i
        \quad \text{and} \quad
        S_{f,\alpha}(P,T)
            = \sum_{i=1}^n f(t_i) \Delta \alpha_i.
    \end{equation*}
    %
    By the mean value theorem we can write $\Delta \alpha_i = \alpha'(s_i) \Delta x_i$ for appropriate $s_i \in (x_{i-1}, x_i)$. It follows that
    %
    \begin{equation*}
        S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T)
            = \sum_{i=1}^n f(t_i) (\alpha'(s_i) - \alpha'(t_i)) \Delta x_i.
    \end{equation*}
    %
    By uniform continuity of $\alpha'$, given $\epsilon > 0$ there exists a $\delta > 0$ such that $\abs{x-y} < \delta$ implies $\abs{\alpha'(x) - \alpha'(y)} < \epsilon$ for all $x,y \in [a,b]$. Choosing a tagged partition $(P,T)$ with $\norm{P} < \delta$ we thus have
    %
    \begin{equation*}
        \abs{S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T)}
            \leq \norm{f}_{\sup} \epsilon (b-a).
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, the left-hand side converges to zero. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ \int_a^b f \dif\alpha - S_{f\alpha'}(P,T) }
            \leq \abs[\bigg]{ \int_a^b f \dif\alpha - S_{f,\alpha}(P,T) } + \abs{ S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T) },
    \end{equation*}
    %
    which converges to zero. Hence $S_{f\alpha'}(P,T)$ converges, so $f\alpha' \in \integrable{a,b}$, and its integral equals the $\alpha$-integral of $f$ as claimed.
\end{proof}


\section{Increasing integrators}

\begin{definition}
    Let $f, \alpha \colon [a,b] \to \reals$ be bounded functions, and assume that $\alpha$ is increasing. Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$, and let
    %
    \begin{align*}
        M_i(f)
            &= \sup \set[\big]{f(x)}{x \in [x_{i-1}, x_i]}, \\
        m_i(f)
            &= \inf \set[\big]{f(x)}{x \in [x_{i-1}, x_i]}.
    \end{align*}
    %
    The numbers
    %
    \begin{equation*}
        U_{f,\alpha}(P)
            = \sum_{i=1}^n M_i(f) \Delta \alpha_i
        \quad \text{and} \quad
        L_{f,\alpha}(P)
            = \sum_{i=1}^n m_i(f) \Delta \alpha_i
    \end{equation*}
    %
    are called the \emph{upper and lower Stieltjes sums} of $f$ with respect to $\alpha$ for the partition $P$.
\end{definition}
%
Since $\alpha$ is increasing we have $\Delta\alpha_i \geq 0$, so it is immediate that
%
\begin{equation*}
    L_{f,\alpha}(P)
        \leq S_{f,\alpha}(P,T)
        \leq U_{f,\alpha}(P)
\end{equation*}
%
for any tagged partition $(P,T)$ of $[a,b]$. It is also clear that, if $P \subseteq P'$, then
%
\begin{equation}
    \label{eq:upper-lower-sum-decreasing-increasing}
    U_{f,\alpha}(P) \geq U_{f,\alpha}(P')
    \quad \text{and} \quad
    L_{f,\alpha}(P) \leq L_{f,\alpha}(P'),
\end{equation}
%
and that for any pair of partitions $P_1$ and $P_2$ we have
%
\begin{equation}
    \label{eq:upper-lower-sum-inequality}
    L_{f,\alpha}(P_1) \leq U_{f,\alpha}(P_2).
\end{equation}


% https://tex.stackexchange.com/questions/44237/lower-and-upper-riemann-integrals
% Have changed lowint and the first one in upint -- need to adjust the rest in upint if I want to use them, also make versions of lowint if I want to use those
\def\upint{\mathchoice%
    {\mkern10mu\overline{\vphantom{\intop}\mkern10mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern2mu\underline{\vphantom{\intop}\mkern10mu}\mkern-12mu\int}

\begin{definition}
    Let $f, \alpha \colon [a,b] \to \reals$ be bounded functions with $\alpha$ increasing. Then the numbers
    %
    \begin{equation*}
        \upint_a^b f \dif\alpha
            = \inf \set[\big]{U_{f,\alpha}(P)}{P \in \calP[a,b]}
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \lowint_a^b f \dif\alpha
            = \sup \set[\big]{L_{f,\alpha}(P)}{P \in \calP[a,b]}
    \end{equation*}
    %
    are called the \emph{upper and lower Stieltjes integrals} of $f$ with respect to $\alpha$ on $[a,b]$.
\end{definition}
%
We also use the notations $\overline{I}(f,\alpha)$ and $\underline{I}(f,\alpha)$ for the upper and lower integrals, respectively, when the interval $[a,b]$ is understood. It follows immediately from the definition and \cref{eq:upper-lower-sum-inequality} that $\underline{I}(f,\alpha) \leq \overline{I}(f,\alpha)$. 


\begin{theorem}[Riemann's condition]
    Let $f,\alpha \colon [a,b] \to \reals$ be bounded functions with $\alpha$ increasing. Then the following conditions are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:integrability} $f \in \integrable[\alpha]{a,b}$.
        \item \label{enum:Riemanns-condition} $f$ satisfies \emph{Riemann's condition} with respect to $\alpha$ on $[a,b]$: For every $\epsilon > 0$ there exists a partition $P$ of $[a,b]$ such that
        %
        \begin{equation}
            \label{eq:Riemanns-condition}
            U_{f,\alpha}(P) - L_{f,\alpha}(P) < \epsilon.
        \end{equation}
        \item \label{enum:upper-lower-integrals-equal} $\underline{I}(f,\alpha) = \overline{I}(f,\alpha)$.
    \end{enumthm}
    %
    In this case we have
    %
    \begin{equation*}
        \lowint_a^b f \dif\alpha
            = \int_a^b f \dif\alpha
            = \upint_a^b f \dif\alpha.
    \end{equation*}
\end{theorem}

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:integrability} $\implies$ \subcref{enum:Riemanns-condition}]
    Let $\epsilon > 0$, and choose a partition $P = \{x_0, \ldots, x_n\}$ of $[a,b]$ such that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{i=1}^n f(t_i) \Delta\alpha_i - \int_a^b f \dif\alpha }
        < \epsilon
    \end{equation*}
    %
    for all $t_i \in [x_{i-1},x_i]$. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i }
        < 2 \epsilon
    \end{equation*}
    %
    for all $t_i, t_i' \in [x_{i-1},x_i]$. For any $\delta > 0$ there exist $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i) - f(t_i')
        > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    From this it follows that
    %
    \begin{align*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P)
        &= \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i \\
        &< \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i + \delta (\alpha(b) - \alpha(a)) \\
        &< 3\epsilon
    \end{align*}
    %
    for an appropriate choice of $\delta$. Since $\epsilon$ was arbitrary, this proves \subcref{enum:Riemanns-condition}.
    
    \item[\subcref{enum:Riemanns-condition} $\implies$ \subcref{enum:upper-lower-integrals-equal}]
    If $P$ is any partition of $[a,b]$ we have
    %
    \begin{equation*}
        L_{f,\alpha}(P)
        \leq \lowint_a^b f \dif\alpha
        \leq \upint_a^b f \dif\alpha
        \leq U_{f,\alpha}(P).
    \end{equation*}
    %
    Thus \cref{eq:Riemanns-condition} implies that $0 \leq \overline{I}(f,\alpha) - \underline{I}(f,\alpha) < \epsilon$ for every $\epsilon > 0$, proving \subcref{enum:upper-lower-integrals-equal}.
    
    \item[\subcref{enum:upper-lower-integrals-equal} $\implies$ \subcref{enum:integrability}]
    Let $\epsilon > 0$. There exists a partition $P$ of $[a,b]$ such that
    %
    \begin{equation*}
        \underline{I}(f,\alpha) - \epsilon
        < L_{f,\alpha}(P)
        \leq S_{f,\alpha}(P,T)
        \leq U_{f,\alpha}(P)
        < \overline{I}(f,\alpha) + \epsilon
    \end{equation*}
    %
    for any choice of points $T$ such that $(P,T)$ is a tagged partition. Denoting the common value of $\underline{I}(f,\alpha)$ and $\overline{I}(f,\alpha)$ by $A$, this shows that $\abs{S_{f,\alpha}(P',T') - A} < \epsilon$ for all tagged partitions $(P',T')$ with $P \subseteq P'$. Hence $f \in \integrable[\alpha]{a,b}$, and the integral of $f$ with respect to $\alpha$ equals $A$.
\end{proofsec}
\end{proof}


\section{Mean value theorems}

\begin{proposition}[The first mean value theorem]
    \label{prop:mean-value-1}
    Let $\alpha \colon [a,b] \to \reals$ be increasing, and let $f \in \integrable[\alpha]{a,b}$. Let $m = \inf_{x \in [a,b]} f(x)$ and $M = \sup_{x \in [a,b]} f(x)$. Then there exists a $c \in \reals$ with $m \leq c \leq M$ such that
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = c \int_a^b \dif\alpha
            = c (\alpha(y) - \alpha(x)).
    \end{equation*}
\end{proposition}

\begin{proof}
    If $\alpha(a) = \alpha(b)$ then both sides are zero, so assume that $\alpha(a) < \alpha(b)$. From the inequalities
    %
    \begin{equation*}
        m (\alpha(b) - \alpha(a))
            \leq \int_a^b f \dif\alpha
            \leq M (\alpha(b) - \alpha(a))
    \end{equation*}
    %
    follow that
    %
    \begin{equation*}
        m 
            \leq \frac{1}{\alpha(b) - \alpha(a)} \int_a^b f \dif\alpha
            \leq M,
    \end{equation*}
    %
    so defining $c$ as the middle term, the claim follows.
\end{proof}

% [TODO MVT2?]


\section{Integrators of bounded variation}

\begin{theorem}
    \label{thm:alpha-integrable-implies-V-integrable}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_\alpha(a,x)$ for $x \in (a,b]$ and $V(a) = 0$. Then $\integrable[\alpha]{a,b} \subseteq \integrable[V]{a,b}$.
\end{theorem}

\begin{proof}
    Let $f \in \integrable[\alpha]{a,b}$, and choose $M > 0$ such that $\abs{f} \leq M$. Choose a partition $P = \{x_0, \ldots, x_n\}$ of $[a,b]$ such that $V(b) < \sum_{i=1}^n \abs{\Delta\alpha_i} + \epsilon$. Then
    %
    \begin{align*}
        \sum_{i=1}^n (M_i(f) - m_i(f)) ( \Delta V_i - \abs{\Delta\alpha_i})
            &\leq 2M \sum_{i=1}^n (\Delta V_i - \abs{\Delta\alpha_i}) \\
            &= 2M \biggl( V(b) - \sum_{i=1}^n \abs{\Delta\alpha_i} \biggr) \\
            &< 2M \epsilon.
    \end{align*}
    %
    Also choose $P$ such that $\abs{ \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i } < \epsilon$ for all $t_i, t_i' \in [x_{i-1}, x_i]$. Next let $\delta > 0$. For $i = 1, \ldots, n$, if $\Delta\alpha_i \geq 0$ choose $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i) - f(t_i')
            > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    If instead $\Delta\alpha_i < 0$, choose $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i') - f(t_i)
            > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    It follows that
    %
    \begin{equation*}
        \sum_{i=1}^n (M_i(f) - m_i(f)) \abs{\Delta\alpha_i}
            < \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i
              + \delta V(b)
            < 2 \epsilon
    \end{equation*}
    %
    for an appropriate choice of $\delta$. Combining these inequalities yields
    %
    \begin{equation*}
        U_{f,V}(P) - L_{f,V}(P)
            = \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta V_i
            < 2(M+1)\epsilon,
    \end{equation*}
    %
    and since $\epsilon$ was arbitrary, this shows that $f \in \integrable[V]{a,b}$.
\end{proof}
%
Since $\alpha = V - (V - \alpha)$ and both $V$ and $V - \alpha$ are increasing, this allows us to reduce questions about integrators of bounded variation to questions about monotonic integrators. In particular it lets us use Riemann's condition to prove integrability with respect to integrators of bounded variation.


\begin{corollary}
    \label{cor:integrable-on-subinterval}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Then $f \in \integrable[\alpha]{c,d}$ for every subinterval $[c,d]$ of $[a,b]$.
\end{corollary}

\begin{proof}
    By \cref{thm:alpha-integrable-implies-V-integrable} and \cref{prop:integral-linearity}, it suffices to prove the claim when $\alpha$ is increasing. Furthermore, by \cref{prop:integral-dividing-interval} it suffices to show that $f$ is $\alpha$-integrable on $[a,x]$ for all $x \in (a,b]$. Given $\epsilon > 0$, by \cref{enum:Riemanns-condition} there is a partition $P$ of $[a,b]$ such that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P) < \epsilon.
    \end{equation*}
    %
    By \cref{eq:upper-lower-sum-decreasing-increasing}, adjoining $x$ to $P$ preserves the inequality. Writing $P = \{x_0, \ldots, x_n\}$ we may thus assume that $x = x_k$ for some $k \in \{1, \ldots, n\}$. Letting $P' = P \intersect [a,x]$ we thus have
    %
    \begin{align*}
        U_{f,\alpha}(P') - L_{f,\alpha}(P')
            &= \sum_{i=1}^k (M_i(f) - m_i(f)) \Delta\alpha_i
             \leq \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i \\
            &= U_{f,\alpha}(P) - L_{f,\alpha}(P)
             < \epsilon,
    \end{align*}
    %
    where the first inequality follows since every term in the second sum is non-negative. Thus $f$ is integrable on $[a,x]$.
\end{proof}


\begin{proposition}
    Let $f,\alpha \colon [a,b] \to \reals$ be functions with $f$ continuous and $\alpha$ of bounded variation. Then $f$ is $\alpha$-integrable, and $\alpha$ is $f$-integrable.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $\epsilon > 0$. Uniform continuity of $f$ furnishes a $\delta < 0$ such that $\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} < \epsilon$ for $x,y \in [a,b]$. Let $P = \{x_0,\ldots,x_n\}$ be a partition with $\norm{P} < \delta$. Then $M_i(f) - m_i(f) \leq \epsilon$, implying that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P)
            = \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i
            \leq \epsilon (\alpha(b) - \alpha(a)),
    \end{equation*}
    %
    and since $\epsilon$ was arbitrary, it follows from Riemann's condition that $f \in \integrable[\alpha]{a,b}$. The final claim follows from \cref{prop:integration-by-parts}.
\end{proof}

\fleuronbreak

For functions $f \colon [a,b] \to \reals$ and $g \colon f([a,b]) \to \reals$, when is the composition $g \circ f$ integrable? In the Lebesgue theory all that is required is that $f$ and $g$ are measurable, but as far as I know, no such result is available for the Riemann integral. However, in the case where $g$ is continuous, \cref{prop:continuity-integrability} below shows that $g \circ f$ is indeed integrable. We first prove this and then prove a couple of important corollaries.

\begin{proposition}
    \label{prop:continuity-integrability}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Choose $m,M \in \reals$ such that $m \leq f \leq M$. If $\phi \colon [m,M] \to \reals$ is continuous, then $\phi \circ f \in \integrable[\alpha]{a,b}$.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Put $g = \phi \circ f$ and let $\epsilon > 0$. Uniform continuity of $\phi$ yields a $\delta > 0$ such that $\abs{s-t} < \delta$ implies $\abs{\phi(s) - \phi(t)} < \epsilon$ for $s,t \in [m,M]$. Also choose $\delta$ such that $\delta \leq \epsilon$. Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$ such that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P) < \delta^2.
    \end{equation*}
    %
    Let $A$ consist of those numbers $i \in \{1, \ldots, n\}$ such that $M_i(f) - m_i(f) < \delta$, and let $B$ consist of the remaining $i$. For $i \in A$ we then have $M_i(g) - m_i(g) \leq \epsilon$.

    Let $K > 0$ be such that $\abs{\phi} \leq K$. For $i \in B$ we then have $M_i(g) - m_i(g) \leq 2K$. Furthermore, we have
    %
    \begin{equation*}
        \sum_{i \in B} \Delta\alpha_i
            \leq \frac{1}{\delta} \sum_{i \in B} (M_i(f) - m_i(f)) \Delta\alpha_i
            \leq \frac{1}{\delta} \bigl( U_{f,\alpha}(P) - L_{f,\alpha}(P) \bigr)
            < \delta.
    \end{equation*}
    %
    It thus follows that
    %
    \begin{align*}
        U_{g, \alpha}(P) - L_{g, \alpha}(P)
            &= \sum_{i \in A} (M_i(g) - m_i(g)) \Delta\alpha_i
               + \sum_{i \in B} (M_i(g) - m_i(g)) \Delta\alpha_i \\
            &\leq \epsilon (\alpha(b) - \alpha(a))
               + 2K \delta \\
            &\leq (\alpha(b) - \alpha(a) + 2K) \epsilon.
    \end{align*}
    %
    Since $\epsilon$ was arbitrary, it follows that $g \in \integrable[\alpha]{a,b}$.
\end{proof}


For any function $f \colon X \to \reals$, recall that the \emph{positive and negative parts} of $f$ are the functions $f^+ = f \join 0$ and $f^- = -(f \meet 0)$ respectively, and that $f = f^+ - f^-$. We notice that $f^+$ and $f^-$ are both non-negative.

\begin{corollary}
    \label{cor:positive-negative-part}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Then a function $f \colon [a,b] \to \reals$ is $\alpha$-integrable if and only if $f^+$ and $f^-$ are, in which case
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \int_a^b f^+ \dif\alpha - \int_a^b f^- \dif\alpha.
    \end{equation*}
\end{corollary}

\begin{proof}
    This follows immediately from \cref{prop:continuity-integrability}, since the maps $x \mapsto x \join 0$ and $x \mapsto -(x \meet 0)$ are continuous.
\end{proof}


\begin{corollary}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f,g \in \integrable[\alpha]{a,b}$. Then the functions $\abs{f}$ and $fg$ are also $\alpha$-integrable. In particular, $\integrable[\alpha]{a,b}$ is an $\reals$-algebra.
    
    If $\alpha$ is increasing we also have
    %
    \begin{equation}
        \label{eq:integral-triangle-inequality}
        \abs[\bigg]{ \int_a^b f \dif\alpha }
            \leq \int_a^b \abs{f}\dif\alpha.
    \end{equation}
\end{corollary}
%
Recall from \cref{prop:integral-linearity} that $\integrable[\alpha]{a,b}$ is always a vector space. 

\begin{proof}
    Integrability of $\abs{f}$ follows from \cref{prop:continuity-integrability} since $x \mapsto \abs{x}$ is continuous. The inequality \cref{eq:integral-triangle-inequality} follows since $f \leq \abs{f}$, and since the $\alpha$-integral is increasing when $\alpha$ is.

    For the product $fg$, notice that
    %
    \begin{equation*}
        2fg = (f+g)^2 - f^2 - g^2,
    \end{equation*}
    %
    and that the function $x \mapsto x^2$ is continuous.
\end{proof}


\section{The fundamental theorems of calculus}

\begin{theorem}[The first fundamental theorem of calculus]
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Define a function $F \colon [a,b] \to \reals$ by
    %
    \begin{equation*}
        F(x)
            = \int_a^x f \dif\alpha.
    \end{equation*}
    %
    Then the following hold:
    %
    \begin{enumthm}
        \item \label{enum:integral-is-BV} $F$ is of bounded variation.
        \item \label{enum:continuity-of-integral} Every point of continuity of $\alpha$ is also a point of continuity of $F$.
        \item \label{enum:derivative-of-integral} Assume that $\alpha$ is increasing. If $f$ is continuous and $\alpha$ differentiable at $x \in (a,b)$, then $F$ is differentiable at $x$ with $F'(x) = f(x) \alpha'(x)$.
    \end{enumthm}
\end{theorem}
%
Notice that the integral above exists by \cref{cor:integrable-on-subinterval}, and so do the integrals in the proof below.

% [TODO (ii) requires that alpha continuous implies V continuous!]

\begin{proof}
\begin{proofsec}
    \item[Proof of \subcref{enum:integral-is-BV}]
    Since $\boundedvar{a,b}$ is a vector space, we may assume that $\alpha$ is increasing. By \cref{cor:positive-negative-part} we may further assume that $f$ is positive. Then $F$ is increasing, hence of bounded variation.\footnote{E.g. Apostol [TODO ref] attempt to prove this using \cref{prop:mean-value-1}, but his argument is not, as far as I can tell, correct as stated. Hence we use a different argument, more in the spirit of Lebesgue.}
    
    \item[Proof of \subcref{enum:continuity-of-integral}]
    By \cref{prop:total-variation-continuity} we may assume that $\alpha$ is increasing. Let $x,y \in [a,b]$ with $x \neq y$, and let $I$ denote the closed interval between $x$ and $y$. \Cref{prop:mean-value-1} now furnishes a $c_{xy} \in \reals$ with
    %
    \begin{equation*}
        \inf_{t \in [a,b]} f(t)
            \leq \inf_{t \in I} f(t)
            \leq c_{xy}
            \leq \sup_{t \in I} f(t)
            \leq \sup_{t \in [a,b]} f(t),
    \end{equation*}
    %
    such that
    %
    \begin{equation}
        \label{eq:fundamental-theorem-difference}
        F(y) - F(x)
            = \int_x^y f \dif\alpha
            = c_{xy} (\alpha(y) - \alpha(x)).
    \end{equation}
    %
    Assume now that $\alpha$ is continuous at $x$. Since $y \mapsto c_{xy}$ is bounded on $[a,b]$ (since $f$ is), letting $y \to x$ thus yields continuity of $F$ at $x$.

    \item[Proof of \subcref{enum:derivative-of-integral}]
    If $f$ is continuous at $x$, then $c_{xy} \to f(x)$ as $y \to x$, since $c_{xy}$ is bounded by the supremum and infimum of $f$ in a neighbourhood of $x$. Now \cref{eq:fundamental-theorem-difference} implies that
    %
    \begin{equation*}
        \frac{F(y) - F(x)}{y-x}
            = c_{xy} \frac{\alpha(y) - \alpha(x)}{y-x}
            \xrightarrow[y \to x]{}
            f(x) \alpha'(x),
    \end{equation*}
    %
    as desired.
\end{proofsec}
\end{proof}


\begin{remark}
    In the case $\alpha(x) = x$, \subcref{enum:derivative-of-integral} has a proof that does not use \cref{prop:mean-value-1}: Simply note that
    %
    \begin{equation*}
        \frac{F(y)-F(x)}{y-x} - f(x)
            = \frac{1}{y-x} \int_x^y (f(t) - f(x)) \dif t,
    \end{equation*}
    %
    and notice that the integrand can be made less than any $\epsilon > 0$ if $\abs{t-x} < \delta$ for an appropriate $\delta > 0$. I am not sure that this proof can be generalised.
\end{remark}


\begin{theorem}[The second fundamental theorem of calculus]
    Let $f \in \integrable{a,b}$. If there exists a continuous function $F \colon [a,b] \to \reals$ that is differentiable on $(a,b)$ with $F' = f$, then
    %
    \begin{equation*}
        \int_a^b f
            = F(b) - F(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$. The mean value theorem furnishes points $t_i \in (x_{i-1}, x_i)$ such that $\Delta F_i = F'(t_i) \Delta x_i = f(t_i) \Delta x_i$. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ F(b) - F(a) - \int_a^b f }
            = \abs[\bigg]{ \sum_{i=1}^n f(t_i)\Delta x_i - \int_a^b f }
            < \epsilon
    \end{equation*}
    %
    if $P$ is fine enough. Since $\epsilon$ was arbitrary, this proves the theorem.
\end{proof}


\section{Limit and continuity theorems}


\begin{proposition}
    \label{thm:integral-continuity}
    Let $f \colon [a,b] \times [c,d] \to \reals$ be continuous, and let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Then the function $F \colon [c,d] \to \reals$ given by
    %
    \begin{equation*}
        F(y)
            = \int_a^b f(x,y) \dif\alpha(x)
    \end{equation*}
    %
    is continuous.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. By uniform continuity of $f$, given $\epsilon > 0$ there is a $\delta > 0$ such that $\norm{z - z'} < \delta$ implies $\abs{f(z) - f(z')} < \epsilon$ for $z,z' \in [a,b] \times [c,d]$. Given $y,y' \in [c,d]$ with $\abs{y - y'} < \delta$ we thus have
    %
    \begin{equation*}
        \abs{F(y) - F(y')}
            \leq \int_a^b \abs{f(x,y) - f(x,y')} \dif\alpha(x)
            \leq \epsilon(\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, this shows that $F$ is continuous.
\end{proof}


\begin{proposition}
    Let $f \colon [a,b] \times [c,d] \to \reals$ be bounded, and let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Assume that $f(\,\cdot\,,y) \in \integrable[\alpha]{a,b}$ for all $y \in [c,d]$, that $f(x,\,\cdot\,)$ is continuous on $[c,d]$ and differentiable on $(c,d)$ for all $x \in [a,b]$, and that $D_2 f$ is continuous on $[a,b] \times (c,d)$. Then the function $F \colon [c,d] \to \reals$ given by
    %
    \begin{equation*}
        F(y)
            = \int_a^b f(x,y) \dif\alpha(x)
    \end{equation*}
    %
    is differentiable on $(c,d)$ and
    %
    \begin{equation*}
        F'(y)
            = \int_a^b D_2 f(x,y) \dif\alpha(x).
    \end{equation*}
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $y,y_0 \in (c,d)$ with $y \neq y_0$. By the mean value theorem we have
    %
    \begin{equation*}
        \frac{F(y) - F(y_0)}{y - y_0}
            = \int_a^b \frac{f(x,y) - f(x,y_0)}{y - y_0} \dif\alpha(x)
            = \int_a^b D_2 f(x, y_x) \dif\alpha(x)
    \end{equation*}
    %
    for some $y_x \in (c,d)$ lying between $y$ and $y_0$, depending on $x$. Let $I \subseteq (c,d)$ be a non-trivial compact interval containing $y$. Then $D_2 f$ is uniformly continuous on $[a,b] \times I$, so given $\epsilon > 0$ there is a $\delta > 0$ such that $\norm{z-z'} < \delta$ implies $\abs{D_2 f(z) - D_2 f(z')} < \epsilon$ for $z,z' \in [a,b] \times I$. For $y,y_0 \in I$ with $\abs{y-y_0} < \delta$ we also have $\abs{y_x-y_0} < \delta$ for all $x \in [a,b]$, and so
    %
    \begin{align*}
        \abs[\bigg]{ \int_a^b D_2 f(x, y_x) \dif\alpha(x)
            - \int_a^b D_2 f(x, y_0) \dif\alpha(x) }
            &\leq \int_a^b \abs{ D_2 f(x, y_x) - D_2 f(x, y_0) } \dif\alpha(x) \\
            &\leq \epsilon (\alpha(b) - \alpha(a)).
    \end{align*}
    %
    Since $\epsilon$ was arbitrary, this shows that $F$ is differentiable at $y_0$ with derivative as claimed.
\end{proof}


\begin{proposition}
    Let $\alpha$ be of bounded variation on $[a,b]$, and let $(f_n)_{n\in\naturals}$ be a sequence of $\alpha$-integrable functions on $[a,b]$ that converge uniformly to a function $f$. Then $f$ is also $\alpha$-integrable on $[a,b]$, and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \lim_{n\to\infty} \int_a^b f_n \dif\alpha.
    \end{equation*}
    %
    In particular, $\integrable[\alpha]{a,b}$ is a closed subspace of $C[a,b]$ equipped with the uniform norm.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $\epsilon_n = \norm{f_n - f}_\infty$ such that
    %
    \begin{equation*}
        f_n - \epsilon_n
            \leq f
            \leq f_n + \epsilon_n
    \end{equation*}
    %
    for $n \in \naturals$. It follows that
    %
    \begin{equation*}
        \int_a^b (f_n - \epsilon_n) \dif\alpha
            \leq \lowint_a^b f \dif\alpha
            \leq \upint_a^b f \dif\alpha
            \leq \int_a^b (f_n + \epsilon_n) \dif\alpha,
    \end{equation*}
    %
    and hence,
    %
    \begin{equation*}
        0
            \leq \upint_a^b f \dif\alpha - \lowint_a^b f \dif\alpha
            \leq 2 \epsilon_n (\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    Thus the upper and lower integrals of $f$ are equal, so $f$ is $\alpha$-integrable. Finally we have
    %
    \begin{equation*}
        \abs[\bigg]{ \int_a^b f_n \dif\alpha - \int_a^b f \dif\alpha }
            \leq \int_a^b \abs{f_n - f} \dif\alpha
            \leq \epsilon_n (\alpha(b) - \alpha(a)),
    \end{equation*}
    %
    proving the claim.
\end{proof}


\section{Line integrals}

\newcommand{\partition}{\mathcal{P}}
\newcommand{\grad}{\nabla}

\noindent Recall that a \emph{path} in a topological space $X$ is a continuous map $\gamma \colon [a,b] \to X$, and that $\gamma$ is \emph{closed} if $\gamma(a) = \gamma(b)$. A subset $\Gamma \subseteq X$ is called a \emph{curve} in $X$ if there is a path $\alpha$ in $X$ whose image is $\Gamma$. The image of a path $\gamma$ is called its \emph{trace} and is denoted $\gamma^*$.

\begin{definition}[Equivalence of paths]
	Let $\alpha \colon [a,b] \to X$ and $\beta \colon [c,d] \to X$ be paths in a topological space $X$. If there is an increasing homeomorphism $\phi \colon [c,d] \to [a,b]$ such that $\beta = \alpha \circ \phi$, then $\alpha$ and $\beta$ are said to be \emph{properly equivalent}.

	If $\alpha$ and $\beta$ are closed paths with $\alpha(a) \neq \beta(c)$, then we also say that they are properly equivalent if there is a point $e \in (c,d)$ such that $\alpha$ and $\gamma$ are properly equivalent in the above sense, where $\gamma \colon [e, d-c+e] \to X$ is given by
	%
	\begin{equation*}
		\gamma(t) =
		\begin{cases}
			\beta(t), & t \in [e,d], \\
			\beta(t-d+c), & t \in [d,d-c+e].
		\end{cases}
	\end{equation*}
	
	If the map $\phi$ above is decreasing, then we say that $\alpha$ and $\beta$ are \emph{improperly equivalent}. The paths $\alpha$ and $\beta$ are \emph{equivalent} if they are either properly or improperly equivalent.
\end{definition}
%
Note that the condition that $\phi$ be an increasing (decreasing) homeomorphism is equivalent to it being continuous, strictly increasing (decreasing) and surjective. Also note that equivalent paths trace out the same curve in $X$.

\begin{definition}[Line integrals] % TODO: Assume f is bounded, as for regular integrals?
	Let $\gamma \colon [a,b] \to \setR^d$ be a path, and let $f \colon \gamma^* \to \setR^d$ be a vector field. Given a tagged partition $(P,T)$ of $[a,b]$ then, with notation as above, we form the sums
    %
    \begin{equation*}
        S_{f,\gamma}(P,T)
            = \sum_{i=1}^n f(\gamma(t_i)) \cdot (\gamma(x_i) - \gamma(x_{i-1})).
    \end{equation*}
    %
    Define the \emph{line integral} of $f$ with respect to $\gamma$ as the limit of the net $S_{f,\gamma}$, if the limit exists. We denote this integral by $\int f \cdot \dif\gamma$.
\end{definition}
%
Notice that if $\alpha$ and $\beta$ are properly equivalent paths, then
%
\begin{equation*}
	\int f \cdot \dif\alpha = \int f \cdot \dif\beta.
\end{equation*}
%
If $\alpha$ and $\beta$ are instead improperly equivalent, then the two integrals are equal but with opposite signs.


\begin{proposition}
	Let $\gamma \colon [a,b] \to \setR^d$ be a path, and let $f \colon \gamma^* \to \setR^d$ be a bounded function. Then
	%
	\begin{equation*}
		\int f \cdot \dif\gamma
			= \sum_{k=1}^d \int_a^b f_k \circ \gamma \dif\gamma_k
	\end{equation*}
	%
	whenever each Riemann--Stieltjes integral on the right exists. If in addition $\gamma$ is piecewise $C^1$, then
	%
	\begin{equation*}
		\int f \cdot \dif\gamma
			= \int_a^b f(\gamma(t)) \cdot \gamma'(t) \dif t.
	\end{equation*}
\end{proposition}

\begin{proof}
	Notice that
	%
	\begin{equation*}
		S_{f,\gamma}(P,T)
			= \sum_{k=1}^d \sum_{i=1}^n f_k(\gamma(t_i)) (\gamma_k(t_i) - \gamma_k(t_{i-1})).
	\end{equation*}
	%
	Since the inner sums on the right-hand side approximate the Riemann--Stieltjes integrals $\int_a^b f_k \circ \gamma \dif\gamma_k$, the first claim follows by taking limits. The second claim follows by [reference].
\end{proof}


\begin{theorem}[Integral of a gradient]
	Let $U \subseteq \setR^d$ be open, and let $\phi \in C^1(U)$. For every pair of points $x,y \in U$ and every piecewise $C^1$ path $\gamma \colon [a,b] \to U$ with $\gamma(a) = x$ and $\gamma(b) = y$ we have
	%
	\begin{equation*}
		\int \grad\phi \cdot \dif\gamma
			= \phi(y) - \phi(x).
	\end{equation*}
\end{theorem}
%
If $f = \grad\phi$, then $\phi$ is called a \emph{potential function} for $f$.

\begin{proof}
	Let $a = t_0 < \cdots < t_n = b$ be a partition of $[a,b]$ such that $\gamma'$ is continuous on each subinterval. By the chain rule,
	%
	\begin{equation*}
		(\phi \circ \gamma)'(t)
			= \grad\phi(\gamma(t)) \cdot \gamma'(t)
	\end{equation*}
	%
	on each open subinterval $(t_{i-1}, t_i)$. By [reference],
	%
	\begin{align*}
		\int \grad\phi \cdot \dif\gamma
			&= \sum_{i=1}^n \int_{t_{i-1}}^{t_i} \grad\phi(\gamma(t)) \cdot \gamma'(t) \dif t
			= \sum_{i=1}^n \int_{t_{i-1}}^{t_i} (\phi \circ \gamma)'(t) \dif t \\
			&= \phi(\gamma(b)) - \phi(\gamma(a))
			= \phi(y) - \phi(x),
	\end{align*}
	%
	as desired.
\end{proof}



\begin{theorem}
	Let $U \subseteq \setR^d$ be an open, connected set, and let $f \colon U \to \setR^d$ be a continuous function. Fix a point $x_0 \in U$. For each $x \in U$ and each pair of polygonal paths $\alpha, \beta \colon [a,b] \to U$ joining $x_0$ and $x$, assume that
	%
	\begin{equation*}
		\int f \cdot \dif\alpha
			= \int f \cdot \dif\beta.
	\end{equation*}
	%
	Then there exists a function $\phi \in C^1(U)$ such that $f = \grad\phi$.
\end{theorem}
%
Notice that since $U$ is connected, such polygonal paths exist between any pair of points.

\begin{proof}
	Let $x \in U$, and let $\alpha \colon [a,b] \to U$ be a polygonal curve joining $x_0$ and $x$. Define
	%
	\begin{equation*}
		\phi(x) = \int f \cdot \dif\alpha.
	\end{equation*}
	%
	By hypothesis, the number $\phi(x)$ does not depend on the particular choice of $\alpha$. We show that each partial derivative $D_k \phi(x)$ exists and equals $f_k(x)$.

	Let $B(x,\delta) \subseteq U$ for some $\delta > 0$, and let $\lambda \in [-\delta/2, \delta/2]$. Define a path $\gamma \colon [0,1] \to B(x,\delta)$ by $\gamma(t) = (1-t)x + t(x + \lambda e_k)$, where $e_k$ is the $k$th standard basis vector. Then
	%
	\begin{equation*}
		\phi(x + \lambda e_k) - \phi(x)
			= \int f \cdot \dif\gamma.
	\end{equation*}
	%
	Furthermore, $\gamma_k'(t) = \lambda$ and $\gamma_i'(t) = 0$ for $i \neq k$. Thus $\gamma$ is $C^1$, and so
	%
	\begin{align*}
		\phi(x + \lambda e_k) - \phi(x)
			&= \sum_{i=1}^d \int_0^1 f_i(\gamma(t)) \gamma_i'(t) \dif t \\
			&= \lambda \int_0^1 f_k(\gamma(t)) \dif t
			 = \lambda \int_0^1 g(t,\lambda) \dif t,
	\end{align*}
	%
	where $g(t,\lambda) = f_k((1-t)x + t(x + \lambda e_k))$. Since $g$ is continuous on $[0,1] \times [-\delta/2, \delta/2]$, \cref{thm:integral-continuity} implies that
	%
	\begin{equation*}
		\lim_{\lambda \to 0} \int_0^1 g(t,\lambda) \dif t
			= \int_0^1 g(t,0) \dif t
			= \int_0^1 f_k(x) \dif t
			= f_k(x),
	\end{equation*}
	%
	proving that $D_k \phi(x) = f_k(x)$. Thus $\grad\phi(x) = f(x)$ for all $x \in U$, and $\phi \in C^1(U)$ since $f$ is continuous.
\end{proof}


\chapter{Convergence}

It is well-known that a Cauchy sequence $(x_n)_{n\in\naturals}$ in a metric space $S$ converges to some $x \in S$ if and only if $(x_n)$ has a \emph{subsequence} that converges to $x$.

In this section we highlight a similar feature of convergence in measure and convergence in mean: If $(f_n)_{n\in\naturals}$ is a Cauchy sequence in measure, and if there is a subsequence that converges \emph{pointwise a.e.} to some function $f$, then $(f_n)$ also converges to $f$ in measure. Similarly for convergence in mean.

Furthermore, Markov's inequality implies that convergence in mean is stronger than convergence in measure. In particular, a sequence that is Cauchy in mean is also Cauchy in measure. Hence when we show that convergence in measure and in mean are complete, it suffices to show that being Cauchy in measure implies the existence of a pointwise a.e. convergent subsequence.

\begin{definition}[Convergence in measure]
    Let $(X,\calE,\mu)$ be a measure space. Let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$, and let further $f \in \measurable(\calE)$. We say that $(f_n)$ \emph{converges to $f$ in $\mu$-measure} if for every $\epsilon > 0$,
    %
    \begin{equation*}
        \lim_{n\to\infty} \mu\bigl( \{ \abs{f_n - f} > \epsilon \}\bigr) = 0.
    \end{equation*}
    %
    Furthermore, $(f_n)$ is called a \emph{Cauchy sequence in $\mu$-measure} if, for every $\epsilon > 0$,
    %
    \begin{equation*}
        \lim_{m,n\to\infty} \mu\bigl( \{ \abs{f_m - f_n} > \epsilon \} \bigr) = 0.
    \end{equation*}
\end{definition}

We prove that convergence in $\mu$-measure is complete.

\begin{lemma}
    \label{thm:convergence-in-measure-lemma}
    Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$. If there exists a sequence $(\epsilon_n)_{n\in\naturals}$ of strictly positive numbers such that
    %
    \begin{equation}
        \label{eq:convergence-in-measure-complete-lemma}
        \sum_{n=1}^\infty \epsilon_n < \infty
        \quad \text{and} \quad
        \sum_{n=1}^\infty \mu \bigl( \{ \abs{f_{n+1} - f_n} > \epsilon_n \} \bigr) < \infty,
    \end{equation}
    %
    then there exists a function $f \in \measurable(\calE)$ such that $(f_n)$ converges to $f$ both $\mu$-a.e. and in $\mu$-measure.
\end{lemma}

\begin{proof}
    For $n \in \naturals$, denote the set $\{ \abs{f_{n+1} - f_n} > \epsilon_n \}$ by $E_n$, and define sets $F_k = \bigunion_{n=k}^\infty E_n$ and $F = \bigintersect_{k\in\naturals} F_k$. Notice that $F = \limsup_{n\to\infty} E_n$, so the first Borel--Cantelli lemma implies that $\mu(F) = 0$.
    
    For $m \geq n$ we find that
    %
    \begin{equation}
        \label{eq:convergence-in-measure-Cauchy}
        \abs{ f_m - f_n }
            \leq \sum_{i=n}^{m-1} \abs{f_{i+1} - f_i}
            \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}.
    \end{equation}
    %
    If furthermore $x \in F_k^c$ and $m \geq n \geq k$, then
    %
    \begin{equation*}
        \abs{ f_m(x) - f_n(x) }
            \leq \sum_{i=n}^\infty \epsilon_i.
    \end{equation*}
    %
    The right-hand side converges to zero as $n \to \infty$, which shows that $(f_n(x))$ is a Cauchy sequence in $\reals$ for $x \in F_k^c$, hence for $x \in F^c$. Letting $f = \lim_{n\to\infty} f_n \indicator{F^c}$ we thus find that $(f_n)$ converges to $f \in \measurable(\calE)$ $\mu$-a.e.

    Next we show that $f_n \to f$ in $\mu$-measure as $n \to \infty$. Letting $m \to \infty$ in \cref{eq:convergence-in-measure-Cauchy} we find that
    %
    \begin{equation*}
        \abs{ f_n - f }
        \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}
    \end{equation*}
    %
    $\mu$-a.e. Now let $\epsilon > 0$, and choose an $N \in \naturals$ such that $\sum_{i=N}^\infty \epsilon_i \leq \epsilon$. For $n \geq N$, $\abs{f_n - f} > \epsilon$ then implies that
    %
    \begin{equation*}
        \sum_{i=n}^\infty \epsilon_i
            \leq \epsilon
            < \abs{f_n - f}
            \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}
    \end{equation*}
    %
    $\mu$-a.e., which in turn implies that $\epsilon_i < \abs{f_{i+1} - f_i}$ $\mu$-a.e. for some $i \geq n$. Hence it follows that
    %
    \begin{equation*}
        \mu\bigl( \{ \abs{f_n - f} > \epsilon \}\bigr)
            \leq \mu \Bigl( \bigunion_{i=n}^\infty E_i \Bigr)
            \leq \sum_{i=n}^\infty \mu(E_i),
    \end{equation*}
    %
    which converges to zero by \cref{eq:convergence-in-measure-complete-lemma}.
\end{proof}


\begin{theorem}[Completeness of convergence in measure]
    Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in $\mu$-measure. Then there exists a function $f \in \measurable(\calE)$ such that $f_n \to f$ in $\mu$-measure. Furthermore, $(f_n)$ has a subsequence that converges to $f$ $\mu$-a.e.
\end{theorem}

\begin{proof}
    We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in $\mu$-measure. If $(f_n)$ has a subsequence that converges $\mu$-a.e. to function $f \in \measurable(\calE)$, then $(f_n)$ also converges to $f$ in $\mu$-measure.
    \end{displaytheorem}
    %
    Let $(f_{n_k})$ be such a subsequence. For $\epsilon > 0$ we then have
    %
    \begin{equation*}
        \{ \abs{f_n - f} > \epsilon \}
            \subseteq \{ \abs{f_n - f_{n_k}} > \epsilon/2 \} \union \{ \abs{f_{n_k} - f} > \epsilon/2 \},
    \end{equation*}
    %
    and the measures of the sets on the right-hand side go to zero as $n \to \infty$ (since $n_k \geq n$). This proves the lemma.

    To prove the theorem, choose a subsequence $(g_k) = (f_{n_k})$ such that
    %
    \begin{equation*}
        \mu \bigl( \{ \abs{g_{k+1} - g_k} > 2^{-k} \} \bigr)
            \leq 2^{-k}.
    \end{equation*}
    %
    \cref{thm:convergence-in-measure-lemma} then implies the existence of a function $f \in \measurable(\calE)$ such that $g_k \to f$ both $\mu$-a.e. and in $\mu$-measure. The claim then follows from the above lemma.
\end{proof}


\begin{definition}[Convergence in mean]
    Let $(X,\calE,\mu)$ be a measure space. Let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$, and let further $f \in \measurable(\calE)$. If $p \in (0,\infty)$ we say that $(f_n)$ \emph{converges to $f$ in the $\mu$-$p$-th mean} if
    %
    \begin{equation*}
        \lim_{n\to\infty} \int_X \abs{f_n - f}^p \dif\mu = 0.
    \end{equation*}
    %
    Furthermore, $(f_n)$ is called a \emph{Cauchy sequence in the $\mu$-$p$-th mean} if
    %
    \begin{equation*}
        \lim_{m,n\to\infty} \int_X \abs{f_m - f_n}^p \dif\mu = 0.
    \end{equation*}
\end{definition}

\begin{remark}
    If $(f_n)$ converges to $f$ in the $\mu$-$p$-th mean, then $f_n - f \in \calL^p(\mu)$ for $n \geq N$ for some $N \in \naturals$. Furthermore, if $f \in \calL^p(\mu)$, then $f_n = (f_n - f) + f \in \calL^p(\mu)$ for $n \geq N$.
    
    On the other hand, if $f_n \in \calL^p(\mu)$ for large enough $n$, then $f = (f - f_n) + f_n \in \calL^p(\mu)$. In particular, $\calL^p(\mu)$ is a closed subspace of $\measurable(\calE)$.
\end{remark}


\begin{theorem}[Completeness of convergence in mean]
    Let $(X,\calE,\mu)$ be a measure space, let $p \in (0,\infty)$, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in the $\mu$-$p$-th mean. Then there exists a function $f \in \measurable(\calE)$ such that $f_n \to f$ in the $\mu$-$p$-th mean. Furthermore, $(f_n)$ has a subsequence that converges to $f$ $\mu$-a.e.

    In particular, $\calL^p(\mu)$ is complete.
\end{theorem}

\begin{proof}
    We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $(X,\calE,\mu)$ be a measure space, let $p \in (0,1)$, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in the $\mu$-$p$-th mean. If $(f_n)$ has a subsequence that converges $\mu$-a.e. to function $f \in \measurable(\calE)$, then $(f_n)$ also converges to $f$ in the $\mu$-$p$-th mean.
    \end{displaytheorem}
    %
    Let $(f_{n_k})_{k\in\naturals}$ be a subsequence that converges $\mu$-a.e. to $f$. For $n \in \naturals$, Fatou's lemma implies that
    %
    \begin{align*}
        \int_X \abs{f_n - f}^p \dif\mu
            &= \int_X \liminf_{k\to\infty} \abs{f_n - f_{n_k}}^p \dif\mu
             \leq \liminf_{k\to\infty} \int_X \abs{f_n - f_{n_k}}^p \dif\mu \\
            &\leq \sup_{m \geq n} \int_X \abs{f_n - f_m}^p \dif\mu,
    \end{align*}
    %
    which converges to zero as $n \to \infty$ as desired.

    We now prove the theorem. Markov's inequality implies that $(f_n)$ is also a Cauchy sequence in $\mu$-measure, so [reference] yields a function $f \in \measurable(\calE)$ such that $f_n \to f$ in $\mu$-measure, and such that $(f_n)$ has a subsequence $(f_{n_k})_{k\in\naturals}$ that converges to $f$ $\mu$-a.e. The lemma then implies that $(f_n)$ converges to $f$ in the $\mu$-$p$-th mean.

    For the last claim, if $f_n \in \calL^p(\mu)$ for all $n \in \naturals$, then since $\calL^p(\mu)$ is closed we also have $f \in \calL^p(\mu)$.
\end{proof}


% \subsection{Completeness part 2}

% Let $X$ be a topological vector space. If $(x_n)_{n\in\naturals}$ is a sequence in $X$, then the series $\sum_{n=1}^\infty x_n$ is said to \emph{converge} to $x \in X$ if $\sum_{i=1}^n x_i \to x$ as $n \to \infty$. If $X$ is equipped with a pseudometric $\rho$, then the above series is said to be \emph{absolutely convergent} if $\sum_{n=1}^\infty \rho(x_n,0) < \infty$. If $\rho$ is invariant, then notice that
% %
% \begin{equation*}
%     \rho(x+y, 0)
%         = \rho(x, -y)
%         \leq \rho(x, 0) + \rho(0, -y)
%         = \rho(x, 0) + \rho(y, 0)
% \end{equation*}
% %
% for all $x,y \in X$. The most important case is of course when $\rho$ is induced from a seminorm $\norm{\,\cdot\,}$, in which case $\norm{x} = \rho(x,0)$ for $x \in X$.

% \begin{lemma}
%     Let $X$ be a vector space equipped with an invariant pseudometric $\rho$. Then $X$ is complete if and only if every absolutely convergent series in $X$ converges.
% \end{lemma}

% \begin{proof}
%     First assume that $X$ is complete, and consider a sequence $(x_n)_{n\in\naturals}$ in $X$ with $\sum_{n=1}^\infty \rho(x_n,0) < \infty$. Letting $s_n = \sum_{i=1}^n x_i$, if $m < n$ then
%     %
%     \begin{equation*}
%         \rho(s_m, s_n)
%             = \rho(s_n - s_m, 0)
%             \leq \sum_{i = m+1}^n \rho(x_i, 0)
%             \to 0
%     \end{equation*}
%     %
%     as $m,n \to \infty$. Thus $(s_n)$ is Cauchy and hence convergent.

%     Conversely, assume that every absolutely convergent sequence in $X$ converges, and let $(x_n)_{n\in\naturals}$ be a Cauchy sequence in $X$. Choose a strictly increasing sequence $(n_k)_{k\in\naturals}$ of natural numbers such that $\rho(x_m,x_n) < 2^{-k}$ for $m,n \geq n_k$. Let $y_1 = x_{n_1}$ and $y_k = x_{n_k} - x_{n_{k-1}}$ for $k > 1$. Then $\sum_{i=1}^k y_i = x_k$, and
%     %
%     \begin{equation*}
%         \sum_{k=1}^\infty \rho(y_k, 0)
%             \leq \rho(y_1, 0) + \sum_{k=1}^\infty 2^{-k}
%             = \rho(y_1, 0) + 1
%             < \infty.
%     \end{equation*}
%     %
%     Thus $\lim_{k\to\infty} x_{n_k} = \sum_{k=1}^\infty y_k$ exists, and so $(x_n)$ is a Cauchy sequence with a convergent subsequence, and hence is itself convergent.
% \end{proof}


% \subsection{Completeness part 3}


\chapter{Portmanteau theorems}

If $P$ is a Borel probability measure on a topological space $X$, then a subset $A \subseteq X$ is called a \emph{$P$-continuity set} if $P(\boundary A) = 0$.

Recall that a topological space $X$ is called a \emph{perfect space} or a \emph{$G_\delta$ space} if every closed subset of $X$ is a $G_\delta$ set, i.e. a countable intersection of open sets. (Equivalently, if every open subset is an $F_\sigma$ set, a countable union of closed sets.) Furthermore, if $A$ and $B$ are closed subsets of $X$, a \emph{Urysohn function} for the pair $(A,B)$ is a continuous function $f \colon X \to [0,1]$ with $f(A) = 1$ and $f(B) = 0$.\footnote{The ordering of $A$ and $B$ is only relevant insofar as it is relevant on which set the Urysohn function in question vanishes. If $f \colon X \to [0,1]$ is a Urysohn function for the pair $(A,B)$, then the function $1-f$ is a Urysohn function for the pair $(B,A)$.} By Urysohn's lemma, a topological space is normal (i.e. every pair of disjoint closed sets can be separated by disjoint open sets) if and only if there is a Urysohn function for every pair of disjoint closed subsets. Finally, a (not necessarily Hausdorff) normal $G_\delta$ space is called a \emph{perfectly normal space}. If it is also Hausdorff it also called a \emph{$T_6$-space} or a \emph{perfectly $T_4$ space}.

It is easy to show that (pseudo-)metrisable spaces are perfectly normal. Another notable class of perfectly normal (Hausdorff) spaces are the CW complexes. [TODO: Reference, proof somewhere else maybe?]

\begin{theorem}
    Let $(P_n)_{n\in\naturals}$ and $P$ be probability measures on a perfectly normal space $X$. Then the following conditions are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:portmanteau-weak-convergence} $P_n \wto P$.

        \item \label{enum:portmanteau-closed} $\limsup_{n\to\infty} P_n(F) \leq P(F)$ for all closed $F \subseteq X$.

        \item \label{enum:portmanteau-open} $\liminf_{n\to\infty} P_n(G) \geq P(G)$ for all open $G \subseteq X$.

        \item \label{enum:portmanteau-continuity-sets} $P_n(A) \to P(A)$ for all $P$-continuity sets $A \subseteq X$.
    \end{enumthm}
\end{theorem}


\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:portmanteau-weak-convergence} $\implies$ \subcref{enum:portmanteau-closed}]
    Let $F$ be a closed subset of $X$. Since $X$ is perfect, there is a decreasing sequence $(F_k)_{k\in\naturals}$ of open sets such that $F = \bigintersect_{k\in\naturals} F_k$. Furthermore, since $X$ is normal there is for each $k \in \naturals$ a Urysohn function $g_k$ for the pair $(F,F_k^c)$. We clearly have $\indicator{F} \leq g_k \leq \indicator{F_k}$ so
    %
    \begin{equation*}
        \limsup_{n\to\infty} P_n(F)
            \leq \limsup_{n\to\infty} \int_X g_k \dif P_n
            = \int_X g_k \dif P
            \leq P(F_k).
    \end{equation*}
    %
    Since $F_k \downarrow F$ as $k \to \infty$, continuity of $P$ implies the claim.

    \item[\subcref{enum:portmanteau-closed} $\Leftrightarrow$ \subcref{enum:portmanteau-open}]
    This follows easily by taking complements.

    \item[\subcref{enum:portmanteau-closed} \& \subcref{enum:portmanteau-open} $\implies$ \subcref{enum:portmanteau-continuity-sets}]
    For $A \subseteq X$ we have
    %
    \begin{align*}
        P(\interior{A})
            &\leq \liminf_{n\to\infty} P_n(\interior{A})
             \leq \liminf_{n\to\infty} P_n(A) \\
            &\leq \limsup_{n\to\infty} P_n(A)
             \leq \limsup_{n\to\infty} P_n(\closure{A})
             \leq P(\closure{A}).
    \end{align*}
    %
    If $A$ is a $P$-continuity set then $P(\interior{A}) = P(\closure{A})$, which implies \subcref{enum:portmanteau-continuity-sets}.

    \item[\subcref{enum:portmanteau-continuity-sets} $\implies$ \subcref{enum:portmanteau-weak-convergence}]
    Given $f \in C_b(X)$, by linearity we may assume that $0 \leq f \leq 1$. Then
    %
    \begin{equation*}
        \int_X f \dif P
            = \int_0^\infty P( f \geq t ) \dif t
            = \int_0^1 P( f \geq t ) \dif t,
    \end{equation*}
    %
    and similarly for $P_n$. Since $f$ is continuous, we have $\boundary \{ f \geq t \} \subseteq \{ f = t \}$. Now notice that $\{ f = t \}$ is a $P$-null set except for countably many $t \in (0,1)$, since $P$ is a finite measure.\footnote{Indeed, $f$ is a random variable whose discrete support is precisely this set of $t$s. But the discrete support of any random variable is countable.} Hence $\{ f \geq t \}$ is a $P$-continuity set for all but countably many $t$. It then follows from \subcref{enum:portmanteau-continuity-sets} and the dominated convergence theorem that
    %
    \begin{equation*}
        \int_X f \dif P_n
            = \int_0^1 P_n( f \geq t ) \dif t
            \xrightarrow[n\to\infty]{} \int_0^1 P( f \geq t ) \dif t
            = \int_X f \dif P
    \end{equation*}
    %
    as claimed.
\end{proofsec}
\end{proof}


\begin{remark}
    In the case that $X$ is pseudometrisable, we may pick a pseudometric $\rho$. The sets $F_k$ can then be given explicitly by
    %
    \begin{equation*}
        F_k
            = \set[\bigg]{x \in S}{\rho(x,F) < \frac{1}{k}}.
    \end{equation*}
    %
    Furthermore, we may take the Urysohn functions $g_k$ to be given by $g_k(x) = (1 - k\rho(x,F)) \join 0$. Notice that these are Lipschitz since
    %
    \begin{align*}
        \abs{ g_k(x) - g_k(y) }
            &= \abs[\big]{ (1 - k\rho(x,F)) \join 0 - (1 - k\rho(y,F)) \join 0 } \\
            &\leq \abs{ k\rho(x,F) - k\rho(y,F) }
             \leq k \rho(x,y)
    \end{align*}
    %
    for all $x,y \in X$. In particular they are uniformly continuous. Hence weak convergence in a metrisable space may be characterised by the bounded, uniformly continuous functions, or even the bounded Lipschitz functions. Notice also that this is independent of the metric.
\end{remark}


















\chapter{Dynkin systems and monotone classes}

\begin{definition}[Dynkin systems, $\pi$-systems]
    Let $X$ be a set. A collection $\calD$ of subsets of $X$ is a \emph{Dynkin system} in $X$ if it has the following properties:
    %
    \begin{enumdef}
        \item $X \in \calD$,
        \item $B \setminus A \in \calD$ whenever $A,B \in \calD$ and $A \subseteq B$, and
        \item $\bigunion_{n\in\naturals} A_n \in \calD$ for any increasing sequence $(A_n)_{n\in\naturals}$ of sets in $\calD$.
    \end{enumdef}
    %
    Furthermore, a collection $\calS$ of subsets of $X$ is called a \emph{$\pi$-system} if it is closed under intersections.
\end{definition}
%
It is easy to show that if $\calD$ is both a Dynkin system in $X$ and a $\pi$-system, then it is in fact a $\sigma$-algebra in $X$.

\begin{definition}[Monotone classes, set algebras]
    Let $X$ be a set. A collection $\calM$ of subsets of $X$ is a \emph{monotone class} if it is closed under countable increasing unions and countable decreasing intersections.

    Furthermore, a collection $\calA$ of subsets of $X$ is called a \emph{set algebra} in $X$ if it is closed under finite unions and complements.
\end{definition}
%
We note that a set algebra $\calA$ in $X$ is automatically closed under finite intersections, and that it also contains both $\emptyset$ and $X$. It is easy to show that if $\calM$ is both a monotone class and a set algebra in $X$, then it is in fact a $\sigma$-algebra in $X$.

Notice that we have two pairs of properties that ensure that a collection of sets is a $\sigma$-algebra. On the one hand we should think of Dynkin systems and monotone classes as being analogous, and similarly for $\pi$-systems and set algebras on the other. The latter pair of properties are algebraic, while the first pair are somehow analytic (or continuous), in that they involve infinitely many operations.

It turns out that if $\calS$ is a $\pi$-system, then the Dynkin system $\delta(\calS)$ generated by $\calS$ is also a $\pi$-system. Similarly, if $\calA$ is a set algebra, then the monotone class $M(\calA)$ generated by $\calA$ is also a set algebra. We have the following two results whose proofs are basically identical:

\begin{theorem}[Dynkin's lemma]
    Let $\calS$ be a $\pi$-system in a set $X$. Then $\delta(\calS)$ is also a $\pi$-system, and in particular
    %
    \begin{equation}
        \label{eq:Dynkins-lemma}
        \delta(\calS) = \sigma(\calS).
    \end{equation}
\end{theorem}

\begin{proof}
    The identity \cref{eq:Dynkins-lemma} follows if $\delta(\calS)$ is a $\pi$-system: For then it is also a $\sigma$-algebra, and then $\sigma(\calS) \subseteq \delta(\calS)$.

    For $A \in \delta(\calS)$ define
    %
    \begin{equation*}
        \calD_A
            = \set{B \in \delta(\calS)}{A \intersect B \in \delta(\calS)}.
    \end{equation*}
    %
    This is easily seen to be a Dynkin system. Also notice that $B \in \calD_A$ if and only if $A \in \calD_B$ for all $A,B \in \delta(\calS)$. Furthermore, if $A \in \calS$ then $\calS \subseteq \calD_A$, and so $\delta(\calS) \subseteq \calD_A$. In other words,
    %
    \begin{equation*}
        B \in \calD_A
        \quad
        \text{for $A \in \calS$ and $B \in \delta(\calS)$}.
    \end{equation*}
    %
    By symmetry we then have
    %
    \begin{equation*}
        A \in \calD_B
        \quad
        \text{for $A \in \calS$ and $B \in \delta(\calS)$},
    \end{equation*}
    %
    and since $\calD_B$ is a Dynkin system it follows that $\delta(\calS) \subseteq \calD_B$. Hence $\delta(\calS)$ is a $\pi$-system as desired.
\end{proof}


\begin{theorem}[The monotone class lemma]
    Let $\calA$ be a set algebra in a set $X$. Then $M(\calA)$ is also a set algebra, and in particular
    %
    \begin{equation}
        \label{eq:monotone-class-lemma}
        M(\calA) = \sigma(\calA).
    \end{equation}
\end{theorem}

\begin{proof}
    The identity \cref{eq:monotone-class-lemma} follows if $M(\calA)$ is a set algebra: For then it is also a $\sigma$-algebra, and then $\sigma(\calA) \subseteq M(\calA)$.

    For $A \in M(\calA)$ define
    %
    \begin{equation*}
        \calM_A
            = \set{B \in M(\calA)}{\text{$A \setminus B$, $B \setminus A$, and $A \intersect B$ are in $M(\calA)$}}.
    \end{equation*}
    %
    This is easily seen to be a monotone class. Also notice that $B \in \calM_A$ if and only if $A \in \calM_B$ for all $A,B \in M(\calA)$. Furthermore, if $A \in \calA$ then $\calA \subseteq \calM_A$, and so $M(\calA) \subseteq \calM_A$. In other words,
    %
    \begin{equation*}
        B \in \calM_A
        \quad
        \text{for $A \in \calA$ and $B \in M(\calA)$}.
    \end{equation*}
    %
    By symmetry we then have
    %
    \begin{equation*}
        A \in \calM_B
        \quad
        \text{for $A \in \calA$ and $B \in M(\calA)$},
    \end{equation*}
    %
    and since $\calM_B$ is a monotone class it follows that $M(\calA) \subseteq \calM_B$. Hence $M(\calA)$ is a set algebra as desired.
\end{proof}


\chapter{Complex analysis}

[Should it be here?]

\newcommand{\diam}{\operatorname{diam}}
\newcommand{\hol}{\mathcal{H}}

If $V \subseteq \complex$ is open and $f \colon V \to \complex$ is differentiable at every point in $V$, then we say that $f$ is \emph{holomorphic} on $V$. The set of functions holomorphic on $V$ is denoted $\hol(V)$.

\begin{theorem}[The Cauchy--Goursat Lemma]
    If $f \in \hol(V)$, then
    %
    \begin{equation*}
        \int_{\boundary T} f(x) \dif z = 0
    \end{equation*}
    %
    for every triangle $T \subseteq V$.
\end{theorem}

\begin{proof}
    Notice that any triangle $T$ can be subdivided into four smaller triangles $T^1, \ldots, T^4$ whose corners are the corners and midpoints of the sides of $T$. We then clearly have
    %
    \begin{equation*}
        \int_{\boundary T} g(z) \dif z
            = \sum_{i=1}^4 \int_{\boundary T^i} g(z) \dif z
    \end{equation*}
    %
    for all $g \in C(T)$.

    Let $T_0 \subseteq V$ be a triangle, and consider the integral
    %
    \begin{equation*}
        I
            = \int_{\boundary T_0} f(z) \dif z.
    \end{equation*}
    %
    By the above considerations we have
    %
    \begin{equation*}
        \abs{I}
            \leq 4 \abs[\bigg]{ \int_{\boundary T_0^i} f(z) \dif z }
    \end{equation*}
    %
    for at least one value of $i$. For this value of $i$ let $T_1 = T_0^i$. Continuing this process yields a sequence $(T_n)_{n\in\naturals}$ of triangles such that
    %
    \begin{equation*}
        \abs{I}
            \leq 4^n \abs[\bigg]{ \int_{\boundary T_n} f(z) \dif z }
    \end{equation*}
    %
    for $n \in \naturals_0$.

    Furthermore, each of the four triangles in a subdivision of a triangle $T$ have side lengths half of those of $T$, so
    %
    \begin{equation*}
        \diam T_n
            = 2^{-n} \diam T_0
    \end{equation*}
    %
    for $n \in \naturals_0$. Thus there exists a point $z_0 \in \complex$ such that $\bigintersect_{n\in\naturals_0} T_n = \{ z_0 \}$ since $(T_n)$ is a sequence of closed sets whose diameters tend to zero. It follows that
    %
    \begin{equation*}
        \sup_{z \in \boundary T_n} \abs{z - z_0}
            \leq 2^{-n} \diam T_0.
    \end{equation*}
    %
    Given $\epsilon > 0$ there exists an $r > 0$ such that
    %
    \begin{equation*}
        \abs{ f(z) - f(z_0) - f'(z_0)(z - z_0) }
            \leq \epsilon \abs{z - z_0}
    \end{equation*}
    %
    for $z \in B(z_0,r)$, and there further exists an $N \in \naturals$ such that $n \geq N$ implies $T_n \subseteq B(z_0,r)$. Now notice that the function $z \mapsto f(z_0) + f'(z_0)(z - z_0)$ has an antiderivative, so its integral along $\boundary T_n$ is zero. Denoting the length of the curve $\boundary T_n$ by $L_n$ we have $L_n \leq 2 \diam T_n$. Hence,
    %
    \begin{align*}
        \abs[\bigg]{ \int_{\boundary T_n} f(z) \dif z }
            &\leq \int_{\boundary T_n} \abs{ f(z) - f(z_0) - f'(z_0)(z - z_0) } \dif z \\
            &\leq L_n \epsilon \sup_{z \in \boundary T_n} \abs{z - z_0} \\
            &\leq 2^{-2n+1} \epsilon (\diam T_0)^2
    \end{align*}
    %
    for $n \geq N$, and so
    %
    \begin{equation*}
        \abs{I}
            \leq 2 \epsilon (\diam T_0)^2.
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, it follows that $I = 0$ as desired.
\end{proof}


\chapter{The extended real line}

\newcommand{\exreals}{\closure{\reals}}

Let $+\infty$ (or simply $\infty$) and $-\infty$ denote elements disjoint from $\reals$. The \emph{extended real line}, as a set, is then the union $\exreals = \reals \union \{\pm\infty\}$. We extend the ordering on $\reals$ to $\exreals$ by declaring that $-\infty < a$ for all $a \neq -\infty$ and that $\infty > a$ for all $a \neq \infty$. This clearly makes $\exreals$ into a totally ordered set. We further equip it with order topology, i.e. the topology generated by open rays $\set{x \in \exreals}{a < x}$ and $\set{x \in \exreals}{x < b}$ for all $a,b \in \exreals$. One easily sees that this is a Hausdorff topology (in fact it is $T_5$, which all order topologies are), so in particular singletons are closed and $\reals$ is open.

Since $\exreals$ is a topological space we can consider the Borel $\sigma$-algebra $\borel(\exreals)$. Before characterising this we recall two elementary results:
%
\begin{enumerate}
    \item If $(X,\calT)$ is a topological space and $A \subseteq X$ is any subspace, then $\borel(A) = \borel(X)_A$. That is, the Borel $\sigma$-algebra generated by the subspace topology on $A$ agrees with the restriction of the Borel $\sigma$-algebra on $X$ to $A$.

    \item If $(X,\calE)$ is a measurable space and $A \in \calE$, then
    %
    \begin{equation*}
        \calE
            = \set{E \union F}{E \in \calE_A, F \in \calE_{A^c}}.
    \end{equation*}
    %
    The inclusion \enquote{$\supseteq$} is obvious, and the other inclusion follows since if $B \in \calE$, then
    %
    \begin{equation*}
        B
            = (B \intersect A) \union (B \intersect A^c),
    \end{equation*}
    %
    and $B \intersect A \in \calE_A$ and $B \intersect A^c \in \calE_{A^c}$ by the definition of the subspace $\sigma$-algebra.
\end{enumerate}
%
This easily implies the following result:

\begin{proposition}
    The Borel $\sigma$-algebra on $\exreals$ is given by
    %
    \begin{equation*}
        \borel(\exreals)
            = \set[\big]{A \union S}{A \in \borel(\reals), S \subseteq \{\pm\infty\}}
            = \set{B \subseteq \exreals}{B \intersect \reals \in \borel(\reals)}.
    \end{equation*}
\end{proposition}

\begin{proof}
    To prove the first equality we only need to verify that $\borel(\{\pm\infty\}) = 2^{\{\pm\infty\}}$. But this is obvious since both sets $\{\infty\}$ and $\{-\infty\}$ are closed. The second equality easily follows from the first. (Note that we cannot simply use the fact that we can characterise $\borel(\reals)$ in terms of $\borel(\exreals)$, which we can do since $\reals$ is a subset of $\exreals$, since we are trying to do the exact opposite.)
\end{proof}

\nocite{*}

\printbibliography


\end{document}

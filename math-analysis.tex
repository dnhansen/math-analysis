% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Miscellaneous analysis notes}
\newcommand\docauthor{Danny Nyg√•rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{latex-sty/topologycommands}
% \usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{fit, patterns}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}
\setlist{
    listparindent=\parindent,
    parsep=0pt,
}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\setF}{\mathbb{F}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\borel}{\mathcal{B}}
\newcommand{\measurable}{\mathcal{M}}
\newcommand{\wto}{\Rightarrow}
\DeclarePairedDelimiter{\net}{\langle}{\rangle}
\newcommand{\strucS}{\mathfrak{S}}
\DeclarePairedDelimiter{\gen}{\langle}{\rangle} % Generating set
\newcommand{\frakL}{\mathfrak{L}}

\newenvironment{displaytheorem}{%
	\begin{displayquote}\itshape%
}{%
	\end{displayquote}%
}

% Break
\usepackage{adforn}
\newcommand\fleuronbreak{\fancybreak{\textcolor{linkcolor}{\adfhangingflatleafleft}}}


\newcommand{\posints}{\ints_{+}}
\newcommand{\exreals}{\closure{\reals}}


\begin{document}

\maketitle


\chapter{Basic concepts}

\section{The real numbers}

\begin{definition}[Peano systems]
    A \emph{Peano system} is a tuple $(X,0_X,S)$ such that $X$ is a set, $0_X$ is some element in $X$, and $S \colon X \to X$ is a map, that has the following properties:
    %
    \begin{enumdef}
        \item $S$ is injective.
        \item For all $x \in X$, $0_X \neq S(x)$.
        \item For all $A \subseteq X$, if $0_X \in A$ and $x \in A$ implies $S(x) \in A$, then $A = X$.
    \end{enumdef}
\end{definition}
%
We define a strict total order $<$ on a Peano system by declaring that $x < S(x)$, and that $x < y$ implies $S(x) < S(y)$, for all $x,y \in X$. Notice that with this ordering, $0_X$ is the minimum of $X$.

\begin{remark}
    Recall that an ordered set $X$ is \emph{well-ordered} if any nonempty subset of $X$ has a minimum. Contrary to what many textbook authors seem to suggest, the induction principle (3) is \emph{not} equivalent to being well-ordered, given the other axioms. While any Peano system is well-ordered, as we prove below, the converse does not hold: For instance, if $\naturals$ are the natural numbers (as we define below), the linear sum $\naturals \oplus \naturals$ is well-ordered but does not satisfy the induction principle (compare the ordinal $\omega + \omega$).

    The usual \enquote{proof} that being well-ordered implies the induction principle uses that the element $0_X$ above is the only element of $X$ without a predecessor, and with this modification to the Peano axioms well-ordering does imply the induction principle (and vice-versa).
\end{remark}

\begin{corollary}[Well-ordering of Peano systems]
    Any Peano system $(X,0_X,S)$ is well-ordered.
\end{corollary}

\begin{proof}
    Let $A \subseteq X$ be nonempty, and let $L$ be the subset of $X$ consisting of lower bounds of $A$. Then clearly $0_X \in L$, and if $a \in A$ then $S(a) \not\in L$. Hence $L \neq X$, so since $X$ is a Peano system there is an $m \in L$ such that $S(m) \not\in L$. We must then have $m \in A$, since otherwise $S(m) \in L$. Thus $m$ is the minimum of $A$.
\end{proof}


\begin{definition}[The real numbers]
    The \emph{real numbers} is the unique (up to isomorphism) complete ordered field, and is denoted $\reals$.
\end{definition}
%
We denote the operations on $\reals$ by the usual symbols. In particular, the ordering is denoted $\leq$, and the additive and multiplicative identities are denoted $0$ and $1$ respectively. Furthermore, if $A \subseteq \reals$ then we write e.g.
%
\begin{align*}
    x + A
        &\defn \set{x + a}{a \in A}, \\
    cA
        &\defn \set{ca}{a \in A},
\end{align*}
%
for $x,c \in \reals$. We further write $-A$ instead of $-1A$.

A subset $A$ of $\reals$ is said to be \emph{inductive} if $0 \in A$, and if $a \in A$ implies $a + 1 \in A$ for all $a \in \reals$. Any intersection of inductive subsets is itself inductive, so the intersection of all inductive subsets is inductive, in particular nonempty. This is clearly the smallest inductive subset of $\reals$, and is called the \emph{natural numbers} and is denoted $\naturals$. Notice that the positive axis $[0,\infty)$ is inductive, so all natural numbers are nonnegative and $0$ is the minimum of $\naturals$.

\begin{proposition}
    If $S \colon \naturals \to \naturals$ is given by $S(n) = n+1$, then $(\naturals,0,S)$ is a Peano system.
\end{proposition}

\begin{proof}
    Clearly $0 \in \naturals$, and $S$ is injective by cancellation of addition. Also, notice that $S(n) = n+1 > n \geq 0$ for all $n \in \naturals$, so $0$ is not in the image of $S$. Finally, let $A \subseteq \naturals$ be such that $0 \in A$ and $n \in A$ implies $S(n) \in A$. Then $A$ is an inductive subset of $\reals$, so $\naturals \subseteq A$. Hence $(\naturals,0,S)$ is a Peano system.
\end{proof}


Furthermore, the \emph{integers} is the set $\ints = \naturals \union (-\naturals)$. We further denote by $\posints$ the subset of strictly positive integers, which is just the set $\naturals \setminus \{0\}$. Finally, the \emph{rational numbers} is the set $\rationals$ of numbers on the form $m/n$, where $m,n \in \ints$ and $n > 0$. This is easily seen to be a countable subfield of $\reals$.


\begin{proposition}[The Archimedean property of $\reals$]
    The set $\naturals$ is unbounded in $\reals$. In particular, for any $r > 0$ there is an $n \in \posints$ such that $1/n < r$. Furthermore, for $x,y \in \reals$ with $x > 0$ there exists an $n \in \posints$ such that $nx > y$.
\end{proposition}

\begin{proof}
    Assume towards a contradiction that $\naturals$ is bounded. Then it has a supremum $s = \sup \naturals$, so $n \leq s$ for all $n \in \naturals$. But then also $n+1 \leq s$ for all $n \in \naturals$ since $\naturals$ is inductive, so $n \leq s-1$ for all $n \in \naturals$, contradicting that $s$ is the supremum of $\naturals$.

    If $r > 0$ then there is an $n \in \posints$ with $n > 1/r$. Since both $r$ and $n$ are positive, this implies that $1/n < r$.

    Finally let $x,y \in \reals$ with $x > 0$. There is an $n \in \posints$ with $n > y/x$, so since $x$ is positive this implies that $nx > y$ as desired.
\end{proof}


\begin{corollary}[Density of $\rationals$ in $\reals$]
    For each $a,b \in \reals$ with $a < b$ there is a rational number in the interval $(a,b)$. In particular, the set $\rationals$ is dense in $\reals$.
\end{corollary}

\begin{proof}
    First assume that $a \geq 0$. By [TODO Archimedes] there is an $n \in \posints$ with $1/n < b-a$. Furthermore, there is an $m \in \naturals$ such that $m/n > a$, so choose the smallest such $m$ in accordance with [TODO well-ordered]. Then $(m-1)/n \leq a$, and so
    %
    \begin{equation*}
        \frac{m}{n}
            = \frac{m-1}{n} + \frac{1}{n}
            < a + (b-a)
            = b,
    \end{equation*}
    %
    as desired.

    Instead assume that $a < 0$, and choose an $n \in \naturals$ with $n \geq -a$. The above then yields a rational number $q \in (a+n,b+n)$, so $q-n \in (a,b)$ is the desired rational number.
\end{proof}


\begin{lemma}[Bernoulli's inequality]
    Let $r \geq -1$. Then for all $n \in \naturals$,
    %
    \begin{equation*}
        (1+r)^n \geq 1 + nr,
    \end{equation*}
    %
    with equality iff $n \in \{0,1\}$ or $r = 0$.
\end{lemma}

\begin{proof}
    This is obvious if $n = 0$, so assume that the inequality holds for some $n \in \naturals$. Then
    %
    \begin{equation*}
        (1+r)^{n+1}
            = (1+r)(1+r)^n
            \geq (1+r)(1+nr)
            = 1 + (n+1)r + nr^2
            \geq 1 + (n+1)r,
    \end{equation*}
    %
    as desired. Notice that the final inequality is strict if $r \neq 0$ and $n \geq 1$.
\end{proof}


\begin{theorem}[Existence of roots]
    For all $a > 0$ and $n \in \posints$ there is a unique $r > 0$ such that $r^n = a$.
\end{theorem}

\begin{proof}
    Consider the function $f \colon \reals \to \reals$ given by $f(x) = x^n$. By [TODO Bernoulli + Archimedes], $f(x) \to \infty$ as $x \to \infty$, so there is an $x_0 > 0$ such that $x_0^n \geq a$. Furthermore, since $f(0) = 0$ and $f$ is continuous, the intermediate value theorem yields an $r \in [0,x_0]$ such that $r^n = a$ as desired.
\end{proof}


\section{The exponential function}

\begin{lemma}
    Consider the functional equation
    %
    \begin{equation*}
        f(x+y) = f(x)f(y),
    \end{equation*}
    %
    where $x,y \in \reals$, and $f \colon G \to \reals$ where $G$ is a subgroup of $\reals$. Any solution is either identically zero or everywhere nonzero. In particular, any nonzero solution is a group homomorphism $G \to R^*$.
    
    Let $a \in [0,\infty)$. If $G = \rationals$, then the equation has a unique solution $f \colon \rationals \to \reals$ given the restriction $f(1) = a$.

    Furthermore, if $f \colon G \to \reals$ solves the equation and $f$ is continuous at some point in $G$, then $f$ is continuous. Hence the equation has a unique solution $f \colon G \to \reals$ with $f(1) = a$ among the functions $G \to \reals$ that are continuous at any point.
\end{lemma}

\begin{proof}
    Let $f \colon G \to \reals$ be a solution. First notice that $f(0) = f(0+0) = f(0)f(0)$, so either $f(0) = 0$ or $f(0) = 1$. In the former case we have $f(x) = f(x+0) = f(x)f(0) = 0$ for all $x \in G$, so $f$ is identically zero. Hence assume that $f(0) = 1$. Then $1 = f(x-x) = f(x)f(-x)$, so $f(x)\inv = f(-x)$. In particular, $f$ is everywhere nonzero, hence a group homomorphism $G \to \reals^*$.
    
    Assume that $G = \rationals$ and let $g \colon \rationals \to \reals$ be another solution. For $n \in \posints$ we have
    %
    \begin{equation*}
        f(n)
            = f(\underbrace{1 + \cdots + 1}_{n})
            = \underbrace{f(1) \cdots f(1)}_{n}
            = f(1)^n
            = g(1)^n
            = \cdots
            = g(n).
    \end{equation*}
    %
    Furthermore, since $f(-n) = f(n)\inv = g(n)\inv = g(-n)$, $f$ and $g$ agree on $\ints$. Next let $m \in \ints$ and notice that
    %
    \begin{equation*}
        f(\tfrac{m}{n})^n
            = f(n \tfrac{m}{n})
            = f(m)
            = g(m)
            = g(\tfrac{m}{n})^n.
    \end{equation*}
    %
    Hence $f$ and $g$ agree on $\rationals$ as claimed.

    Next assume that $f \colon G \to \reals$ is a solution that is continuous at some point. If $f$ is identically zero then it is obviously continuous, and if not then it is a group homomorphism. But both $G$ and $\reals^*$ are topological groups, so this implies that $f$ is continuous everywhere.
\end{proof}


\begin{theorem}
    Given $b > 0$ there exists a continuous function $E_b \colon \reals \to \reals$ such that $E_b(m/n) = (b^m)^{1/n}$ for all $m,n \in \ints$ with $n > 0$. This function has the property that
    %
    \begin{equation*}
        E_b(x+y) = E_b(x) E_b(y)
    \end{equation*}
    %
    for $x,y \in \reals$. Furthermore, $E_b$ is strictly increasing if $b > 1$, strictly decreasing if $b < 1$. If $b \neq 1$, then $E_{1/b}(x) = E_b(-x)$ and $E_b(\reals) = (0,\infty)$.
    
    The function $E_b$ is the unique function that satisfies $E_b(m/n) = (b^m)^{1/n}$ and that is continuous at $0$. We write $b^x = E_b(x)$ for $x \in \reals$.
\end{theorem}
%
The function $E_b$ is also denoted $\exp_b$ and is called the \emph{exponential function with base $b$}.

\begin{proof}
    We first assume that $b > 1$. The proof of the existence of the function $E_b$ in this case is by the following stages:
    %
    \begin{enumerate}
        \item Given $m,n,p,q \in \ints$ with $n,q > 0$ and $r = m/n = p/q$, then
        %
        \begin{equation*}
            (b^m)^{1/n}
                = (b^p)^{1/q}.
        \end{equation*}
        %
        Thus defining $E_b(r) = (b^m)^{1/n}$ makes sense.

        \item If $r,s \in \rationals$, then $E_b(r+s) = E_b(r) E_b(s)$. In particular, $E_b$ is strictly increasing on $\rationals$. Furthermore, $E_b(r) \to \infty$ as $r \to \infty$, and $E_b(r) \to 0$ as $r \to -\infty$.
        
        \item The map $E_b$ is continuous on $\rationals$.
        
        \item For $x \in \reals$, let
        %
        \begin{equation*}
            L_x
                = \set{E_b(t)}{t \in \rationals, t \leq x}.
        \end{equation*}
        %
        Then $E_b(r) = \sup L_r$ for $r \in \rationals$, so it makes sense to define $E_b(x) = \sup L_x$ for all $x \in \reals$. Hence $E_b$ is strictly increasing and therefore continuous on $\reals$. In particular, $E_b(\reals) = (0,\infty)$.

        \item For $x,y \in \reals$ we have $E_b(x+y) = E_b(x) E_b(y)$.
    \end{enumerate}
    
    \begin{proofsec}
        \item[Part (1)]
        Notice that $mq = pn$, so
        %
        \begin{equation*}
            \bigl[ (b^m)^{1/n} \bigr]^{pn}
                = (b^m)^p
                = (b^p)^m
                = \bigl[ (b^p)^{1/q} \bigr]^{mq}
                = \bigl[ (b^p)^{1/q} \bigr]^{pn}.
        \end{equation*}
        %
        The (positive) $pn$th root of this number is unique, so $(b^m)^{1/n} = (b^p)^{1/q}$.

        \item[Part (2)]
        Write $r = m/n$ and $s = p/q$ for appropriate $m,n,p,q \in \ints$ with $n,q > 0$. Then
        %
        \begin{equation*}
            [ (b^m)^{1/n} (b^p)^{1/q} ]^{nq}
                = b^{mq} b^{pn}
                = b^{mq + pn}.
        \end{equation*}
        %
        Taking the $nq$th root implies that
        %
        \begin{equation*}
            E_b(r) E_b(s)
                = (b^m)^{1/n} (b^p)^{1/q}
                = (b^{mq+pn})^{1/nq}
                = E_b(r + s),
        \end{equation*}
        %
        where we in the last equality use that
        %
        \begin{equation*}
            r + s
                = \frac{m}{n} + \frac{p}{q}
                = \frac{mq + pn}{nq}.
        \end{equation*}
        %
        To see that $E_b$ is increasing on $\rationals$, notice that the assumption that $b > 1$ implies that $E_b(s) > 1$ for $s > 0$. If also $r \in \rationals$, then since $E_b(r) \neq 0$ by [TODO lemma], we have
        %
        \begin{equation*}
            E_b(r + s)
                = E_b(r) E_b(s)
                > E_b(r),
        \end{equation*}
        %
        so $E_b$ is strictly increasing.

        Finally, by [TODO Bernoulli] $b^m \to \infty$ as $m \to \infty$. Since $1 = b^m b^{-m}$, this implies that $b^m \to 0$ as $m \to -\infty$.
        
        \item[Part (3)]
        We begin by showing that $E_b$ is continuous from above at $0$. Since $E_b$ is monotonic and $E_b(0) = 1$, it suffices to show that $\lim_{n\to\infty} E_b(1/n) = 1$. Clearly $1/n \downarrow 0$, so assume that $E_b(1/n)$ did not converge to $1$. Then there would be some $\epsilon > 0$ such that $E_b(1/n) \geq 1 + \epsilon$, i.e. $b \geq (1 + \epsilon)^n$, for all $n \in \naturals$. But by [TODO Bernoulli's inequality] this is impossible, so we must have $E_b(1/n) \to 1$. Since $E_b(-m/n) = (b^{-m})^{1/n} = E_{1/b}(m/n)$, $E_b$ is also continuous from below at $0$. [TODO lemma] then implies that $E_b$ is continuous everywhere.

        \item[Part (4)]
        Since $E_b$ is increasing on $(\infty,r] \intersect \rationals$, it obtains its maximum at $r$. Hence we obviously have $E_b(r) = \sup L_r$.

        If $x \leq y$ then $L_x \subseteq L_y$, and hence $E_b(x) \leq E_b(y)$. Thus $E_b$ is increasing on $\reals$. It is in fact strictly increasing, since if $x < y$ and $p < q$ are rational numbers in the interval $(x,y)$, then since $E_b$ is strictly increasing on $\rationals$ we have
        %
        \begin{equation*}
            E_b(x)
                \leq E_b(p)
                < E_b(q)
                \leq E_b(y).
        \end{equation*}
        %
        Furthermore, since $E_b$ continuous on a dense subset of $\reals$, it is clearly also continuous on $\reals$. Finally, the claim $E_b(\reals) = (0,\infty)$ follows from the limit results in part (2).

        \item[Part (5)]
        Let $(r_n)$ and $(s_n)$ be sequences in $\rationals$ with limits $x$ and $y$ respectively. Then $r_n+s_n$ converges to $x+y$, so
        %
        \begin{equation*}
            E_b(x+y)
                = \lim_{n\to\infty} E_b(r_n + s_n)
                = \lim_{n\to\infty} E_b(r_n) E_b(s_n)
                = E_b(x) E_b(y),
        \end{equation*}
        %
        as desired.
    \end{proofsec}

    This proves existence when $b > 1$. If $b = 1$ then existence is obvious, so assume that $b < 1$. In this case we simply define $E_b(x) = E_{1/b}(-x)$. This clearly has the desired properties. Uniqueness is immediate from [TODO lemma].
\end{proof}
%
Let $b \neq 1$. Then $E_b$ is a bijection onto $(0,\infty)$, so it has an inverse $L_b \colon (0,\infty) \to \reals$. This is called the \emph{logarithm with base $b$}, and is also denoted $\log_b$. This satisfies a functional equation similar to the one satisfied by $E_b$: For $x,y \in (0,\infty)$ we have
%
\begin{equation*}
    xy
        = E_b(L_b(x)) E_b(L_b(y))
        = E_b(L_b(x) + L_b(y)),
\end{equation*}
%
and applying $L_b$ to both sides yields
%
\begin{equation*}
    L_b(xy) = L_b(x) + L_b(y).
\end{equation*}
%
For $n \in \naturals$ this in particular implies that $L_b(x^n) = n L_b(x)$.

\fleuronbreak

We next turn to alternative characterisations of the exponential function. We first define the number
%
\begin{equation*}
    \e = \sum_{n=0}^\infty \frac{1}{n!}.
\end{equation*}
%
The exponential function $E_{\e} = \exp_{\e}$ with base $\e$ is simply called the \emph{exponential function} and is denoted $\exp$. Similarly, the logarithm $L_{\e} = \log_{\e}$ with base $\e$ is called the \emph{natural logarithm} and is denoted $\log$.

\begin{proposition}
    For $x \in \reals$ we have
    %
    \begin{equation*}
        \exp x
            = \sum_{n=0}^\infty \frac{x^n}{n!}.
    \end{equation*}
    %
    In particular $\exp' = \exp$ and $\log' x = 1/x$ for $x \in (0,\infty)$.
\end{proposition}

\begin{proof}
    Denote the right-hand side by $E(x)$. First let $f(x) = E(x)E(-x)$ and notice that, since $E' = E$, $f' = 0$. Hence $E(x)E(-x) = 1$. Next define $g(x) = E(x)E(y)E(-(x+y))$ for $y \in \reals$. Using the above, one easily sees that $g' = 0$, so $g(x) = g(0) = 1$. Hence $E(x)E(y) = E(x+y)$, so [TODO lemma] and [TODO $\e^x$ satisfies func eq] implies that $\exp e = E(x)$ as claimed.
\end{proof}


\begin{proposition}
    Let $x \in \reals$. Then
    %
    \begin{equation*}
        \exp x
            = \lim_{n \to \infty} \biggl( 1 + \frac{x}{n} \biggr)^n.
    \end{equation*}
\end{proposition}

\begin{proof}
    This is obvious if $x = 0$, so assume that $x \neq 0$. Then we have
    %
    \begin{equation*}
        \lim_{n \to \infty} n \log(1 + \tfrac{x}{n})
            = x \lim_{n \to \infty} \frac{\log(1 + \tfrac{x}{n}) - \log 1}{x/n}
            = x \log' 1
            = x.
    \end{equation*}
    %
    (Notice that $1 + \tfrac{x}{n} > 0$ if $n$ is large enough.) Continuity of $\exp$ then implies that
    %
    \begin{equation*}
        \lim_{n \to \infty} \biggl( 1 + \frac{x}{n} \biggr)^n
            = \exp \biggl( \lim_{n \to \infty} n \log(1 + \tfrac{x}{n}) \biggr)
            = \exp x,
    \end{equation*}
    %
    as desired.
\end{proof}


\section{Sequences of real numbers}

% \begin{definition}
%     Let $X$ be a set. A \emph{sequence} in $X$ is a map $a \colon \naturals \to X$. For $n \in \naturals$, we usually write $a_n$ for $a(n)$ and denote $a$ by $(a_n)_{n\in\naturals}$ or simply $(a_n)$.
% \end{definition}


% \begin{definition}
%     Let $(S,\rho)$ be a metric space, and let $(a_n)_{n\in\naturals}$ be a sequence in $S$. We say that $(a_n)$ \emph{converges} to a point $a \in S$ if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $n \geq N$ implies that $\rho(a_n,a) < \epsilon$. In this case we call $a$ the \emph{limit} of $(a_n)$ and write $a_n \to a$ as $n \to \infty$, and we say that $(a_n)$ is \emph{convergent}.

%     Furthermore, $(a_n)$ is called a \emph{Cauchy sequence} if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $m,n \geq N$ implies that $\rho(a_m,a_n) < \epsilon$. If every Cauchy sequence in $S$ is convergent, then $S$ is said to be \emph{complete}.
% \end{definition}
% %
% Notice that limits of sequences in metric spaces are unique. It is also clear that convergent sequences are Cauchy, and that Cauchy sequences are bounded: We say that a sequence $(a_n)_{n\in\naturals}$ in a metric space is \emph{bounded} if the set $\set{a_n}{n\in\naturals}$ is bounded.

% If $(X,\leq)$ is a poset, a sequence $(a_n)_{n\in\naturals}$ in $X$ is \emph{increasing} (\emph{decreasing}) if $m \leq n$ implies $a_m \leq a_n$ ($a_m \geq a_n$) for all $m,n \in \naturals$. If $m < n$ implies $a_m < a_n$ ($a_m > a_n$), then $(a_n)$ is \emph{strictly} increasing (decreasing). A sequence that is either (strictly) increasing or (strictly) decreasing is called \emph{(strictly) monotonic}.

% Given a sequence $(a_n)_{n\in\naturals}$ in a set $X$ and a strictly increasing sequence $(n_k)_{k\in\naturals}$ in $\naturals$, the sequence $(a_{n_k})_{k\in\naturals}$ is called a \emph{subsequence} of $(a_n)$. In particular, every sequence is a subsequence of itself.

\begin{lemma}
    Let $(a_n)_{n\in\naturals}$ be a sequence in a metric space $(S,\rho)$. If $(a_n)$ is both Cauchy and has a convergent subsequence, then $(a_n)$ itself is convergent.
\end{lemma}

\begin{proof}
    Let $(a_{n_k})_{k\in\naturals}$ be a convergent subsequence of $(a_n)$, and let $\epsilon > 0$. Choose $N_1, N_2 \in \naturals$ such that
    %
    \begin{equation*}
        m,n \geq N_1
        \quad \implies \quad
        \rho(a_m, a_n) < \frac{\epsilon}{2}
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        k \geq N_2
        \quad \implies \quad
        \rho(a_{n_k}, a) < \frac{\epsilon}{2},
    \end{equation*}
    %
    where $a \in S$ is the limit of $(a_{n_k})$. For $n \geq N_1 \join N_2$ we thus have
    %
    \begin{equation*}
        \rho(a_n, a)
            \leq \rho(a_n, a_m) + \rho(a_m, a)
            < \frac{\epsilon}{2} + \frac{\epsilon}{2}
            = \epsilon,
    \end{equation*}
    %
    showing that $a_n \to a$ as $n \to \infty$.
\end{proof}


\begin{proposition}
    Let $(a_n)_{n\in\naturals}$ be a monotonic sequence in $\reals$. Then $(a_n)$ is convergent if and only if it is bounded, in which case it converges to $\sup_{n\in\naturals} a_n$ if it is increasing and $\inf_{n\in\naturals} a_n$ if it is decreasing.
\end{proposition}

\begin{proof}
    If $(a_n)$ is convergent then it is bounded, so assume that it is bounded and let $\epsilon > 0$. For definiteness we assume that it is increasing and let $s = \sup_{n\in\naturals} a_n$. By definition of $s$ there exists an $N \in \naturals$ such that $s - a_N < \epsilon$. Since $(a_n)$ is increasing and $s$ is an upper bound of the sequence, we thus have
    %
    \begin{equation*}
        0 \leq s - a_n < \epsilon
    \end{equation*}
    %
    for all $n \geq N$, proving that $a_n \to s$.
\end{proof}

\begin{lemma}
    Every sequence in $\reals$ has a monotonic subsequence.
\end{lemma}

\begin{proof}
    Let $(a_n)_{n\in\naturals}$ be a sequence in $\reals$. We say that $n \in \naturals$ is a \emph{peak} if $a_n \geq a_m$ for all $m \geq n$. If $(a_n)$ has infinitely many peaks, the subsequence consisting of these constitute a decreasing subsequence.

    Hence we assume that $(a_n)$ only has finitely many peaks. We construct an increasing sequence $(n_k)_{n\in\naturals}$ in $\naturals$ as follows: Let $n_1 \in \naturals$ be such that all peaks are strictly less than $n_1$, and assume that $n_1, \ldots, n_{k-1}$ have been chosen such that $a_1 \leq \cdots \leq a_{n_{k-1}}$. Since $a_{n_{k-1}}$ is not a peak there is an $n' > n_{k-1}$ such that $a_{n_{k-1}} < a_{n'}$. Letting $n_k = n'$ we obtain an increasing subsequence $(a_{n_k})$ of $(a_n)$, proving the claim.
\end{proof}

\begin{theorem}[The Bolzano--Weierstrass theorem]
    Every subset of $\reals^d$ is sequentially compact if and only if it is closed and bounded.
\end{theorem}
%
We recall that a topological space $X$ is \emph{sequentially compact} if every sequence in $X$ has a convergent subsequence.

\begin{proof}
    We begin with the case $d = 1$. Let $A \subseteq \reals$ be closed and bounded, and let $(a_n)_{n\in\naturals}$ be a sequence in $A$. Let $(a_{n_k})$ be a monotonic subsequence of $(a_n)$, and notice that $(a_{n_k})$ is convergent since it is bounded.

    The case for general $d$ follows by induction in $d$, by noticing that a sequence in $\reals^d$ converges if and only if each coordinate sequence converges.

    For the converse, let $A \subseteq \reals^d$ be sequentially compact. If $A$ were not bounded we could choose $a_n \in A \intersect B(0,n)$ for all $n \in \naturals$, yielding a sequence $(a_n)$ with no convergent subsequence. Furthermore, if $(a_n)$ is a sequence in $A$ converging to a point $a \in \reals^d$, then it has a subsequence converging to a point $a' \in A$, and by [lemma] we must have $a = a'$. Thus $A$ is also closed.
\end{proof}


\begin{theorem}[Completeness of $\reals^d$]
    The Euclidean space $\reals^d$ is complete.
\end{theorem}

\begin{proof}
    Let $(a_n)_{n\in\naturals}$ be a Cauchy sequence in $\reals^d$. Hence it is bounded, and so it has a convergent subsequence by the Bolzano--Weierstrass theorem. But then $(a_n)$ itself converges by [lemma], so $\reals^d$ is complete.
\end{proof}


\begin{theorem}[The Heine--Borel theorem]
    Every subset of $\reals^d$ is compact if and only if it is closed and bounded.
\end{theorem}

\begin{proof}
    Of course every compact set is closed in any Hausdorff space and bounded in any metric space, so we only consider the other implication.
    
    We first show that closed and bounded intervals are compact. Consider the interval $[a,b]$, and let $\calU$ be an open cover of $[a,b]$. Define the set
    %
    \begin{equation*}
        A
            = \set[\big]{x \in [a,b]}{\text{$[a,x]$ has a finite subcover in $\calU$}}.
    \end{equation*}
    %
    We clearly have $a \in A$ since a point is covered by a single set in $\calU$. If $s = \sup A$ then $a \leq s \leq b$. Suppose that $s < b$ and choose a set $U \in \calU$ with $s \in U$. There exist $r,t \in U$ such that $r < s < t$, and so $r \in A$. Let $\calU'$ denote a finite subcover of $[a,r]$ in $\calU$. Then $\calU' \union \{U\}$ is a finite subcover of $[a,t]$, contradicting the assumption that $s < b$. Hence $s = b$.

    Next, choose $V \in U$ with $b \in V$, and let $c \in V$ with $c < b$. Then $c \in A$, and adjoining $V$ to a finite subcover of $[a,c]$ yields a finite subcover of $[a,b]$, so $b \in A$. Thus $[a,b]$ is compact.

    Finally, let $K \subseteq \reals^d$ be closed and bounded. Since it is bounded it is contained in some cube $[-a,a]^d$. But this cube is a product of compact sets and hence compact, so $K$ is a closed subset of a compact set. The claim follows.
\end{proof}


% If $A$ is a subset of a metric space $S$, recall that $A$ is \emph{totally bounded} if, for every $\epsilon > 0$, $A$ can be covered by finitely many open balls of radius $\epsilon$.

% \begin{theorem}
%     If $A$ is a subset of a metric space $(S,\rho)$, then the following are equivalent:
%     %
%     \begin{enumthm}
%         \item \label{enum:complete-totally-bounded} $A$ is complete and totally bounded.
%         \item \label{enum:sequentially-compact} $A$ is sequentially compact.
%         \item \label{enum:compact} $A$ is compact.
%     \end{enumthm}
% \end{theorem}

% \begin{proof}
% \begin{proofsec}
%     \item[\subcref{enum:complete-totally-bounded} $\implies$ \subcref{enum:sequentially-compact}]
%     Assume that $A$ is complete and totally bounded, and let $(x_n)_{n\in\naturals}$ be a sequence in $A$. Now $A$ can be covered by finitely many balls of radius $1$, at least one of which, say $B_1$, contains $x_n$ for infinitely many $n$, say for $n \in N_1 \subseteq \naturals$. Similarly, $A \intersect B_1$ may be covered by finitely many balls of radius $1/2$, and again there is a ball $B_2$ containing $x_n$ for infinitely many $n \in N_1$, say for $n \in N_2$. Continuing recursively we obtain a sequence of balls $B_i$ of radius $1/i$ and a decreasing sequence $(N_i)_{i\in\naturals}$ of infinite subsets of $\naturals$ such that $x_n \in B_i$ for $n \in N_i$.

%     Next, choose a strictly increasing sequence $(n_i)_{i\in\naturals}$ of naturals numbers such that $n_i \in N_i$. Then $\rho(x_{n_i}, x_{n_j}) < 2/i$ for $i \leq j$, so $(x_{n_i})_{i\in\naturals}$ is a Cauchy sequence., and since $A$ is complete it has a limit in $A$.

%     \item[\subcref{enum:sequentially-compact} $\implies$ \subcref{enum:complete-totally-bounded}]
%     Assume that $A$ is sequentially compact. We first show that $A$ is complete, so let $(x_n)_{n\in\naturals}$ be a Cauchy sequence in $A$. This has a subsequence that converges to a point $x$ in $A$, so [lemma] implies that $(x_n)$ also converges to $x$.
    
%     Now suppose that $A$ is not totally bounded, and let $\epsilon > 0$ be such that $A$ cannot be covered by finitely many $\epsilon$-balls. We construct a sequence $(x_n)_{n\in\naturals}$ in $A$ as follows: Choose any $x_1 \in A$, and given $x_1, \ldots, x_n$ choose $x_{n+1} \in A \setminus \bigunion_{i=1}^n B(x_i,\epsilon)$. Then $\rho(x_m,x_n) \geq \epsilon$ for all $m,n \in \naturals$ with $m \neq n$, so $(x_n)$ has no convergent subsequence.
    
%     \item[\subcref{enum:complete-totally-bounded} \& \subcref{enum:sequentially-compact} $\implies$ \subcref{enum:compact}]
%     Suppose that $A$ is complete, totally bounded and sequentially compact, and let $\calU$ be an open cover of $A$. It suffices to show that there some $\epsilon > 0$ such that any $\epsilon$-ball intersecting $A$ is contained in some $U \in \calU$, since $A$ can be covered by finitely many such balls.

%     Assume towards a contradiction that for every $n \in \naturals$ there is a ball $B_n$ of radius $1/n$ intersecting $A$ such that $B_n$ is contained in no $U \in \calU$. Picking $x_n \in B_n$ for $n \in \naturals$, we may assume that the sequence $(x_n)_{n\in\naturals}$ converges to some $x \in A$ by passing to an appropriate subsequence. Then $x \in U$ for some $U \in \calU$, and since $U$ is open there is an $\epsilon > 0$ such that $B(x,\epsilon) \subseteq U$. Choosing $n \in \naturals$ large enough that $\rho(x_n,x) < \epsilon/2$ and $1/n < \epsilon/2$, we have $B_n \subseteq B(x,\epsilon) \subseteq U$, which is a contradiction.

%     \item[\subcref{enum:compact} $\implies$ \subcref{enum:sequentially-compact}]
%     We prove the contrapositive, so assume that $A$ is not sequentially compact, and let $(x_n)_{n\in\naturals}$ be a sequence in $A$ with no convergent subsequence. Every $x \in A$ is then contained in an open ball $B_x$ containing $x_n$ for only finitely many $n$. Thus $\{B_x\}_{x \in A}$ is an open cover of $A$ with no finite subcover, and $A$ is not compact.
% \end{proofsec}
% \end{proof}


% \section{Limit superior and limit inferior}

% \begin{proposition}
%     Let $(x_n)_{n \in \naturals}$ and $(y_n)_{n \in \naturals}$ be sequences in $\exreals$.
%     %
%     \begin{enumprop}
%         \item $ \limsup_{n \to \infty} (-x_n) = - \limsup_{n \to \infty} x_n$.

%         \item $\limsup_{n \to \infty} x_n$ is the largest subsequential limit of $(x_n)$, and $\liminf_{n \to \infty} x_n$ is the smallest. In particular, $(x_n)$ is convergent if and only if
%         %
%         \begin{equation*}
%             x
%                 \defn \limsup_{n \to \infty} x_n
%                 = \liminf_{n \to \infty} x_n,
%         \end{equation*}
%         %
%         in which case the limit of $(x_n)$ is $x$.
        
%         \item We have the inequalities
%         %
%         \begin{equation*}
%             \limsup_{n \to \infty} (x_n + y_n)
%                 \leq \limsup_{n \to \infty} x_n + \limsup_{n \to \infty} y_n,
%         \end{equation*}
%         %
%         and
%         %
%         \begin{equation*}
%             \liminf_{n \to \infty} (x_n + y_n)
%                 \geq \liminf_{n \to \infty} x_n + \liminf_{n \to \infty} y_n.
%         \end{equation*}

%         \item If $(y_n)$ converges to a point in $\reals$, then
%         %
%         \begin{equation*}
%             \limsup_{n \to \infty} (x_n + y_n)
%                 = \limsup_{n \to \infty} x_n + \lim_{n \to \infty} y_n,
%         \end{equation*}
%         %
%         and
%         %
%         \begin{equation*}
%             \liminf_{n \to \infty} (x_n + y_n)
%                 = \liminf_{n \to \infty} x_n + \lim_{n \to \infty} y_n.
%         \end{equation*}
%     \end{enumprop}
% \end{proposition}

% \begin{proof}
%     Notice that all claims about limits inferior will follow by [TODO ref] from the corresponding properties of limits superior.
    
%     - This follows from basic properties of limits, suprema and infima.

%     - Let $s = \limsup_{n \to \infty} x_n$. We first show that $s$ is greater than any subsequential limit of $(x_n)$, so let $(x_{n_k})_{k \in \naturals}$ be a subsequence converging to $x \in \reals$. Notice that $n_k \geq k$ for all $k$. For $K \in \naturals$ and $k \geq K$ we thus have
%     %
%     \begin{equation*}
%         x_{n_k}
%             \leq \sup_{m \geq n_k} x_m
%             \leq \sup_{m \geq n_K} x_m
%             \leq \sup_{m \geq K} x_m,
%     \end{equation*}
%     %
%     which implies that $x \leq \sup_{m \geq K} x_m$. Taking the limit $K \to \infty$ implies that $x \leq s$ as claimed.

%     Next we show that $(x_n)$ has a subsequence converging to $s$. First let $n_0 = 0$. For $k \in \naturals$ we then recursively choose $n_k \in \naturals$ such that $n_k > n_{k-1}$, and such that
%     %
%     \begin{equation*}
%         - \frac{1}{k} + \sup_{n > n_{k-1}} x_n
%             < x_{n_k}
%             \leq \sup_{n > n_{k-1}} x_n.
%     \end{equation*}
%     %
%     It then easily follows that $x_{n_k} \to s$.

%     - This follows from the inequality
%     %
%     \begin{equation*}
%         \sup_{n \geq k} (x_n + y_n)
%             \leq \sup_{n \geq k} x_n + \sup_{n \geq k} y_n.
%     \end{equation*}

%     - By [TODO ref] it suffices to show the inequality \enquote{$\geq$}. Let $(x_{n_k})$ be a subsequence of $(x_n)$ converging to $\limsup_{n \to \infty} x_n$. Then $(x_{n_k} + y_{n_k})$ is a subsequence of $(x_n + y_n)$, so
%     %
%     \begin{align*}
%         \limsup_{n \to \infty} (x_n + y_n)
%             &\geq \lim_{k \to \infty} (x_{n_k} + y_{n_k})
%              = \lim_{k \to \infty} x_{n_k} + \lim_{k \to \infty} y_{n_k} \\
%             &= \limsup_{n \to \infty} x_n + \lim_{n \to \infty} y_n.
%     \end{align*}
% \end{proof}


\section{Infinite series}

\begin{proposition}[Cauchy criterion for series]
    Any series $\sum_{n=1}^\infty a_n$ with complex terms converges if and only if for every $\epsilon > 0$ there exists an $N \in \naturals$ such that
    %
    \begin{equation*}
        m \geq n \geq N
        \quad \implies \quad
        \abs[\bigg]{ \sum_{k=n}^m a_k } < \epsilon.
    \end{equation*}
\end{proposition}

\begin{proof}
    Let $s_n = \sum_{k=1}^n a_n$. By completeness of $\complex$, the series converges if and only if $(s_n)_{n \in \naturals}$ is a Cauchy sequence. This is the case just when for every $\epsilon > 0$ there exists an $N \in \naturals$ such that $m \geq n \geq N$ implies that
    %
    \begin{equation*}
        \epsilon
            > \abs{s_m - s_n}
            = \abs[\bigg]{ \sum_{k=n+1}^m a_k },
    \end{equation*}
    %
    as claimed.
\end{proof}


\begin{corollary}
    If a series $\sum_{n=1}^\infty a_n$ with complex terms converges, then $a_n \to 0$ as $n \to \infty$.
\end{corollary}

\begin{proof}
    Let $m = n + 1$ in [TODO ref].
\end{proof}


\begin{corollary}
    An absolutely convergent series with complex terms is convergent.
\end{corollary}

\begin{proof}
    This follows from [TODO ref] by noting that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{k=n}^m a_k }
            \leq \sum_{k=n}^m \abs{a_k}
    \end{equation*}
    %
    for all $m,n \in \naturals$ with $m \geq n$.
\end{proof}


\begin{proposition}[The ratio test]
    Let $\sum_{n=1}^\infty a_n$ be a series with complex terms such that $a_n \neq 0$ for large enough $n$. Define
    %
    \begin{equation*}
        r
            = \liminf_{n \to \infty} \abs[\bigg]{ \frac{a_{n+1}}{a_n} }
        \quad \text{and} \quad
        R
            = \limsup_{n \to \infty} \abs[\bigg]{ \frac{a_{n+1}}{a_n} }.
    \end{equation*}
    %
    If $R < 1$ then the series converges absolutely, and if $r > 1$ then the series diverges.
\end{proposition}

\begin{proof}
    First assume that $R < 1$ and choose $\rho \in (R,1)$. Then there exists an $N \in \naturals$ such that $n \geq N$ implies that $\abs{a_{n+1}} \leq \rho \abs{a_n}$. In particular we have $\abs{a_{N+n}} \leq \rho^n \abs{a_N}$, so
    %
    \begin{equation*}
        \sum_{n=N+1}^\infty \abs{a_n}
            = \sum_{n=1}^\infty \abs{a_{N+n}}
            \leq \sum_{n=1}^\infty \rho^n \abs{a_N}
            = \abs{a_N} \frac{\rho}{1-\rho}
            < \infty.
    \end{equation*}
    %
    Hence the series converges.

    Next assume that $r > 1$. Then there exists an $N \in \naturals$ such that $n \geq N$ implies that $\abs{a_{n+1}} > \abs{a_n}$, so the terms do not approach zero. Hence the series diverges by [TODO cor ref].
\end{proof}


\begin{proposition}[The root test]
    Consider a series $\sum_{n=1}^\infty a_n$ with complex terms, and let
    %
    \begin{equation*}
        C
            = \limsup_{n \to \infty} \sqrt[n]{\abs{a_n}}.
    \end{equation*}
    %
    If $C < 1$ then the series converges absolutely, and if $C > 1$ then the series diverges.
\end{proposition}

\begin{proof}
    First assume that $C < 1$. Let $\rho \in (C,1)$ and choose $N \in \naturals$ such that $n \geq N$ implies $\sqrt[n]{\abs{a_n}} \leq \rho$. This implies that $\abs{a_n} \leq \rho^n$, but since the geometric series $\sum_{n=N}^\infty \rho^n$ converges, so does the original series.

    On the other hand, if $C > 1$ then $\sqrt[n]{\abs{a_n}} > 1$ for infinitely many $n$. But then the terms $a_n$ cannot go to zero, so [TODO cor ref] implies that the series diverges.
\end{proof}


\begin{corollary}[The Cauchy--Hadamard theorem]
    Let $\sum_{n=0}^\infty c_n(z - a)^n$ be a power series with complex coefficients and $a \in \complex$, and let
    %
    \begin{equation*}
        R
            = \frac{1}{ \limsup_{n \to \infty} \sqrt[n]{\abs{c_n}} },
    \end{equation*}
    %
    where $R = \infty$ if the denominator is zero. The series then converges when $\abs{z-a} < R$ and diverges when $\abs{z-a} > R$.
\end{corollary}

\begin{proof}
    Let $C$ be as in [TODO ref root test], and notice that
    %
    \begin{equation*}
        C
            = \limsup_{n \to \infty} \sqrt[n]{\abs{c_n(z-a)^n}}
            = \abs{z-a} \limsup_{n \to \infty} \sqrt[n]{\abs{c_n}}
            = \frac{\abs{z-a}}{R},
    \end{equation*}
    %
    where $C = 0$ if $R = \infty$ and $C = \infty$ if $R = 0$. The claim immediately follows.
\end{proof}


\section{Unordered sums}

\newcommand{\finsubsets}[1]{\calF(#1)}

If $X$ is a set, then we denote by $\finsubsets{X}$ the collection of finite subsets of $X$. This is directed by set inclusion.

\begin{definition}
    Let $g \colon I \to G$ be a map from a set $I$ into an abelian topological group $G$, and write $g(i) = g_i$. Define a net $S \colon \finsubsets{I} \to G$ by $S(J) = \sum_{i \in J} g_i$. If $S$ has a limit, then such a limit is called an \emph{unordered sum} of $g$ over $I$, and if this is unique then it is denoted
    %
    \begin{equation*}
        \sum_{i \in I} g_i.
    \end{equation*}
\end{definition}
%
Notice that since the sum is \emph{unordered}, we cannot generalise this definition to nonabelian groups. Also since the sum is unordered, we would expect that if we rearrange the terms in the sum, its convergence properties should stay the same. More precisely, if the unordered sum $\sum_{i \in I} g_i$ converges to $g \in G$ and $\phi \colon I \to I$ is a bijection, then we would expect that $\sum_{i \in I} g_{\phi(i)}$ also converges to $g$. But in the notation above, $S \circ \phi$ is clearly a (Willard) subnet of $S$, so this follows.

\begin{lemma}
    \label{lem:unordered-sum-subsequence}
    Let $G$ be an abelian topological group, and assume that the unordered sum $\sum_{i \in I} g_i$ converges to $g \in G$. If $(J_n)_{n\in\naturals}$ is an increasing sequence in $\finsubsets{I}$ with $I = \bigunion_{n\in\naturals} J_n$, then the sequence $(\sum_{i \in J_n} g_i)_{n\in\naturals}$ converges to $g$.
\end{lemma}

\begin{proof}
    Simply notice that the sequence $(\sum_{i \in J_n} g_i)_{n\in\naturals}$ is a (Willard) subnet of $\sum_{i \in I} g_i$.
\end{proof}


\begin{proposition}
    Let $G$ be an abelian topological group. If the unordered sum $\sum_{n \in \naturals} g_n$ converges to $g$, then the series $\sum_{n=1}^\infty g_n$ converges to $g$. Furthermore, every rearrangement of $\sum_{n=1}^\infty g_n$ converges to $g$.
\end{proposition}

\begin{proof}
    For $n \in \naturals$ let $J_n = \{1, \ldots, n\}$ and consider the sequence $(s_n)_{n\in\naturals}$ given by $s_n = \sum_{i \in I_n} g_i$. Notice that $s_n = \sum_{i=1}^n g_i$, so the claim follows from \cref{lem:unordered-sum-subsequence}.
\end{proof}


\newcommand{\calN}{\mathcal{N}}
\newcommand{\nhoods}[1]{\calN_{#1}}
\newcommand{\pnhoods}[1]{\calN'_{#1}}
\newcommand{\limitval}{\calL}
% [TODO: consider giving \nhoods an optional parameter for the topological space. E.g. \calN_space(point).]

\begin{lemma}
    An unordered sum $\sum_{i \in I} g_i$ in $G$ is Cauchy if and only if the sum has the following property: For every neighbourhood $N$ of $0$ there exists a $J \in \finsubsets{I}$ such that if $K \in \finsubsets{I}$ with $J \intersect K = \emptyset$, then $\sum_{i \in K} g_i \in N$. In other words,
    %
    \begin{equation*}
        \forall N \in \nhoods{0} \,
            \exists J \in \finsubsets{I} \,
            \forall K \in \finsubsets{I}\colon
            J \intersect K = \emptyset
            \quad \implies \quad
            \sum_{i \in K} g_i \in N.
    \end{equation*}
\end{lemma}

\begin{proof}
    Suppose the unordered sum is Cauchy. Let $N$ be a neighbourhood of $0$. Since the sum is Cauchy there exists a $J \in \finsubsets{I}$ such that $J \subseteq L,L'$ implies that
    %
    \begin{equation*}
        \sum_{i \in L} g_i - \sum_{i \in L'} g_i
            \in N,
    \end{equation*}
    %
    for $L,L' \in \finsubsets{I}$. Hence if $K \in \finsubsets{I}$ is disjoint from $J$, then
    %
    \begin{equation*}
        \sum_{i \in K} g_i
            = \sum_{i \in J \union K} g_i - \sum_{i \in J} g_i
            \in N,
    \end{equation*}
    %
    as desired.

    Conversely assume that the sum has the property above, let $U$ be an open neighbourhood of $0$, and let $V$ be a symmetric open neighbourhood of $0$ with $V + V \subseteq U$. Let $J \in \finsubsets{I}$ be such that $J \intersect K = \emptyset$ implies that $\sum_{i \in K} g_i \in V$ for $K \in \finsubsets{I}$. If $J \subseteq L,L'$, then
    %
    \begin{equation*}
        \sum_{i \in L} g_i - \sum_{i \in L'} g_i
            = \sum_{i \in L \setminus J} g_i - \sum_{i \in L' \setminus J} g_i
            \in V - V
            \subseteq U.
    \end{equation*}
    %
    Thus the sum is Cauchy as claimed. % TODO: Don't use explicitly open neighbourhoods, there's no reason for it.
\end{proof}


Since the topology on a topological group $G$ is homogeneous, $G$ is automatically $R_0$. That is, if $g,h \in G$ and $g$ has a neighbourhood not containing $h$, then $h$ has a neighbourhood not containing $g$. In other words, topologically distinguishable points are separated. In particular, the specialisation preorder $\leq$ is symmetric, so that $g \leq h$ implies $g \equiv h$.

\begin{lemma}
    Let $G$ be an abelian topological group that is also first countable. If the unordered sum $\sum_{i \in I} g_i$ is Cauchy, then the set $\set{i \in I}{g_i \not\equiv 0}$ is countable.
\end{lemma}

\begin{proof}
    Let $\calB$ be a countable neighbourhood basis at $0$. By taking intersections we may assume that the neighbourhoods in $\calB$ form a decreasing sequence $(N_n)_{n\in\naturals}$. For each $n \in \naturals$, choose $J_n \in \finsubsets{I}$ such that $J_n \intersect K = \emptyset$ implies that $\sum_{i \in K} g_i \in N_n$. Now let $J = \bigunion_{n\in\naturals} J_n$. For any $i \in I \setminus J$ the set $\{i\}$ is disjoint from each $J_n$, so
    %
    \begin{equation*}
        g_i
            = \sum_{j \in \{i\}} g_j \in N_n.
    \end{equation*}
    %
    Since the $N_n$ form a neighbourhood basis at $0$, every neighbourhood of $0$ intersects $g_i$, i.e. $g_i \in \closure{\{0\}}$. But this means that $g_i \leq 0$ in the specialisation preorder, so $g_i \equiv 0$ since $G$ is $R_0$. Hence it follows that
    %
    \begin{equation*}
        \set{i \in I}{g_i \not\equiv 0}
            \subseteq J,
    \end{equation*}
    %
    so the former set is countable since $J$ is, as desired.
\end{proof}


\section{Limits of functions} % [TODO move somewhere else?]

Let $X$ be a topological space, and let $A \subseteq X$. A subset $N$ is a \emph{neighbourhood} of $A$ if there is an open set $U$ such that $A \subseteq U \subseteq N$. The set of neighbourhoods of $A$ is denoted $\nhoods{A}$ and is called the \emph{neighbourhood filter} of $A$ (since it is indeed a filter). If $A = \{x\}$ is a singleton we also write $\nhoods{x}$ for the neighbourhood filter of $x$. A \emph{punctured neighbourhood} of $x \in X$ is a set on the form $N \setminus \{x\}$, where $N \in \nhoods{x}$. The set of punctured neighbourhoods of $x$ is denoted $\pnhoods{x}$.

A point $a \in X$ is called an \emph{adherent point} of $A \subseteq X$ if every neighbourhood of $a$ intersects $A$, i.e. if $a \in \closure{A}$. Furthermore, $a$ is called a \emph{limit point} of $A$ if every \emph{punctured} neighbourhood of $a$ intersects $A$, or equivalently if $a$ is an adherent point of $A \setminus \{a\}$.

\begin{definition}[Limits of functions]
    \label{def:limit-of-function}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $L \subseteq A$ and $a \in X$ is an adherent point of $L$, then the triple $(f,L,a)$ is called a \emph{limit in $X$}. If $b \in Y$ is such that
    %
    \begin{equation*}
        \forall N \in \nhoods{b}\,
            \exists M \in \nhoods{a} \colon
            f(M \intersect L) \subseteq N,
    \end{equation*}
    %
    then we say that $b$ is a \emph{limit value} for the limit $(f,L,a)$. The set of such limit values is denoted $\limitval(f,L,a)$.

    If $\limitval(f,L,a)$ contains a single element $b$, then we also write
    %
    \begin{equation*}
        b
            = \lim_{a, L} f
            = \lim_{\substack{x \to a \\ x \in L}} f(x).
    \end{equation*}

    % be a map between topological spaces. If $a \in X$ is an adherent point of a subset $A \subseteq X$ and $b \in Y$, then we say that $f$ \emph{converges to $b$ in $Y$ at $a$ in $A$}, or that $f(x)$ \emph{converges to $b$ in $Y$ as $x$ converges to $a$ in $A$}, if

    % In this case, $b$ is called a \emph{limit of $f$ at $a$ in $A$} or a \emph{limit of $f(x)$ as $x$ converges to $a$ in $A$}.

    % If there is a unique such limit $b$, then we write
    % %
    % \begin{equation*}
    %     b
    %         = \lim_{a \in A} f
    %         = \lim_{\substack{x \to a \\ a \in A}} f(x).
    % \end{equation*}
\end{definition}

\begin{remark}
    \label{rem:limit-neighbourhoods}
    In \cref{def:limit-of-function} we did not specify whether the neighbourhoods of $a$ were neighbourhoods in $A$ or in $X$. In fact, this does not matter: The map $M \mapsto M \intersect A$ gives a one-to-one correspondence between neighbourhoods of $a$ in $X$ and $A$, and since $L$ is a subset of $A$ we have
    %
    \begin{equation*}
        f((M \intersect A) \intersect L) = f(M \intersect L).
    \end{equation*}
    %
    Hence we may use either neighbourhoods in $X$ or in $A$ when taking limits.
\end{remark}

\begin{proposition}[Uniqueness of limits]
    \label{prop:Hausdorff-limits-unique}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $Y$ is Hausdorff, then any limit $(f,L,a)$ in $X$ has at most one limit point.
\end{proposition}

\begin{proof}
    Let $b \in Y$ be a limit value for $(f,L,a)$, let $b' \neq b$, and let $N,N' \subseteq Y$ be disjoint neighbourhoods of $b$ and $b'$, respectively. Since $b$ is a limit value, there is a neighbourhood $M \in \nhoods{a}$ such that $f(M \intersect L) \subseteq N$. If $M'$ is any neighbourhood of $a$ then $M \intersect M'$ is also a neighbourhood of $a$, so it intersects $L$.\footnote{Notice that we here make crucial use of the requirement that $a$ be an adherent point of $L$.} But since
    %
    \begin{equation*}
        f((M \intersect M') \intersect L)
            \subseteq f(M \intersect L)
            \subseteq N,
    \end{equation*}
    %
    the set $f(M' \intersect L)$ intersects $N$, so it does not lie in $N'$. Hence $b'$ is not a limit value for $(f,L,a)$.
\end{proof}

% Overwrite \ball from topologycommands.sty
\renewcommand{\ball}[3][]{B_{#1}(#2,#3)}
\newcommand{\cball}[3][]{\overline{B}_{#1}(#2,#3)}
\newcommand{\pball}[3][]{B'_{#1}(#2,#3)}

\begin{example}
    \Cref{def:limit-of-function} provides a very general notion of limit, of which the familiar limiting processes are special cases:
    %
    \begin{enumexample}
        \item \label{enum:deleted-limit} Recall the standard definition of a limit of a function between topological spaces: If $a$ is a limit point of $A$, then we usually say that $b \in Y$ is a limit of $f(x)$ as $x \to a$ if
        %
        \begin{equation*}
            \forall N \in \nhoods{b}\,
                \exists M' \in \pnhoods{a} \colon
                f(M') \subseteq N.
        \end{equation*}
        %
        We recover this notion by considering the triple $(f,A \setminus \{a\},a)$: For notice that if $M$ is a neighbourhood of $a$, then
        %
        \begin{equation*}
            M \intersect (A \setminus \{a\})
                = (M \setminus \{a\}) \intersect A,
        \end{equation*}
        %
        and every punctured neighbourhood of $a$ is on the form $M \setminus \{a\}$. Here we recall from \cref{rem:limit-neighbourhoods} that it is immaterial whether we ue neighbourhoods of $a$ in $A$ or in $X$, so the intersection with $A$ makes no difference.

        Notice that it is crucial that $a$ is a limit point of $A$ and not just an adherent point, since $a$ is a limit point of $A$ if and only if $a$ is an adherent point of $A \setminus \{a\}$, which we require in our definition of limits.

        Below we shall take this notion of limit as standard, so if $Y$ is Hausdorff and hence limits are unique by \cref{prop:Hausdorff-limits-unique}, we use the shorthand notation
        %
        \begin{equation*}
            b
                = \lim_a f
                = \lim_{x \to a} f(x).
        \end{equation*}
        %
        If the set $A$ is not clear from context, we may disambiguate by writing \enquote{for $x \in A \setminus \{a\}$},  or something to that effect.

        \item Some authors distinguish between \emph{deleted} and \emph{non-deleted} limits. The notion of limits in \subcref{enum:deleted-limit} is that of deleted limits, since we only reture that $f(M') \subseteq N$ for a \emph{punctured} neighbourhood $M'$ of $a$. By contrast, in a \emph{non-deleted} limit we require that $f(M) \subseteq N$ for an ordinary neighbourhood $M$ of $a$. We may recover the notion of non-deleted limits by considering triples $(f,A,a)$.
        
        Non-deleted limits are relatively rare in the English-language literature (one notable exception is \textcite{taoanalysisI,taoanalysisII}, who allows for both types), but it seems to be the prevailing notion in the French-language literature.
        
        \item Consider next a function $f \colon A \to Y$, where $A \subseteq \reals$ contains an interval $(a,b)$. In this case we may consider the one-sided limits of $f(x)$ as $x$ approaches either $a$ from above or $b$ from below. These may be described in the above formalism by the triples $(f,(a,b),a)$ and $(f,(a,b),b)$, respectively (indeed, we may choose any subinterval of $(a,b)$ with $a$ or $b$ as a limit point, respectively, and the set of limit values is clearly independent of such a choice). If they exist, we denote the corresponding limit values by
        %
        \begin{equation*}
            \lim_{x \downarrow a} f(x)
            \quad \text{and} \quad
            \lim_{x \uparrow b} f(x),
        \end{equation*}
        %
        respectively.
    \end{enumexample}
\end{example}


\begin{lemma}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $L_1 \subseteq L_2 \subseteq A$ and $a \in X$ is an adherent point of both $L_1$ and $L_2$, then
    %
    \begin{equation*}
        \limitval(f,L_2,a)
            \subseteq \limitval(f,L_1,a).
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $b \in \limitval(f,L_2,a)$ and let $N \in \nhoods{b}$. Then there is an $M \in \nhoods{a}$ such that
    %
    \begin{equation*}
        f(M \intersect L_1)
            \subseteq f(M \intersect L_2)
            \subseteq N,
    \end{equation*}
    %
    showing that $b \in \limitval(f,L_1,a)$ as desired.
\end{proof}


\begin{proposition}
    Let $X$ and $Y$ be topological spaces, let $A \subseteq X$, and let $f \colon A \to Y$. If $a \in A$ is a limit point of $A$, then the following are equivalent:
    %
    \begin{enumprop}
        \item $f$ is continuous at $a$.
        \item $f(a) \in \limitval(f, A, a)$.
        \item $f(a) \in \limitval(f, A \setminus \{a\}, a)$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    Assume that $f$ is continuous at $a$, and let $N \in \nhoods{f(a)}$. By continuity there is an\footnote{Recall that we may use either neighbourhoods of $a$ in $A$ or in $X$, cf. \cref{rem:limit-neighbourhoods}.} $M \in \nhoods{a}$ such that $f(M) \subseteq N$, implying that $f(M \intersect A) \subseteq N$. Next, (ii) implies (iii) by [TODO ref lemma].
    
    Finally, if $f(a) \in \limitval(f, A \setminus \{a\}, a)$ then given $N \in \nhoods{f(a)}$ there is a neighbourhood $M \in \nhoods{a}$ in $A$ such that $f(M \setminus \{a\}) = f(M \intersect A \setminus \{a\}) \subseteq N$. Since $N$ contains $f(a)$ we have $f(M) \subseteq N$ so $f$ is continuous at $a$.
\end{proof}


\chapter{Differentiation}

\section{Differentiability}

% \begin{definition}[Differentiability on $\reals$]
%     Let $A \subseteq \reals$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals$. If the limit
%     %
%     \begin{equation*}
%         \lim_{x \to a} \frac{f(x) - f(a)}{x - a}
%     \end{equation*}
%     %
%     exists and equals $L \in \reals$, then we say that $f$ is \emph{differentiable} at $a$. The number $L$ is called the \emph{derivative} of $f$ at $a$ and is denoted $f'(a)$.
% \end{definition}
% %
% % \begin{remark}
% %     \label{rem:differentiability-reformulation}
% %     Recall that our standard notion of limit is that of \emph{deleted limits}, cf. \cref{enum:deleted-limit}. Hence the above is shorthand for the limit
% %     %
% %     \begin{equation*}
% %         \lim_{\substack{x \to a \\ a \in A \setminus \{a\}}} \frac{f(x) - f(a)}{x - a}.
% %     \end{equation*}
% %     %
% %     That is, we do not allow $x$ to equal $a$ (as is of course necessary since we divide by $x-a$).
    
% %     Notice also that we may reformulate the definition of differentiability of $f$ at $a$ by instead requiring that the limit
% %     %
% %     \begin{equation*}
% %         \lim_{\substack{h \to 0 \\ h \in (A-a) \setminus \{0\}}} \frac{f(a+h) - f(a)}{h}
% %     \end{equation*}
% %     %
% %     exist.
% % \end{remark}

% \begin{lemma}[Hadamard's lemma on $\reals$]
%     \label{lem:Hadamard-1D}
%     Let $A \subseteq \reals$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals$. The following are equivalent:
%     %
%     \begin{enumlem}
%         \item $f$ is differentiable at $a$.

%         \item There exists an $L(a) \in \reals$ and a function $\epsilon_a \colon A-a \to \reals$ such that
%         %
%         \begin{equation*}
%             \lim_{h \to 0} \frac{\epsilon_a(h)}{h} = 0,
%             \quad \text{and} \quad
%             f(a+h) - f(a)
%                 = L(a)h + \epsilon_a(h)
%         \end{equation*}
%         %
%         for all $h \in A-a$.

%         \item There exists a function $\phi = \phi_a \colon A \to \reals$, continuous at $a$, such that
%         %
%         \begin{equation*}
%             f(x) - f(a)
%                 = \phi_a(x) (x - a)
%         \end{equation*}
%         %
%         for all $x \in A$.
%     \end{enumlem}
%     %
%     If any of these conditions are satisfied, then
%     %
%     \begin{equation*}
%         f'(a) = \phi_a(a) = L(a),
%         \quad \text{and} \quad
%         \phi_a(x)
%             = f'(a) + \frac{\epsilon_a(x-a)}{x-a}
%     \end{equation*}
%     %
%     for all $x \in A \setminus \{a\}$.
% \end{lemma}
% %
% We will call the function $\phi = \phi_a$ an \emph{Hadamard function} for $f$ at $a$.

% \begin{proof}
% \begin{proofsec}
%     \item[(i) $\implies$ (ii)]
%     Let $L(a) = f'(a)$ and put
%     %
%     \begin{equation*}
%         \epsilon_a(h)
%             = f(a+h) - f(a) - f'(a)h
%     \end{equation*}
%     %
%     for $h \in A - a$. Then we have
%     %
%     \begin{equation*}
%         \frac{\epsilon_a(h)}{h}
%             = \frac{f(a+h) - f(a) - f'(a)h}{h}
%             = \frac{f(a+h) - f(a)}{h} - f'(a)
%             \xrightarrow[h \to 0]{} 0
%     \end{equation*}
%     %
%     as required.

%     \item[(ii) $\implies$ (iii)]
%     Define
%     %
%     \begin{equation*}
%         \phi_a(x) =
%         \begin{cases}
%             L(a) + \frac{1}{x-a} \epsilon_a(x-a),
%                 & x \in A \setminus \{a\}, \\
%             L(a),
%                 & x = a.
%         \end{cases}
%     \end{equation*}
%     %
%     Then $\phi_a$ is continuous at $a$, and furthermore
%     %
%     \begin{equation*}
%         \phi_a(x)(x-a)
%             = L(a)(x-a) + \epsilon_a(x-a)
%             = f(x) - f(a)
%     \end{equation*}
%     %
%     for $x \in A \setminus \{a\}$ as required.

%     \item[(iii) $\implies$ (i)]
%     Notice that
%     %
%     \begin{equation*}
%         \frac{f(x) - f(a)}{x-a}
%             = \phi_a(x-a)
%             \xrightarrow[x \to a]{} \phi_a(a),
%     \end{equation*}
%     %
%     so $f$ is differentiable at $a$.
% \end{proofsec}
% \end{proof}




\begin{definition}[Differentiability]
    \label{def:differentiability}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$. If there exists a linear map $L \in \calL(\reals^d, \reals^m)$ such that
    %
    \begin{equation}
        \label{eq:differentiability-definition}
        \lim_{x \to a} \frac{\norm{f(x) - f(a) - L(x-a)}}{\norm{x - a}}
            = 0,
    \end{equation}
    %
    then $f$ is said to be \emph{differentiable} at $a$. The map $L$ is called a \emph{derivative of $f$ at $a$}. If this is unique, then we denote it by $f'(a)$. If $E \subseteq A$ and $f$ is differentiable at $a$ for all $a \in E$, then we say that $f$ is \emph{differentiable on $E$}. If $f$ is differentiable on $A$, then we simply say that $f$ is \emph{differentiable}.
    
    If $f$ is differentiable and its derivative is unique everywhere, then the map $f' \colon A \to \calL(\reals^d,\reals^m)$ is called the \emph{derivative of $f$}.
\end{definition}
%
Note that for $f$ to be differentiable on $E$, every point in $E$ must be a limit point of $A$. In particular, for $f$ to be differentiable every point in $A$ must be a limit point of $A$. This is for instance the case when $A$ is open or when $A$ is an interval.


\begin{remarkbreak}[Differentiability on $\reals$]
    In the case where $d = 1$, the quotient in \cref{eq:differentiability-definition} goes to zero if and only if
    %
    \begin{equation*}
        \frac{f(x) - f(a)}{x - a} - L
            = \frac{f(x) - f(a) - L(x-a)}{x - a}
    \end{equation*}
    %
    goes to zero, i.e. the difference quotient converges to $L$. Furthermore, if also $m = 1$ then $L$ is multiplication by a scalar, so in this case \cref{def:differentiability} is equivalent to the usual definition of the derivative.
\end{remarkbreak}


\begin{lemma}
    \label{lem:differentiability-coordinate-functions}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f = (f_1, \ldots, f_m) \colon A \to \reals^m$. Then $f$ is differentiable at $a$ if and only if each $f_i$ is differentiable at $a$, and $f'(a) = (f_1'(a), \ldots, f_m'(a))$.

    [TODO inner points??]
\end{lemma}

\begin{proof}
    If $L = (L_1, \ldots, L_m) \in \calL(\reals^d,\reals^m)$, then notice that
    %
    \begin{equation*}
        f(x) - f(a) - L(x-a) =
        \begin{pmatrix}
            f_1(x) - f_1(a) - L_1(x-a) \\
            \vdots \\
            f_m(x) - f_m(a) - L_m(x-a)
        \end{pmatrix}.
    \end{equation*}
    %
    The claim then follows since the quotient in \cref{eq:differentiability-definition} converges to zero if and only if each quotient
    %
    \begin{equation*}
        \frac{\abs{f_i(x) - f_i(a) - L_i(x-a)}}{\norm{x - a}}
    \end{equation*}
    %
    converges to zero.
\end{proof}

\newcommand{\trans}{^{\top}}

\begin{lemma}[Hadamard's lemma]
    \label{lem:Hadamard-multiD}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$. The following are equivalent:
    %
    \begin{enumlem}
        \item $f$ is differentiable at $a$.

        \item There exists a linear map $L(a) \in \calL(\reals^d,\reals^m)$ and a function $\epsilon_a \colon A-a \to \reals^m$ such that
        %
        \begin{equation}
            \label{eq:epsilon_a-properties}
            \lim_{h \to 0} \frac{\norm{\epsilon_a(h)}}{\norm{h}} = 0,
            \quad \text{and} \quad
            f(a+h) - f(a)
                = L(a)h + \epsilon_a(h)
        \end{equation}
        %
        for all $h \in A-a$.

        \item There exists a function $\phi = \phi_a \colon A \to \calL(\reals^d,\reals^m)$, continuous at $a$, such that
        %
        \begin{equation*}
            f(x) - f(a)
                = \phi_a(x) (x - a)
        \end{equation*}
        %
        for all $x \in A$.
    \end{enumlem}
    %
    If any of these conditions are satisfied, then $L(a)$ and $\phi_a(a)$ are derivatives of $f$ at $a$. Conversely, to any derivative $L(a)$ of $f$ at $a$ there exists a function $\epsilon_a \colon A-a \to \reals^m$ with the properties in \cref{eq:epsilon_a-properties}.
    %
    % \begin{equation*}
    %     f'(a) = \phi_a(a) = L(a),
    %     \quad \text{and} \quad
    %     \phi_a(x)
    %         = f'(a) + \frac{1}{\norm{x-a}^2} \epsilon_a(x-a)(x-a)\trans,
    % \end{equation*}
    % %
    % for all $x \in A \setminus \{a\}$.
\end{lemma}
%
We will call the function $\phi = \phi_a$ an \emph{Hadamard function} for $f$ at $a$.

\begin{proof}
    \begin{proofsec}
        \item[(i) $\implies$ (ii)]
        Let $L(a)$ be a derivative of $f$ at $a$ and put
        %
        \begin{equation*}
            \epsilon_a(h)
                = f(a+h) - f(a) - L(a)h
        \end{equation*}
        %
        for $h \in A - a$. Then we have
        %
        \begin{equation*}
            \frac{\norm{\epsilon_a(h)}}{\norm{h}}
                = \frac{\norm{f(a+h) - f(a) - L(a)h}}{\norm{h}}
                \xrightarrow[h \to 0]{} 0
        \end{equation*}
        %
        as required, since $L(a)$ is a derivative.
    
        \item[(ii) $\implies$ (iii)]
        Define
        %
        \begin{equation*}
            \phi_a(x) =
            \begin{cases}
                L(a) + \frac{1}{\norm{x-a}^2} \epsilon_a(x-a)(x-a)\trans,
                    & x \in A \setminus \{a\}, \\
                L(a),
                    & x = a.
            \end{cases}
        \end{equation*}
        %
        We first of all have
        %
        \begin{align*}
            \phi_a(x)(x-a)
                &= L(a)(x-a) + \epsilon_a(x-a) \frac{(x-a)\trans (x-a)}{\norm{x-a}^2} \\
                &= L(a)(x-a) + \epsilon_a(x-a) \\
                &= f(x) - f(a)
        \end{align*}
        %
        as required. Next notice that, since $h$ and $\epsilon_a(h)$ are (column) vectors, we have\footnote{More generally, let $v = (v_1, \ldots, v_d) \in \reals^d$ and $w = (w_1, \ldots, w_m) \in \reals^m$. Then $(wv\trans)_{ij} = w_i v_j$, so
        %
        \begin{equation*}
            \norm{wv\trans}^2
                = \sum_{i,j} \abs{w_i v_j}^2
                = \sum_{i=1}^m \abs{w_i}^2 \sum_{j=1}^d \abs{v_i}^2
                = \norm{w}^2 \norm{v}^2.
        \end{equation*}}
        %
        \begin{equation*}
            \frac{\norm{\epsilon_a(h) h\trans}}{\norm{h}^2}
                = \frac{\norm{\epsilon_a(h)} \, \norm{h}}{\norm{h}^2}
                = \frac{\norm{\epsilon_a(h)}}{\norm{h}}
                \xrightarrow[h \to 0]{} 0,
        \end{equation*}
        %
        so $\phi_a$ is continuous at $a$.
    
        \item[(iii) $\implies$ (i)]
        Notice that
        %
        \begin{align*}
            \frac{\norm{f(x) - f(a) - \phi_a(a)(x-a)}}{\norm{x-a}}
                &= \frac{\norm{\phi_a(x)(x-a) - \phi_a(a)(x-a)}}{\norm{x-a}} \\
                &\leq \frac{\norm{\phi_a(x) - \phi_a(a)} \, \norm{x-a}}{\norm{x-a}} \\
                &= \norm{\phi_a(x) - \phi_a(a)}
                \xrightarrow[x \to a]{} 0,
        \end{align*}
        %
        by continuity of $\phi_a$ at $a$, and continuity of the operator norm, so $f$ is differentiable at $a$, and $\phi_a(a)$ is a derivative of $f$ at $a$.
    \end{proofsec}
\end{proof}


\begin{proposition}
    \label{prop:differentiable-implies-continuous}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$. If $f$ is differentiable at $a$, then $f$ is continuous at $a$.
\end{proposition}

\begin{proof}
    Let $\phi_a$ be an Hadamard function for $f$ at $a$, such that
    %
    \begin{equation}
        f(x)
            = f(a) + \phi_a(x)(x-a)
    \end{equation}
    %
    for $x \in A$. Since $\phi_a$ is continuous at $a$, so is $f$.
\end{proof}


\begin{theorem}[The chain rule]
    Let $A \subseteq \reals^d$ and $B \subseteq \reals^m$, and let $f \colon A \to \reals^m$ and $g \colon B \to \reals^p$ with $f(A) \subseteq B$. Assume that $a \in A$ is a limit point of $A$, that $f(a) \in B$ is a limit point of $B$ with derivative $L(a)$, that $f$ is differentiable at $a$, and that $g$ is differentiable at $f(a)$ with derivative $M(f(a))$. Then $g \circ f$ is differentiable at $a$, and $M(f(a)) \circ L(a)$ is a derivative of $g \circ f$ at $a$.

    In particular, if the above derivatives of $f$, $g$ and $g \circ f$ are uniquely determined, then
    %
    \begin{equation*}
        (g \circ f)'(a)
            = g'(f(a)) f'(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $\phi$ be an Hadamard function for $f$ at $a$, and let $\psi$ be an Hadamard function for $g$ at $f(a)$. Hence
    %
    \begin{equation*}
        f(x) - f(a)
            = \phi(x)(x - a)
        \quad \text{and} \quad
        g(y) - g(f(a))
            = \psi(y)(y - f(a))
    \end{equation*}
    %
    for all $x \in A$ and $y \in B$. Letting $y = f(x)$ we thus have
    %
    \begin{equation*}
        g(f(x)) - g(f(a))
            = \psi(f(x))(f(x) - f(a))
            = \psi(f(x))\phi(x)(x - a).
    \end{equation*}
    %
    Notice that the map $x \mapsto \psi(f(x))\phi(x)$ is continuous at $a$ by \cref{prop:differentiable-implies-continuous}, so it is an Hadamard function for $g \circ f$ at $a$. Thus \cref{lem:Hadamard-multiD} implies that $g \circ f$ is differentiable at $a$ with derivative
    %
    \begin{equation*}
        \psi(f(a)) \phi(a)
            = M(f(a)) \circ L(a),
    \end{equation*}
    %
    as claimed.
\end{proof}

\fleuronbreak

We finally turn to uniqueness of derivatives. First a definition:

\begin{definition}[Directional limit points]
    \label{def:directional-limit-points}
    Let $V$ be a topological vector space, and let $A \subseteq V$. For $a,h \in V$ with $h \neq 0$ we have the following:
    \begin{enumdef}
        \item We say that $a$ is a \emph{limit point of $A$ in the direction of $h$} if $a$ is a limit point of the set\footnotemark{} $A \intersect (a, a + \delta h)$ for some $\delta > 0$.

        \item If $a$ is a limit point of $A$ in the direction of both $h$ and $-h$, then we say that $a$ is a \emph{two-sided limit point of $A$ in the direction of $h$}.

        \item If $a$ is a limit point of $A$ in the direction of $h$ for all $h \in V \setminus \{0\}$, then we say that $a$ is a limit point of $A$ \emph{in every direction}.
    \end{enumdef}
\end{definition}
\footnotetext{Recall that if $x,y \in V$, then we define the \enquote{intervals}
%
\begin{equation*}
    [x,y] = \set{(t-1)x + ty}{t \in [0,1]}
    \quad \text{and} \quad
    (x,y) = \set{(t-1)x + ty}{t \in (0,1)}.
\end{equation*}}
%
Notice that the value of $\delta$ is irrelevant. Notice furthermore that if $h$ and $h'$ are parallel and point in the same direction (i.e. if $h = sh'$ for some $s > 0$), then $a$ is a limit point of $A$ in the direction of $h$ if and only if $a$ is a limit point of $A$ in the direction of $h'$. Finally notice that if $a$ is a limit point of $A$ in the direction of some $h$, then $A$ is a limit point of $A$ in the usual sense (this is clear since $A \intersect (a, a + \delta h)$ is a subset of $A$). [TODO is there any issue in complex vector spaces?]

\begin{lemma}[Uniqueness of derivatives]
    \label{lem:uniqueness-of-derivatives}
    Let $A \subseteq \reals^d$, let $a \in A$ be a limit point of $A$, and let $f \colon A \to \reals^m$ be differentiable at $a$. If $\calB$ is a basis for $\reals^d$ and $a$ is a limit point of $A$ in the direction of each element in $\calB$, then the derivative of $f$ at $a$ is uniquely determined. This is the case in particular if either
    %
    \begin{enumlem}
        \item $d = 1$, or
        \item $a$ is an inner point of $A$.
    \end{enumlem}
\end{lemma}

\begin{proof}
    Let $L(a)$ be a derivative of $f$ at $a$, and let $\epsilon_a$ be as in \cref{lem:Hadamard-multiD}. For $b \in \calB$ consider the set $I_b = \set{t \geq 0}{a+tb \in A}$, and notice that $0$ is a limit point of $I$ since $a$ is a limit point of $A$ in the direction of $b$. Next notice that, by \cref{eq:epsilon_a-properties}, we have
    %
    \begin{equation*}
        L(a)b
            = \frac{1}{t} \bigl( f(a + tb) - f(a) \bigr) - \frac{1}{t} \epsilon_a(tb)
    \end{equation*}
    %
    for $t \in I \setminus \{0\}$. Also by \cref{eq:epsilon_a-properties} the right-most term above goes to zero as $t \downarrow 0$ on $I \setminus \{0\}$, so this implies that
    %
    \begin{equation*}
        L(a)b
            = \lim_{t \downarrow 0} \frac{1}{t} \bigl( f(a + tb) - f(a) \bigr).
    \end{equation*}
    %
    In particular, $L(a)b$ is uniquely determined by $f$ and $a$. But since $\calB$ is a basis for $\reals^d$, this implies that $L(a)$ is uniquely determined by $f$ and $a$.
\end{proof}


\section{Extrema and mean value theorems}

\begin{proposition}
    \label{prop:local-extrema-stationary}
    Let $f \colon (a,b) \to \reals$, and let $c \in (a,b)$. Assume that $f$ is differentiable at $c$ and attains a local extremum at $c$. Then $f'(c) = 0$.
\end{proposition}

\begin{proof}
    Assume for definiteness that $f$ has a local maximum at $c$, and choose $\delta > 0$ such that $f(x) \leq f(c)$ for $x \in (c - \delta, c + \delta)$. For $x \in (c - \delta, c)$ we have
    %
    \begin{equation*}
        \frac{f(x) - f(c)}{x - c} \geq 0,
    \end{equation*}
    %
    and letting $x \uparrow c$ we find that $f'(c) \geq 0$. By considering $x \in (c, c + \delta)$ we similarly find that $f'(c) \leq 0$ as desired.
\end{proof}


\begin{lemma}[Rolle's theorem]
    \label{lem:Rolle}
    Let $f \colon [a,b] \to \reals$ be a continuous function that is differentiable on $(a,b)$. If $f(a) = f(b)$, then there is a $c \in (a,b)$ such that $f'(c) = 0$.
\end{lemma}

\begin{proof}
    If $f$ is constant, then this is obvious. If $f$ is not constant, then since it is continuous it has a local extremum at some $c \in (a,b)$. By \cref{prop:local-extrema-stationary} we thus have $f'(c) = 0$ as desired.
\end{proof}


\begin{theorem}[The generalised mean value theorem]
    \label{thm:generalised-MVT}
    Let $f,g \colon [a,b] \to \reals$ be continuous functions that are differentiable on $(a,b)$. Then there exists a point $c \in (a,b)$ such that
    %
    \begin{equation*}
        \bigl( f(b) - f(a) \bigr) g'(c)
            = \bigl( g(b) - g(a) \bigr) f'(c).
    \end{equation*}
\end{theorem}

\begin{proof}
    Define $h \colon [a,b] \to \reals$ by $h(x) = (f(b) - f(a)) g(x) - (g(b) - g(a)) f(x)$, and notice that $h$ is continuous on $[a,b]$, differentiable on $(a,b)$, and that
    %
    \begin{equation*}
        h(a)
            = f(b)g(a) - g(b)f(a)
            = h(b).
    \end{equation*}
    %
    \Cref{lem:Rolle} thus implies that $h'(c) = 0$ for some $c \in (a,b)$. This proves the claim.
\end{proof}


\begin{corollary}[The mean value theorem]
    Let $f \colon [a,b] \to \reals$ be a continuous function that is differentiable on $(a,b)$. Then there is a $c \in (a,b)$ such that
    %
    \begin{equation*}
        f(b) - f(a)
            = f'(c) (b-a).
    \end{equation*}
\end{corollary}

\begin{proof}
    Let $g(x) = x$ in \cref{thm:generalised-MVT}.
\end{proof}

[TODO monotonicity]


\section{Directional and partial derivatives}

\begin{definition}[Directional and partial derivatives]
    Let $A \subseteq \reals^d$, and let $f \colon A \to \reals^m$. If $a \in A$ is a limit point [TODO do I want to require two-sided limit points, and below limits from both directions?] of $A$ in the direction of $v \in \reals^d \setminus \{0\}$, then we say that $f$ has a \emph{directional derivative} at $a$ in the direction of $v$ if the limit
    %
    \begin{equation*}
        \lim_{t \downarrow 0} \frac{f(a + tv) - f(a)}{t}
    \end{equation*}
    %
    exists. The value of this limit is denoted $D_v f(a)$ and is called the \emph{directional derivative} of $f$ at $a$ in the direction of $v$.

    If $v = e_j$, then we write $D_{e_j} f(a) = D_j f(a)$ and call $D_j f(a)$ the \emph{$j$-th partial derivative} of $f$ at $a$. If $D_j f(a)$ exists for all $j$, then $f$ is said to be \emph{partially differentiable} at $a$. If $E \subseteq A$ and $f$ is partially differentiable at $a$ for all $a \in E$, then we say that $f$ is \emph{partially differentiable on $E$}. If $f$ is partially differentiable on $A$, then we simply say that $f$ is \emph{partially differentiable}.

    If $f$ is partially differentiable on a set $E \subseteq A$, then the functions $D_i f \colon E \to \reals^m$ are called the \emph{partial derivatives} of $f$ (on $E$).
\end{definition}

\begin{proposition}
    Let $A \subseteq \reals^d$, and let $f \colon A \to \reals^m$. If $a \in A$ is a limit point of $A$ in every direction, and if $f$ is differentiable at $a$, then the following hold:
    %
    \begin{enumprop}
        \item The directional derivative $D_v f(a)$ exists for all $v \in \reals^d$, and
        %
        \begin{equation*}
            D_v f(a)
                = f'(a) v.
        \end{equation*}

        \item \label{enumprop:derivative-linear-combination-of-partials} $f$ is partially differentiable at $a$, and
        %
        \begin{equation*}
            f'(a)v
                = \sum_{j=1}^d v_j D_j f(a)
        \end{equation*}
        %
        for all $v = (v_1, \ldots v_d) \in \reals^d$. In particular, writing $f = (f_1, \ldots, f_m)$ the standard matrix representation $J_f(a)$ of $f'(a)$ is given by $J_f(a) = (D_j f_i(a))_{ij}$.
    \end{enumprop}
\end{proposition}
%
Notice that the derivative of $f$ at $a$ is uniquely determined by \cref{lem:uniqueness-of-derivatives}, so the notation $f'(a)$ for the derivative is justified.

\begin{proof}
\begin{proofsec}
    \item[Proof of (i)]
    Let $I_v = \set{t \geq 0}{a+tv \in A}$ and define the function $g \colon I_v \to \reals$ by $g(t) = f(a + tv)$. Then since the function $t \mapsto a+tv$ is differentiable at $0$ with derivative $v$, the chain rule implies that
    %
    \begin{equation*}
        \lim_{t \downarrow 0} \frac{f(a + tv) - f(a)}{t}
            = g'(0)
            = f'(a)v.
    \end{equation*}

    \item[Proof of (ii)]
    Partial differentiability follows from (i). Writing $v = \sum_{j=1}^d v_j e_j$, it follows from (i) that
    %
    \begin{equation*}
        f'(a)v
            = \sum_{j=1}^d v_j f'(a)e_j
            = \sum_{j=1}^d v_j D_{e_j}(a)
            = \sum_{j=1}^d v_j D_j(a).
    \end{equation*}
\end{proofsec}
\end{proof}


\begin{theorem}[Criterion for differentiability]
    \label{thm:continuous-partials-implies-differentiable}
    Let $A \subseteq \reals^d$, let $a$ be an inner point of $A$, and $f \colon A \to \reals^m$. If $f$ is partially differentiable in a neighbourhood of $a$ and each partial derivative is continuous at $a$, then $f$ is differentiable at $a$.
\end{theorem}

\begin{proof}
    We may assume that $m = 1$ by \cref{lem:differentiability-coordinate-functions}. Let $U$ be an open neighbourhood of $a$ on which $f$ is partially differentiable. Let $h = (h_1, \ldots, h_d) \in \reals^d$, and for $k = 1, \ldots, d$ let $h^{(k)} = \sum_{j=1}^k h_j e_j$. Notice that $h^{(d)} = h$. Choose $h$ small enough such that $a + h^{(k)} \in U$ for all $k$. Next let $I_k \subseteq \reals$ be an open interval containing $0$ and $h_k$ such that the map
    %
    \begin{align*}
        g_k \colon I_k &\to \reals, \\
        t &\mapsto f(a + h^{(k-1)} + t e_k),
    \end{align*}
    %
    is differentiable. Applying the mean value theorem on $g_k$ yields an $s_k$ in the interval between $0$ and $h_k$ such that
    %
    \begin{equation*}
        g_k(h_k) - g_k(0)
            = g_k'(s_k) h_k
            = D_k f(a + h^{(k-1)} + s_k e_k) h_k.
    \end{equation*}
    %
    Next define a map $\phi_a \colon U \to \calL(\reals^d,\reals)$ by
    %
    \begin{equation*}
        \phi_a(a+h)x
            = \sum_{k=1}^d D_k f(a + h^{(k-1)} + s_k e_k) x_k
    \end{equation*}
    %
    for $h \in U - a$ and $x = (x_1, \ldots, x_d) \in \reals^d$. This is clearly well-defined, and since the partial derivatives of $f$ are continuous at $a$, so is $\phi_a$ (notice that $s_k$ also goes to zero when $h$ goes to zero).
    
    Finally notice that
    %
    \begin{align*}
        f(a+h) - f(a)
            &= \sum_{k=1}^d \bigl( f(a + h^{(k)}) - f(a + h^{(k-1)}) \bigr) \\
            &= \sum_{k=1}^d \bigl( g_k(h_k) - g_k(0)) \bigr) \\
            &= \sum_{k=1}^d D_k f(a + h^{(k-1)} + s_k e_k) h_k \\
            &= \phi_a(a+h) h.
    \end{align*}
    %
    But then $\phi_a$ is an Hadamard function for $f$ at $a$, so $f$ is differentiable at $a$ by \cref{lem:Hadamard-multiD}.
\end{proof}


\section{Continuous differentiability}

Since $\calL(\reals^d,\reals^m)$ is a finite-dimensional vector space, it has a unique vector space topology [TODO ref to my notes on topological vector spaces]. Hence it makes sense to talk about the continuity of maps into or out of $\calL(\reals^d,\reals^m)$. In particular, if the derivative $f'$ of $f$ is continuous, then we say that $f$ is \emph{continuously differentiable}. The subset of $C(A,\reals^m)$ containing the continuously differentiable functions is denoted $C^1(A,\reals^m)$.

\begin{corollary}
    Let $U \subseteq \reals^d$ be open and let $f \colon U \to \reals^m$. Then $f$ is continuously differentiable if and only if $f$ is partially differentiable and all of its partial derivatives are continuous.
\end{corollary}

\begin{proof}
    If $f$ is partially differentiable, then $D_j f$ equals the function $a \mapsto f'(a) e_j$ by \cref{enumprop:derivative-linear-combination-of-partials}. If $f$ is differentiable then it is partially differentiable, and if $f'$ is continuous then so are the $D_j f$.

    Conversely, if $f$ is partially differentiable with continuous partial derivatives, then \cref{thm:continuous-partials-implies-differentiable} implies that $f$ is differentiable. Continuity of $f'$ now follows since its coordinate functions are continuous.
\end{proof}


\section{Higher-order derivatives}

\begin{theorem}[Clairaut's Theorem]
    Let $A \subseteq \reals^d$ be open, let $a$ be an inner point of $A$, and let $f \colon A \to \reals^m$. For $i,j \in \{1, \ldots, d\}$, if $D_j D_i f$ and $D_i D_j f$ exist in a neighbourhood of $a$ and are continuous at $a$, then
    %
    \begin{equation*}
        D_j D_i f(a)
            = D_i D_j f(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    By permuting indices, we may assume that $i = 1$ and $j = 2$, and indeed that $d = 2$. Furthermore, by \cref{lem:differentiability-coordinate-functions} it suffices to prove the theorem for $m = 1$.

    Let $U$ be an open neighbourhood of $a$ on which $D_j D_i f$ and $D_i D_j f$ exist, and let $x \in U$. We may assume that $U$ is on the form $U_1 \prod U_2$ for open sets $U_1, U_2 \subseteq \reals$. Writing $a = (a_1,a_2)$ and $x = (x_1,x_2)$, define
    %
    \begin{equation*}
        r(x)
            = \frac{f(x) - f(a_1,x_2) - f(x_1,a_2) + f(a)}{(x_1 - a_1)(x_2 - a_2)}
    \end{equation*}
    %
    for $x \neq a$. Let $I \subseteq U_1$ be an open interval containing $a_1$ and $x_1$, and define $g \colon I \to \reals$ by $g(t) = f(t,x_2) - f(t,a_2)$. Then
    %
    \begin{equation*}
        r(x)
            = \frac{g(x_1) - g(a_1)}{(x_1 - a_1)(x_2 - a_2)}.
    \end{equation*}
    %
    Since $g$ is differentiable on $I$, the mean value theorem yields an $s_1 \in \reals$ between $a_1$ and $x_1$ such that
    %
    \begin{equation*}
        r_(x)
            = \frac{g'(s_1)}{x_2 - a_2}
            = \frac{D_1 f(s_1,x_2) - D_1 f(s_1,a_2)}{x_2 - a_2}.
    \end{equation*}
    %
    Similarly, let $J \subseteq U_2$ be an open interval containing $a_2$ and $x_2$, and define $h \colon J \to \reals$ by $h(t) = D_1 f(s_1,t)$. Again the mean value theorem yields an $s_2 \in \reals$ between $x_2$ and $a_2$ such that
    %
    \begin{equation*}
        r(x)
            = h'(s_2)
            = D_2 D_1 f(s_1, s_2).
    \end{equation*}
    %
    Let $s = (s_1,s_2)$. Since $\norm{s - a} \leq \norm{x - a}$, when $x \to a$ we also have $s \to a$. Continuity of $D_2 D_1 f$ at $a$ thus implies that
    %
    \begin{equation*}
        \lim_{x \to a} r(x)
            = \lim_{s \to a} D_2 D_1 f(s)
            = D_2 D_1 f(a).
    \end{equation*}
    %
    Finally notice that the above argument is symmetric in the indices $1$ and $2$, so we similarly find that $\lim_{x \to a} r(x) = D_1 D_2 f(a)$.
\end{proof}


\chapter{Integration}

\section{Functions of bounded variation}

\newcommand{\boundedvar}[1]{\mathit{BV}[#1]}
\newcommand{\integrable}[2][]{\calR_{#1}[#2]}

A \emph{partition} of an interval $[a,b]$ is a collection $P = \{x_0, \ldots, x_n \}$ of real numbers such that
%
\begin{equation*}
    a = x_0 < \cdots < x_n = b.
\end{equation*}
%
In turn, a \emph{tagged partition} of $[a,b]$ is a pair $(P,T)$ where $P$ is a partition of $[a,b]$ and $T = \{t_1, \ldots, t_n\}$ is a multiset of numbers such that $t_i \in [x_{i-1}, x_i]$ for all $i = 1, \ldots, n$. Let $\calP'[a,b]$ denote the set of tagged partitions of $[a,b]$. We define a direction on $\calP'[a,b]$ by $(P,T) \preceq (P',T')$ if $P \subseteq P'$. Notice that $T$ and $T'$ do not appear in this definition. This also induces a direction on the set $\calP[a,b]$ of all (non-tagged) partitions of $[a,b]$.

Given a partition $P = \{x_0, \ldots, x_n \}$ of $[a,b]$ and a function $f \colon [a,b] \to \reals$ we write $\Delta f_i = f(x_i) - f(x_{i-1})$ for $i = 1, \ldots, n$. We furthermore write $\Delta x_i = x_i - x_{i-1}$. Furthermore, define
%
\begin{equation*}
    \norm{P}
        = \max_{1 \leq i \leq n} \Delta x_i
    \quad \text{and} \quad
    \Sigma_f(P)
        = \sum_{i=1}^n \abs{\Delta f_i}.
\end{equation*}
%
The number $\norm{P}$ is called the \emph{norm} of $P$. Notice that the map $P \mapsto \norm{P}$ is decreasing, while the map $P \mapsto \Sigma_f(P)$ is increasing.

\begin{definition}[Total variation]
    Consider a function $f \colon [a,b] \to \reals$. The \emph{total variation} of $f$ on $[a,b]$ is the number
    %
    \begin{equation*}
        V_f(a,b)
            = \sup_{P \in \calP[a,b]} \Sigma_f(P).
    \end{equation*}
    %
    If $V_f(a,b) < \infty$, then we say that $f$ is of \emph{bounded variation} on $[a,b]$. The set of all functions that are of bounded variation on $[a,b]$ is denoted $\boundedvar{a,b}$.
\end{definition}
%
If $f$ is of bounded variation on $[a,b]$, then it is clear that $f$ is also of bounded variation on any subinterval of $[a,b]$. If $c \in (a,b)$ it is also easy to show that
%
\begin{equation}
    \label{eq:total-variation-additive}
    V_f(a,b)
        = V_f(a,c) + V_f(c,b).
\end{equation}
%
If $g \colon [a,b] \to \reals$ is another function of bounded variation on $[a,b]$ and $c \in \reals$, then it is clear from the definition that $f + g$ and $cf$ are also of bounded variation. It is also simple to show that the product $fg$ is of bounded variation. Hence $\boundedvar{a,b}$ is an $\reals$-algebra.

Also note that monotonic functions are of bounded variation on any compact interval.

\begin{lemma}
    \label{lem:total-variation-increasing}
    Let $f \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_f(a,x)$ for $x \in (a, b]$ and $V(a) = 0$. Then the functions $V$ and $V - f$ are increasing on $[a,b]$.
\end{lemma}

\begin{proof}
    The function $V$ is clearly increasing, so consider the function $D = V-f$. Let $x,y \in [a,b]$ with $x < y$, and notice that $f(y) - f(x) \leq V_f(x,y)$. Recalling \cref{eq:total-variation-additive} it follows that
    %
    \begin{equation*}
        D(y) - D(x)
            = V(y) - V(x) - (f(y) - f(x))
            = V_f(y,x) - (f(y) - f(x))
            \geq 0.
    \end{equation*}
\end{proof}


\begin{proposition}
    \label{prop:BV-difference-of-increasing-functions}
    A function $f \colon [a,b] \to \reals$ is of bounded variation if and only if it is the difference of two (strictly) increasing functions.
\end{proposition}

\begin{proof}
    By \cref{lem:total-variation-increasing} we can write $f$ as the difference of two increasing functions as $f = V - (V - f)$. Adding a strictly increasing function to both $V$ and $V - f$ yields the claim.
\end{proof}


\begin{proposition}
    \label{prop:total-variation-continuity}
    Let $f \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_f(a,x)$ for $x \in (a, b]$ and $V(a) = 0$. If $c \in [a,b]$, then $f$ is continuous at $c$ if and only if $V$ is continuous at $c$.
\end{proposition}

\begin{proof}
    We prove the claim in the case where $c$ is an inner point of $[a,b]$. First assume that $V$ is continuous at $c$. Since $V$ is monotonic by \cref{lem:total-variation-increasing}, the left- and right-hand limits $V(c-)$ and $V(c+)$ exist. For $x \in (c,b]$ we have
    %
    \begin{equation*}
        \abs{f(x) - f(c)}
            \leq V(x) - V(c)
            \xrightarrow[x \downarrow c]{}
            V(c+) - V(c)
            = 0,
    \end{equation*}
    %
    so $f$ is right-continuous at $c$. Similarly for left-continuity.

    We prove the converse, so assume that $f$ is continuous at $c$, and let $\epsilon > 0$. There exists a $\delta > 0$ such that $0 < \abs{x - c} < \delta$ implies $\abs{f(x) - f(c)} < \epsilon$. For any partition $P$ of $[c,b]$ we have
    %
    \begin{equation*}
        0
            \leq V(x_1) - V(c)
            = V_f(c,b) - V_f(x_1,b).
    \end{equation*}
    %
    To bound the right-hand side, choose $P = \{x_0, \ldots, x_n\}$ such that
    %
    \begin{equation*}
        V_f(c,b) - \epsilon
            < \sum_{i=1}^n \abs{\Delta f_i}.
    \end{equation*}
    %
    This inequality is conserved when adding points to $P$, so we may assume that $x_1 - x_0 < \delta$, implying that $\abs{\Delta f_1} < \epsilon$. The inequality above then becomes
    %
    \begin{equation*}
        V_f(c,b) - \epsilon
            < \epsilon + \sum_{i=2}^n \abs{\Delta f_i}
            \leq \epsilon + V_f(x_1,b),
    \end{equation*}
    %
    so that $V_f(c,b) - V_f(x_1,b) < 2\epsilon$. Hence $V(x_1) - V(c) \leq 2\epsilon$, so $V(c+) = V(c)$ since $\epsilon$ was arbitrary and $V$ is increasing. A similar argument yields $V(c-) = V(c)$, so $V$ is continuous at $c$.
\end{proof}


\section{Integration}

Next consider two functions $f, \alpha \colon [a,b] \to \reals$. For each tagged partition $(P,T)$ of $[a,b]$ we define the \emph{Riemann--Stieltjes sum}
%
\begin{equation*}
    S_{f,\alpha}(P,T)
        = \sum_{i=1}^n f(t_i) \Delta\alpha_i.
\end{equation*}
%
This induces a net $S_{f,\alpha} \colon \calP'[a,b] \to \reals$.


\begin{definition}[Riemann--Stieltjes integral]
    Let $f,\alpha \colon [a,b] \to \reals$ be bounded functions. We say that $f$ is \emph{Riemann-integrable} with respect to $\alpha$ (or simply \emph{$\alpha$-integrable}) on $[a,b]$ if the net $S_{f,\alpha}$ has a limit $A \in \reals$. In this case $A$ is called the \emph{Riemann--Stieltjes integral} of $f$ with respect to $\alpha$ on $[a,b]$ and is denoted
    %
    \begin{equation*}
        \int_a^b f \dif \alpha
        \quad \text{or} \quad
        \int_a^b f(x) \dif \alpha(x).
    \end{equation*}
    %
    We denote the set of $\alpha$-integrable functions on $[a,b]$ by $\integrable[\alpha]{a,b}$.
\end{definition}
%
We call $f$ the \emph{integrand} and $\alpha$ the \emph{integrator}. In the case where $\alpha(x) = x$, we use the notations
%
\begin{equation*}
    S_f,
    \quad
    \int_a^b f
    \quad \text{and} \quad
    \int_a^b f(x) \dif x.
\end{equation*}
%
The sums $S_f$ are then simply called \emph{Riemann sums} and the integral the \emph{Riemann integral} of $f$ on $[a,b]$. With this choice of $\alpha$, an $\alpha$-integrable function is called \emph{Riemann integrable} on $[a,b]$, and the set of such functions is denoted $\integrable{a,b}$.

\begin{remark}
    In the ordinary Riemann integral it is not necessary to assume that $f$ is bounded, since integrability in this case implies boundedness. We claim that this assumption can be lifted whenever the integrator $\alpha$ is injective. In fact, we only need to assume that there is a partition $Q$ of $[a,b]$ such that $\alpha$ is injective on each subinterval of $Q$.
    
    If the net $S_{f,\alpha}$ has a limit $A \in \reals$, then there is a tagged partition $(P,T)$ of $[a,b]$ such that
    %
    \begin{equation*}
        \abs{A - S_{f,\alpha}(P',T')} < 1,
        \quad \text{implying that} \quad
        \abs{S_{f,\alpha}(P',T')} < M \defn \abs{A} + 1,
    \end{equation*}
    %
    whenever $(P,T) \preceq (P',T')$. By replacing $P$ with $P \union Q$ we may assume that $\alpha$ is injective on each subinterval of $P$, since $\alpha$ is clearly also injective on each subinterval of $P \union Q$.
    
    Write $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Let $x \in [x_{k-1},x_k]$ and consider the multiset $T_x = \{t_1, \ldots, t_{k-1}, x, t_{k+1}, \ldots, t_n\}$ and the corresponding tagged partition $(P,T_x)$. We then have
    %
    \begin{equation*}
        f(x) \Delta \alpha_k
            = S_{f,\alpha}(P,T_x) - \sum_{i \neq k} f(t_i) \Delta \alpha_i.
    \end{equation*}
    %
    Since $\alpha$ is injective on $[x_{k-1},x_k]$ we have $\Delta\alpha_k \neq 0$, so the above implies that
    %
    \begin{equation*}
        \abs{f(x)}
            = \frac{1}{\abs{\Delta \alpha_k}} \abs[\bigg]{ S_{f,\alpha}(P,T_x) - \sum_{i \neq k} f(t_i) \Delta \alpha_i }
            < \frac{1}{\abs{\Delta \alpha_k}} \biggl( M + \abs[\bigg]{ \sum_{i \neq k} f(t_i) \Delta \alpha_i } \biggr),
    \end{equation*}
    %
    where the inequality follows since $(P,T) \preceq (P,T_x)$ for all $x \in [x_{k-1},x_k]$. The right-hand side is thus as upper bound for $\abs{f}$ on $[x_{k-1},x_k]$, and there are finitely many such intervals, so $f$ is bounded on $[a,b]$ as claimed.
\end{remark}

Below we fix an interval $[a,b]$ and (bounded) integrators $\alpha$ and $\beta$ on it.

\begin{proposition}[Linearity of the integral]
    \label{prop:integral-linearity}
    Let $f,g \in \integrable[\alpha]{a,b}$ and $c_1, c_2 \in \reals$. Then:
    %
    \begin{enumprop}
        \item $c_1 f + c_2 g$ is $\alpha$-integrable on $[a,b]$ and
        %
        \begin{equation*}
            \int_a^b (c_1 f + c_2 g) \dif\alpha
                = c_1 \int_a^b f \dif\alpha + c_2 \int_a^b g \dif\alpha.
        \end{equation*}
        %
        In particular, $\integrable[\alpha]{a,b}$ is a vector space.

        \item $f$ is $(c_1 \alpha + c_2 \beta)$-integrable on $[a,b]$ and
        %
        \begin{equation*}
            \int_a^b f \dif(c_1 \alpha + c_2 \beta)
                = c_1 \int_a^b f \dif\alpha + c_2 \int_a^b f \dif\beta.
        \end{equation*}
    \end{enumprop}
\end{proposition}

\begin{proof}
    This follows immediately from the bilinearity of the map $(f,\alpha) \mapsto S_{f,\alpha}$ along with basic properties of nets.
\end{proof}


\begin{proposition}
    \label{prop:integral-dividing-interval}
    Consider $f, \alpha \colon [a,b] \to \reals$ and let $c \in (a,b)$. If two of the three integrals in \cref{eq:integral-dividing-interval} exist, then so does the third and we have
    %
    \begin{equation}
        \label{eq:integral-dividing-interval}
        \int_a^c f \dif\alpha + \int_c^b f \dif\alpha
            = \int_a^b f \dif\alpha.
    \end{equation}
\end{proposition}

\begin{proof}
    Let $(P,T)$ be a tagged partition of $[a,b]$ such that $c \in P$, and let $P_1 = P \intersect [a,c]$ and $P_2 = P \intersect [c,b]$, and $T_1 = T \intersect [a,c]$ and $T_2 = T \intersect [c,b]$. Then
    %
    \begin{equation}
        \label{eq:Riemann-sums-dividing-interval}
        S_{f,\alpha}(P,T)
            = S_{f,\alpha}(P_1,T_1) + S_{f,\alpha}(P_2,T_2).
    \end{equation}
    %
    If two of the three integrals in \cref{eq:integral-dividing-interval} exist, then two of the nets in \cref{eq:Riemann-sums-dividing-interval} converge to the respective integrals. The third net then also converges to the relevant integral: Notice that we assume without loss of generality that $c \in P$, since adding $c$ to a partition simply yields a finer partition. Also notice that any partition $P'$ of e.g. $[a,c]$ is on the form $P' = P \intersect [a,c]$ for some partition $P$ of $[a,b]$.
\end{proof}


\begin{proposition}[Integration by parts]
    \label{prop:integration-by-parts}
    Given functions $f,\alpha \colon [a,b] \to \reals$, assume that $f \in \integrable[\alpha]{a,b}$. Then $\alpha \in \integrable[f]{a,b}$ and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha + \int_a^b \alpha \dif f
            = f(b)\alpha(b) - f(a)\alpha(a).
    \end{equation*}
    %
    In particular, exchanging $f$ and $\alpha$ we have
    %
    \begin{equation*}
        f \in \integrable[\alpha]{a,b}
            \quad \text{if and only if} \quad
            \alpha \in \integrable[f]{a,b}.
    \end{equation*}
\end{proposition}

\begin{proof}
    Let $(P,T)$ be a tagged partition of $[a,b]$ with $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Writing $A = f(b)\alpha(b) - f(a)\alpha(a)$, notice that
    %
    \begin{equation*}
        A
            = \sum_{i=1}^n f(x_i) \alpha(x_i) - \sum_{i=1}^n f(x_{i-1}) \alpha(x_{i-1}),
    \end{equation*}
    %
    and that
    %
    \begin{equation*}
        S_{\alpha,f}(P,T)
            = \sum_{i=1}^n \alpha(t_i) \Delta f_i
            = \sum_{i=1}^n f(x_i) \alpha(t_i) - \sum_{i=1}^n f(x_{i-1}) \alpha(t_i).
    \end{equation*}
    %
    Hence we have
    %
    \begin{align*}
        A - S_{\alpha,f}(P,T)
            &= \sum_{i=1}^n f(x_i) (\alpha(x_i) - \alpha(t_i)) + \sum_{i=1}^n f(x_{i-1}) (\alpha(t_i) - \alpha(x_{i-1})) \\
            &= S_{f,\alpha}(P \union T, P'),
    \end{align*}
    %
    where $P'$ is obtained from $P$ by duplicating\footnote{Recall that if $(P,T)$ is a tagged partition, then $T$ is a \emph{multiset}.} appropriate elements such that each subinterval of $P \union T$ contains the corresponding element from $P'$. Notice that if $P$ and $T$ have any elements in common, these are not duplicated in the union $P \union T$. However, in this case the corresponding terms in the sum above vanish, so the last equality does in fact hold. Since $P \union T$ is finer than $P$, the claim follows by taking the limit of $S_{\alpha,f}$.
\end{proof}


\begin{proposition}[Change of variables in Riemann--Stieljes integrals]
    Let $f \in \integrable[\alpha]{a,b}$, and let $\phi \colon I \to [a,b]$ be a monotonic (or equivalently continuous) bijection where $I$ is an interval with endpoints $c$ and $d$. Assume that $a = \phi(c)$ and $b = \phi(d)$. Then $f \circ \phi \in \integrable[\alpha \circ \phi]{c,d}$ and
    %
    \begin{equation*}
        \int_a^b f \dif \alpha
            = \int_c^d f \circ \phi \dif(\alpha \circ \phi).
    \end{equation*}
\end{proposition}

\begin{proof}
    Since $\phi$ is bijective it is strictly monotonic, so it induces an order isomorphism $\calP'(I) \to \calP'[a,b]$ given by $(P, T) \mapsto (\phi(P), \phi(T))$.

    Now let $(P,T) \in \calP'(I)$ with $P = \{y_0, \ldots, y_n\}$ and $T = \{s_1, \ldots, s_n\}$. Assume for definiteness that $\phi$ is increasing so that $I = [c,d]$, and write $x_i = \phi(y_i)$ and $t_i = \phi(s_i)$. Then we have
    %
    \begin{equation*}
        S_{f \circ \phi, \alpha \circ \phi}(P,T)
            = \sum_{i=1}^n (f \circ \phi)(s_i) \Delta(\alpha \circ \phi)_i
            = \sum_{i=1}^n f \circ \phi(t_i) \Delta \alpha_i
            = S_{f,\alpha}(\phi(P),\phi(T)).
    \end{equation*}
    %
    Since the map $(P,T) \mapsto (\phi(P),\phi(T))$ is an order isomorphism, each side above converges to the corresponding integral, proving the claim.
\end{proof}


\begin{proposition}[Reduction to a Riemann integral]
    \label{prop:reduction-to-Riemann-integral}
    Let $\alpha \in C^1[a,b]$ and $f \in \integrable[\alpha]{a,b}$. Then $f \alpha' \in \integrable{a,b}$, and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \int_a^b f \alpha'.
    \end{equation*}
\end{proposition}

\begin{proof}
    Consider the Riemann(--Stieltjes) sums
    %
    \begin{equation*}
        S_{f\alpha'}(P,T)
            = \sum_{i=1}^n f(t_i) \alpha'(t_i) \Delta x_i
        \quad \text{and} \quad
        S_{f,\alpha}(P,T)
            = \sum_{i=1}^n f(t_i) \Delta \alpha_i.
    \end{equation*}
    %
    By the mean value theorem we can write $\Delta \alpha_i = \alpha'(s_i) \Delta x_i$ for appropriate $s_i \in (x_{i-1}, x_i)$. It follows that
    %
    \begin{equation*}
        S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T)
            = \sum_{i=1}^n f(t_i) (\alpha'(s_i) - \alpha'(t_i)) \Delta x_i.
    \end{equation*}
    %
    By uniform continuity of $\alpha'$, given $\epsilon > 0$ there exists a $\delta > 0$ such that $\abs{x-y} < \delta$ implies $\abs{\alpha'(x) - \alpha'(y)} < \epsilon$ for all $x,y \in [a,b]$. Choosing a tagged partition $(P,T)$ with $\norm{P} < \delta$ we thus have
    %
    \begin{equation*}
        \abs{S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T)}
            \leq \norm{f}_{\sup} \epsilon (b-a).
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, the left-hand side converges to zero. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ \int_a^b f \dif\alpha - S_{f\alpha'}(P,T) }
            \leq \abs[\bigg]{ \int_a^b f \dif\alpha - S_{f,\alpha}(P,T) } + \abs{ S_{f,\alpha}(P,T) - S_{f\alpha'}(P,T) },
    \end{equation*}
    %
    which converges to zero. Hence $S_{f\alpha'}(P,T)$ converges, so $f\alpha' \in \integrable{a,b}$, and its integral equals the $\alpha$-integral of $f$ as claimed.
\end{proof}


\section{Increasing integrators}

\begin{definition}
    Let $f, \alpha \colon [a,b] \to \reals$ be bounded functions, and assume that $\alpha$ is increasing. Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$, and let
    %
    \begin{align*}
        M_i(f)
            &= \sup \set[\big]{f(x)}{x \in [x_{i-1}, x_i]}, \\
        m_i(f)
            &= \inf \set[\big]{f(x)}{x \in [x_{i-1}, x_i]}.
    \end{align*}
    %
    The numbers
    %
    \begin{equation*}
        U_{f,\alpha}(P)
            = \sum_{i=1}^n M_i(f) \Delta \alpha_i
        \quad \text{and} \quad
        L_{f,\alpha}(P)
            = \sum_{i=1}^n m_i(f) \Delta \alpha_i
    \end{equation*}
    %
    are called the \emph{upper and lower Stieltjes sums} of $f$ with respect to $\alpha$ for the partition $P$.
\end{definition}
%
Since $\alpha$ is increasing we have $\Delta\alpha_i \geq 0$, so it is immediate that
%
\begin{equation*}
    L_{f,\alpha}(P)
        \leq S_{f,\alpha}(P,T)
        \leq U_{f,\alpha}(P)
\end{equation*}
%
for any tagged partition $(P,T)$ of $[a,b]$. It is also clear that, if $P \subseteq P'$, then
%
\begin{equation}
    \label{eq:upper-lower-sum-decreasing-increasing}
    U_{f,\alpha}(P) \geq U_{f,\alpha}(P')
    \quad \text{and} \quad
    L_{f,\alpha}(P) \leq L_{f,\alpha}(P'),
\end{equation}
%
and that for any pair of partitions $P_1$ and $P_2$ we have
%
\begin{equation}
    \label{eq:upper-lower-sum-inequality}
    L_{f,\alpha}(P_1) \leq U_{f,\alpha}(P_2).
\end{equation}


% https://tex.stackexchange.com/questions/44237/lower-and-upper-riemann-integrals
% Have changed lowint and the first one in upint -- need to adjust the rest in upint if I want to use them, also make versions of lowint if I want to use those
\def\upint{\mathchoice%
    {\mkern10mu\overline{\vphantom{\intop}\mkern10mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern2mu\underline{\vphantom{\intop}\mkern10mu}\mkern-12mu\int}

\begin{definition}
    Let $f, \alpha \colon [a,b] \to \reals$ be bounded functions with $\alpha$ increasing. Then the numbers
    %
    \begin{equation*}
        \upint_a^b f \dif\alpha
            = \inf \set[\big]{U_{f,\alpha}(P)}{P \in \calP[a,b]}
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \lowint_a^b f \dif\alpha
            = \sup \set[\big]{L_{f,\alpha}(P)}{P \in \calP[a,b]}
    \end{equation*}
    %
    are called the \emph{upper and lower Stieltjes integrals} of $f$ with respect to $\alpha$ on $[a,b]$.
\end{definition}
%
We also use the notations $\overline{I}(f,\alpha)$ and $\underline{I}(f,\alpha)$ for the upper and lower integrals, respectively, when the interval $[a,b]$ is understood. It follows immediately from the definition and \cref{eq:upper-lower-sum-inequality} that $\underline{I}(f,\alpha) \leq \overline{I}(f,\alpha)$. 


\begin{theorem}[Riemann's condition]
    Let $f,\alpha \colon [a,b] \to \reals$ be bounded functions with $\alpha$ increasing. Then the following conditions are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:integrability} $f \in \integrable[\alpha]{a,b}$.
        \item \label{enum:Riemanns-condition} $f$ satisfies \emph{Riemann's condition} with respect to $\alpha$ on $[a,b]$: For every $\epsilon > 0$ there exists a partition $P$ of $[a,b]$ such that
        %
        \begin{equation}
            \label{eq:Riemanns-condition}
            U_{f,\alpha}(P) - L_{f,\alpha}(P) < \epsilon.
        \end{equation}
        \item \label{enum:upper-lower-integrals-equal} $\underline{I}(f,\alpha) = \overline{I}(f,\alpha)$.
    \end{enumthm}
    %
    In this case we have
    %
    \begin{equation*}
        \lowint_a^b f \dif\alpha
            = \int_a^b f \dif\alpha
            = \upint_a^b f \dif\alpha.
    \end{equation*}
\end{theorem}

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:integrability} $\implies$ \subcref{enum:Riemanns-condition}]
    Let $\epsilon > 0$, and choose a partition $P = \{x_0, \ldots, x_n\}$ of $[a,b]$ such that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{i=1}^n f(t_i) \Delta\alpha_i - \int_a^b f \dif\alpha }
        < \epsilon
    \end{equation*}
    %
    for all $t_i \in [x_{i-1},x_i]$. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i }
        < 2 \epsilon
    \end{equation*}
    %
    for all $t_i, t_i' \in [x_{i-1},x_i]$. For any $\delta > 0$ there exist $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i) - f(t_i')
        > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    From this it follows that
    %
    \begin{align*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P)
        &= \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i \\
        &< \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i + \delta (\alpha(b) - \alpha(a)) \\
        &< 3\epsilon
    \end{align*}
    %
    for an appropriate choice of $\delta$. Since $\epsilon$ was arbitrary, this proves \subcref{enum:Riemanns-condition}.
    
    \item[\subcref{enum:Riemanns-condition} $\implies$ \subcref{enum:upper-lower-integrals-equal}]
    If $P$ is any partition of $[a,b]$ we have
    %
    \begin{equation*}
        L_{f,\alpha}(P)
        \leq \lowint_a^b f \dif\alpha
        \leq \upint_a^b f \dif\alpha
        \leq U_{f,\alpha}(P).
    \end{equation*}
    %
    Thus \cref{eq:Riemanns-condition} implies that $0 \leq \overline{I}(f,\alpha) - \underline{I}(f,\alpha) < \epsilon$ for every $\epsilon > 0$, proving \subcref{enum:upper-lower-integrals-equal}.
    
    \item[\subcref{enum:upper-lower-integrals-equal} $\implies$ \subcref{enum:integrability}]
    Let $\epsilon > 0$. There exists a partition $P$ of $[a,b]$ such that
    %
    \begin{equation*}
        \underline{I}(f,\alpha) - \epsilon
        < L_{f,\alpha}(P)
        \leq S_{f,\alpha}(P,T)
        \leq U_{f,\alpha}(P)
        < \overline{I}(f,\alpha) + \epsilon
    \end{equation*}
    %
    for any choice of points $T$ such that $(P,T)$ is a tagged partition. Denoting the common value of $\underline{I}(f,\alpha)$ and $\overline{I}(f,\alpha)$ by $A$, this shows that $\abs{S_{f,\alpha}(P',T') - A} < \epsilon$ for all tagged partitions $(P',T')$ with $P \subseteq P'$. Hence $f \in \integrable[\alpha]{a,b}$, and the integral of $f$ with respect to $\alpha$ equals $A$.
\end{proofsec}
\end{proof}


\section{Mean value theorems for Riemann--Stieltjes integrals}

\begin{proposition}[The first mean value theorem]
    \label{prop:integral-MVT1}
    Let $\alpha \colon [a,b] \to \reals$ be increasing, and let $f \in \integrable[\alpha]{a,b}$. Let $m = \inf_{x \in [a,b]} f(x)$ and $M = \sup_{x \in [a,b]} f(x)$. Then there exists a $c \in \reals$ with $m \leq c \leq M$ such that
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = c \int_a^b \dif\alpha
            = c (\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    In particular, if $f$ is continuous then $c = f(x_0)$ for some $x_0 \in [a,b]$.
\end{proposition}

\begin{proof}
    If $\alpha(a) = \alpha(b)$ then both sides are zero, so assume that $\alpha(a) < \alpha(b)$. From the inequalities
    %
    \begin{equation*}
        m (\alpha(b) - \alpha(a))
            \leq \int_a^b f \dif\alpha
            \leq M (\alpha(b) - \alpha(a))
    \end{equation*}
    %
    follow that
    %
    \begin{equation*}
        m 
            \leq \frac{1}{\alpha(b) - \alpha(a)} \int_a^b f \dif\alpha
            \leq M,
    \end{equation*}
    %
    so defining $c$ as the middle term, the claim follows.
\end{proof}


\begin{proposition}[The second mean value theorem]
    \label{prop:integral-MVT2}
    Let $f,\alpha \colon [a,b] \to \reals$ with $f$ monotonic and $\alpha$ continuous. Then there exists a point $x_0 \in [a,b]$ such that
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = f(a) \int_a^{x_0} \dif\alpha + f(b) \int_{x_0}^b \dif\alpha.
    \end{equation*}
\end{proposition}

\begin{proof}
    By replacing $f$ with $-f$ we may assume that $f$ is increasing. Then $f$ is of bounded variation, so \cref{prop:integration-by-parts} implies that $f \in \integrable[\alpha]{a,b}$. Another application of this proposition along with \cref{prop:integral-MVT1} yields a $x_0 \in [a,b]$
    %
    \begin{align*}
        \int_a^b f \dif\alpha
            &= f(b)\alpha(b) - f(a)\alpha(a) - \int_a^b \alpha \dif f \\
            &= f(b)\alpha(b) - f(a)\alpha(a) - \alpha(x_0)(f(b) - f(a)) \\
            &= f(a) (\alpha(x_0) - \alpha(a)) + f(b)(\alpha(b) - \alpha(x_0)),
    \end{align*}
    %
    as desired.
\end{proof}


We attempt to understand the two mean value theorems. In the case where $\alpha(a) < \alpha(b)$, the first essentially says that the \enquote{$\alpha$-average} of $f$, given by
%
\begin{equation*}
    \frac{1}{\alpha(b) - \alpha(a)} \int_a^b f \dif\alpha,
\end{equation*}
%
lies between the minimum and maximum of $f$, as we would expect. Notice that the theorem yields an element $c$ of the \emph{codomain} of $f$ that serves as the \enquote{mean value} of $f$.

As for the second theorem, if $f$ is monotonic then the integral of $f$ is a weighted sum of the values of $f$ at the endpoints. The theorem says that there is an element $x_0$ of the \emph{domain} that splits up the interval $[a,b]$ such that the corresponding integrals gives the desired weights. This element $x_0$ in some sense serves as a \enquote{mean} of $f$.


\section{Integrators of bounded variation}

\begin{theorem}
    \label{thm:alpha-integrable-implies-V-integrable}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $V(x) = V_\alpha(a,x)$ for $x \in (a,b]$ and $V(a) = 0$. Then $\integrable[\alpha]{a,b} \subseteq \integrable[V]{a,b}$.
\end{theorem}

\begin{proof}
    Let $f \in \integrable[\alpha]{a,b}$, and choose $M > 0$ such that $\abs{f} \leq M$. Choose a partition $P = \{x_0, \ldots, x_n\}$ of $[a,b]$ such that $V(b) < \sum_{i=1}^n \abs{\Delta\alpha_i} + \epsilon$. Then
    %
    \begin{align*}
        \sum_{i=1}^n (M_i(f) - m_i(f)) ( \Delta V_i - \abs{\Delta\alpha_i})
            &\leq 2M \sum_{i=1}^n (\Delta V_i - \abs{\Delta\alpha_i}) \\
            &= 2M \biggl( V(b) - \sum_{i=1}^n \abs{\Delta\alpha_i} \biggr) \\
            &< 2M \epsilon.
    \end{align*}
    %
    Also choose $P$ such that $\abs{ \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i } < \epsilon$ for all $t_i, t_i' \in [x_{i-1}, x_i]$. Next let $\delta > 0$. For $i = 1, \ldots, n$, if $\Delta\alpha_i \geq 0$ choose $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i) - f(t_i')
            > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    If instead $\Delta\alpha_i < 0$, choose $t_i, t_i'$ such that
    %
    \begin{equation*}
        f(t_i') - f(t_i)
            > M_i(f) - m_i(f) - \delta.
    \end{equation*}
    %
    It follows that
    %
    \begin{equation*}
        \sum_{i=1}^n (M_i(f) - m_i(f)) \abs{\Delta\alpha_i}
            < \sum_{i=1}^n (f(t_i) - f(t_i')) \Delta\alpha_i
              + \delta V(b)
            < 2 \epsilon
    \end{equation*}
    %
    for an appropriate choice of $\delta$. Combining these inequalities yields
    %
    \begin{equation*}
        U_{f,V}(P) - L_{f,V}(P)
            = \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta V_i
            < 2(M+1)\epsilon,
    \end{equation*}
    %
    and since $\epsilon$ was arbitrary, this shows that $f \in \integrable[V]{a,b}$.
\end{proof}
%
Since $\alpha = V - (V - \alpha)$ and both $V$ and $V - \alpha$ are increasing, this allows us to reduce questions about integrators of bounded variation to questions about monotonic integrators. In particular it lets us use Riemann's condition to prove integrability with respect to integrators of bounded variation.


\begin{corollary}
    \label{cor:integrable-on-subinterval}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Then $f \in \integrable[\alpha]{c,d}$ for every subinterval $[c,d]$ of $[a,b]$.
\end{corollary}

\begin{proof}
    By \cref{thm:alpha-integrable-implies-V-integrable} and \cref{prop:integral-linearity}, it suffices to prove the claim when $\alpha$ is increasing. Furthermore, by \cref{prop:integral-dividing-interval} it suffices to show that $f$ is $\alpha$-integrable on $[a,x]$ for all $x \in (a,b]$. Given $\epsilon > 0$, by \cref{enum:Riemanns-condition} there is a partition $P$ of $[a,b]$ such that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P) < \epsilon.
    \end{equation*}
    %
    By \cref{eq:upper-lower-sum-decreasing-increasing}, adjoining $x$ to $P$ preserves the inequality. Writing $P = \{x_0, \ldots, x_n\}$ we may thus assume that $x = x_k$ for some $k \in \{1, \ldots, n\}$. Letting $P' = P \intersect [a,x]$ we thus have
    %
    \begin{align*}
        U_{f,\alpha}(P') - L_{f,\alpha}(P')
            &= \sum_{i=1}^k (M_i(f) - m_i(f)) \Delta\alpha_i
             \leq \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i \\
            &= U_{f,\alpha}(P) - L_{f,\alpha}(P)
             < \epsilon,
    \end{align*}
    %
    where the first inequality follows since every term in the second sum is non-negative. Thus $f$ is integrable on $[a,x]$.
\end{proof}


\begin{proposition}
    Let $f,\alpha \colon [a,b] \to \reals$ be functions with $f$ continuous and $\alpha$ of bounded variation. Then $f$ is $\alpha$-integrable, and $\alpha$ is $f$-integrable.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $\epsilon > 0$. Uniform continuity of $f$ furnishes a $\delta < 0$ such that $\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} < \epsilon$ for $x,y \in [a,b]$. Let $P = \{x_0,\ldots,x_n\}$ be a partition with $\norm{P} < \delta$. Then $M_i(f) - m_i(f) \leq \epsilon$, implying that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P)
            = \sum_{i=1}^n (M_i(f) - m_i(f)) \Delta\alpha_i
            \leq \epsilon (\alpha(b) - \alpha(a)),
    \end{equation*}
    %
    and since $\epsilon$ was arbitrary, it follows from Riemann's condition that $f \in \integrable[\alpha]{a,b}$. The final claim follows from \cref{prop:integration-by-parts}.
\end{proof}

\fleuronbreak

For functions $f \colon [a,b] \to \reals$ and $g \colon f([a,b]) \to \reals$, when is the composition $g \circ f$ integrable? In the Lebesgue theory all that is required is that $f$ and $g$ are measurable, but as far as I know, no such result is available for the Riemann integral. However, in the case where $g$ is continuous, \cref{prop:continuity-integrability} below shows that $g \circ f$ is indeed integrable. We first prove this and then prove a couple of important corollaries.

\begin{proposition}
    \label{prop:continuity-integrability}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Choose $m,M \in \reals$ such that $m \leq f \leq M$. If $\phi \colon [m,M] \to \reals$ is continuous, then $\phi \circ f \in \integrable[\alpha]{a,b}$.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Put $g = \phi \circ f$ and let $\epsilon > 0$. Uniform continuity of $\phi$ yields a $\delta > 0$ such that $\abs{s-t} < \delta$ implies $\abs{\phi(s) - \phi(t)} < \epsilon$ for $s,t \in [m,M]$. Also choose $\delta$ such that $\delta \leq \epsilon$. Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$ such that
    %
    \begin{equation*}
        U_{f,\alpha}(P) - L_{f,\alpha}(P) < \delta^2.
    \end{equation*}
    %
    Let $A$ consist of those numbers $i \in \{1, \ldots, n\}$ such that $M_i(f) - m_i(f) < \delta$, and let $B$ consist of the remaining $i$. For $i \in A$ we then have $M_i(g) - m_i(g) \leq \epsilon$.

    Let $K > 0$ be such that $\abs{\phi} \leq K$. For $i \in B$ we then have $M_i(g) - m_i(g) \leq 2K$. Furthermore, we have
    %
    \begin{equation*}
        \sum_{i \in B} \Delta\alpha_i
            \leq \frac{1}{\delta} \sum_{i \in B} (M_i(f) - m_i(f)) \Delta\alpha_i
            \leq \frac{1}{\delta} \bigl( U_{f,\alpha}(P) - L_{f,\alpha}(P) \bigr)
            < \delta.
    \end{equation*}
    %
    It thus follows that
    %
    \begin{align*}
        U_{g, \alpha}(P) - L_{g, \alpha}(P)
            &= \sum_{i \in A} (M_i(g) - m_i(g)) \Delta\alpha_i
               + \sum_{i \in B} (M_i(g) - m_i(g)) \Delta\alpha_i \\
            &\leq \epsilon (\alpha(b) - \alpha(a))
               + 2K \delta \\
            &\leq (\alpha(b) - \alpha(a) + 2K) \epsilon.
    \end{align*}
    %
    Since $\epsilon$ was arbitrary, it follows that $g \in \integrable[\alpha]{a,b}$.
\end{proof}


For any function $f \colon X \to \reals$, recall that the \emph{positive and negative parts} of $f$ are the functions $f^+ = f \join 0$ and $f^- = -(f \meet 0)$ respectively, and that $f = f^+ - f^-$. We notice that $f^+$ and $f^-$ are both non-negative.

\begin{corollary}
    \label{cor:positive-negative-part}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Then a function $f \colon [a,b] \to \reals$ is $\alpha$-integrable if and only if $f^+$ and $f^-$ are, in which case
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \int_a^b f^+ \dif\alpha - \int_a^b f^- \dif\alpha.
    \end{equation*}
\end{corollary}

\begin{proof}
    This follows immediately from \cref{prop:continuity-integrability}, since the maps $x \mapsto x \join 0$ and $x \mapsto -(x \meet 0)$ are continuous.
\end{proof}


\begin{corollary}
    \label{cor:product-abs-integrable}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f,g \in \integrable[\alpha]{a,b}$. Then the functions $\abs{f}$ and $fg$ are also $\alpha$-integrable. In particular, $\integrable[\alpha]{a,b}$ is an $\reals$-algebra.
    
    If $\alpha$ is increasing we also have
    %
    \begin{equation}
        \label{eq:integral-triangle-inequality}
        \abs[\bigg]{ \int_a^b f \dif\alpha }
            \leq \int_a^b \abs{f}\dif\alpha.
    \end{equation}
\end{corollary}
%
Recall from \cref{prop:integral-linearity} that $\integrable[\alpha]{a,b}$ is always a vector space. 

\begin{proof}
    Integrability of $\abs{f}$ follows from \cref{prop:continuity-integrability} since $x \mapsto \abs{x}$ is continuous. The inequality \cref{eq:integral-triangle-inequality} follows since $f \leq \abs{f}$, and since the $\alpha$-integral is increasing when $\alpha$ is.

    For the product $fg$, notice that
    %
    \begin{equation*}
        2fg = (f+g)^2 - f^2 - g^2,
    \end{equation*}
    %
    and that the function $x \mapsto x^2$ is continuous.
\end{proof}


\begin{proposition}
    \label{prop:integral-as-integrator}
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, let $f,g \in \integrable[\alpha]{a,b}$, and define
    %
    \begin{equation*}
        F(x)
            = \int_a^x f \dif\alpha
    \end{equation*}
    %
    for $x \in [a,b]$. Then $g \in \integrable[F]{a,b}$ and
    %
    \begin{equation*}
        \int_a^b fg \dif\alpha
            = \int_a^b g \dif F.
    \end{equation*}
\end{proposition}

\begin{remark}
    It suffices to assume that $fg$ is integrable instead of $f$. Conversely, notice that $fg \in \integrable[\alpha]{a,b}$ by \cref{cor:product-abs-integrable} under the assumptions in \cref{prop:integral-as-integrator}.
    
    If $f$ is continuous and $\alpha$ is continuously differentiable, then this result follows from the first fundamental theorem of calculus, which we prove in \cref{enum:first-fundamental-theorem-of-calculus}. For in this case $F$ is continuously differentiable with $F'(x) = f(x)\alpha'(x)$ (as we show below, we may assume that $\alpha$ is increasing), so applying \cref{prop:reduction-to-Riemann-integral} twice yields
    %
    \begin{equation*}
        \int_a^b g \dif F
            = \int_a^b g F'
            = \int_a^b fg \alpha'
            = \int_a^b fg \dif\alpha.
    \end{equation*}
    %
    However, the claim holds more generally, as we now show.
\end{remark}

\begin{proofof}[Proof of \cref{prop:integral-as-integrator}]
    We may assume that $\alpha$ is increasing: Define $V$ as in \cref{thm:alpha-integrable-implies-V-integrable}, and let
    %
    \begin{equation*}
        F_1(x)
            = \int_a^x f \dif V,
        \quad \text{and} \quad
        F_2(x)
            = \int_a^x f \dif (V - \alpha),
    \end{equation*}
    %
    so that $F = F_1 - F_2$. Then assuming that the proposition holds for increasing integrators, we have
    %
    \begin{align*}
        \int_a^b fg \dif\alpha
            &= \int_a^b fg \dif V - \int_a^b fg \dif (V-\alpha)
             = \int_a^b g \dif F_1 - \int_a^b g \dif F_2 \\
            &= \int_a^b g \dif (F_1 - F_2)
             = \int_a^b g \dif F.
    \end{align*}
    %
    Hence we assume below that $\alpha$ is increasing.
    
    Let $(P,T)$ be a tagged partition of $[a,b]$ with $P = \{x_0, \ldots, x_n\}$ and $T = \{t_1, \ldots, t_n\}$. Then
    %
    \begin{equation*}
        S_{g,F}(P,T)
            = \sum_{i=1}^n g(t_i) \int_{x_{i-1}}^{x_i} f(t) \dif\alpha(t)
            = \sum_{i=1}^n \int_{x_{i-1}}^{x_i} f(t) g(t_i) \dif\alpha(t),
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \int_a^b fg \dif\alpha
            = \sum_{i=1}^n \int_{x_{i-1}}^{x_i} f(t) g(t) \dif\alpha(t).
    \end{equation*}
    %
    Letting $M = \sup_{x \in [a,b]} \abs{f(x)}$ we thus have
    %
    \begin{align*}
        \abs[\bigg]{ S_{g,F}(P,T) - \int_a^b fg \dif\alpha }
            &= \abs[\bigg]{ \sum_{i=1}^n \int_{x_{i-1}}^{x_i} f(t) \bigl( g(t_i) - g(t) \bigr) \dif\alpha(t) } \\
            &\leq M \sum_{i=1}^n \int_{x_{i-1}}^{x_i} \abs{ g(t_i) - g(t) } \dif\alpha(t) \\
            &\leq M \sum_{i=1}^n \int_{x_{i-1}}^{x_i} \bigl( M_i(g) - m_i(g) \bigr) \dif\alpha \\
            &= M \bigl( U_{g,\alpha}(P) - L_{g,\alpha}(P) \bigr).
    \end{align*}
    %
    Since $g$ is $\alpha$-integrable, this can be made as small as desired. This proves the claim.
\end{proofof}


\section{The fundamental theorems of calculus}

\begin{theorem}[The integral as a function of the interval]
    Let $\alpha \colon [a,b] \to \reals$ be of bounded variation, and let $f \in \integrable[\alpha]{a,b}$. Define a function $F \colon [a,b] \to \reals$ by
    %
    \begin{equation}
        \label{eq:integral-function-of-interval}
        F(x)
            = \int_a^x f \dif\alpha.
    \end{equation}
    %
    Then the following hold:
    %
    \begin{enumthm}
        \item \label{enum:integral-is-BV} $F$ is of bounded variation.
        \item \label{enum:continuity-of-integral} Every point of continuity of $\alpha$ is also a point of continuity of $F$.
        \item \label{enum:first-fundamental-theorem-of-calculus} Assume that $\alpha$ is increasing. If $f$ is continuous and $\alpha$ differentiable at $x \in [a,b]$, then $F$ is differentiable at $x$ with $F'(x) = f(x) \alpha'(x)$.
    \end{enumthm}
\end{theorem}
%
The result \cref{enum:first-fundamental-theorem-of-calculus} is known as \emph{the first fundamental theorem of calculus}. Notice that the integral in \cref{eq:integral-function-of-interval} exists by \cref{cor:integrable-on-subinterval}, and so do the integrals in the proof below.

\begin{proof}
\begin{proofsec}
    \item[Proof of \subcref{enum:integral-is-BV}]
    Since $\boundedvar{a,b}$ is a vector space, we may assume that $\alpha$ is increasing. By \cref{cor:positive-negative-part} we may further assume that $f$ is positive. Then $F$ is increasing, hence of bounded variation.\footnote{E.g. \textcite[Theorem~7.32]{apostolanalysis} attempts to prove this using \cref{prop:integral-MVT1}, but his argument is not, as far as I can tell, correct as stated. Hence we use a different argument, more in the spirit of Lebesgue.}
    
    \item[Proof of \subcref{enum:continuity-of-integral}]
    By \cref{prop:total-variation-continuity} we may assume that $\alpha$ is increasing. Let $x,y \in [a,b]$ with $x \neq y$, and let $I$ denote the closed interval between $x$ and $y$. \Cref{prop:integral-MVT1} now furnishes a $c_{xy} \in \reals$ with
    %
    \begin{equation*}
        \inf_{t \in [a,b]} f(t)
            \leq \inf_{t \in I} f(t)
            \leq c_{xy}
            \leq \sup_{t \in I} f(t)
            \leq \sup_{t \in [a,b]} f(t),
    \end{equation*}
    %
    such that
    %
    \begin{equation}
        \label{eq:fundamental-theorem-difference}
        F(y) - F(x)
            = \int_x^y f \dif\alpha
            = c_{xy} (\alpha(y) - \alpha(x)).
    \end{equation}
    %
    Assume now that $\alpha$ is continuous at $x$. Since $y \mapsto c_{xy}$ is bounded on $[a,b]$ (since $f$ is), letting $y \to x$ thus yields continuity of $F$ at $x$.

    \item[Proof of \subcref{enum:first-fundamental-theorem-of-calculus}]
    If $f$ is continuous at $x$, then $c_{xy} \to f(x)$ as $y \to x$, since $c_{xy}$ is bounded by the supremum and infimum of $f$ in a neighbourhood of $x$. Now \cref{eq:fundamental-theorem-difference} implies that
    %
    \begin{equation*}
        \frac{F(y) - F(x)}{y-x}
            = c_{xy} \frac{\alpha(y) - \alpha(x)}{y-x}
            \xrightarrow[y \to x]{}
            f(x) \alpha'(x),
    \end{equation*}
    %
    as desired.
\end{proofsec}
\end{proof}


\begin{remark}
    In the case $\alpha(x) = x$, \subcref{enum:first-fundamental-theorem-of-calculus} has a proof that does not use \cref{prop:integral-MVT1}: Simply note that
    %
    \begin{equation*}
        \frac{F(y)-F(x)}{y-x} - f(x)
            = \frac{1}{y-x} \int_x^y (f(t) - f(x)) \dif t,
    \end{equation*}
    %
    and notice that the integrand can be made less than any $\epsilon > 0$ if $\abs{t-x} < \delta$ for an appropriate $\delta > 0$. I am not sure that this proof can be generalised.
\end{remark}


\begin{theorem}[The second fundamental theorem of calculus]
    Let $f \in \integrable{a,b}$. If there exists a continuous function $F \colon [a,b] \to \reals$ that is differentiable on $(a,b)$ with $F' = f$, then
    %
    \begin{equation*}
        \int_a^b f
            = F(b) - F(a).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $P = \{x_0, \ldots, x_n\}$ be a partition of $[a,b]$. The mean value theorem furnishes points $t_i \in (x_{i-1}, x_i)$ such that $\Delta F_i = F'(t_i) \Delta x_i = f(t_i) \Delta x_i$. It follows that
    %
    \begin{equation*}
        \abs[\bigg]{ F(b) - F(a) - \int_a^b f }
            = \abs[\bigg]{ \sum_{i=1}^n f(t_i)\Delta x_i - \int_a^b f }
            < \epsilon
    \end{equation*}
    %
    if $P$ is fine enough. Since $\epsilon$ was arbitrary, this proves the theorem.
\end{proof}


\section{Further results on Riemann integrals}

\begin{corollary}[The second mean value theorem for Riemann integrals]
    Let $f,g \colon [a,b] \to \reals$ with $f$ monotonic and $g \in \integrable{a,b}$. Then there exists a point $x_0 \in [a,b]$ such that
    %
    \begin{equation*}
        \int_a^b fg
            = f(a) \int_a^{x_0} g + f(b) \int_{x_0}^b g.
    \end{equation*}
\end{corollary}
%
Both \textcite[Theorem~7.37]{apostolanalysis} and \textcite[Lemma~8.41]{folland2007} assume that $g$ is continuous (the latter even assumes that $f$ is right-continuous), but as far as I can tell the argument below goes through without this assumption.

\begin{proof}
    Define $\alpha(x) = \int_a^x g$. Then $\alpha$ is continuous by \cref{enum:continuity-of-integral}, so \cref{prop:integral-MVT2} yields an $x_0 \in [a,b]$ such that
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = f(a) \int_a^{x_0} \dif\alpha + f(b) \int_{x_0}^b \dif\alpha.
    \end{equation*}
    %
    Furthermore, since both $f$ and $g$ is Riemann-integrable ($f$ since it is monotonic), \cref{prop:integral-as-integrator} implies that
    %
    \begin{align*}
        \int_a^b fg
            &= \int_a^b f \dif\alpha
             = f(a) \int_a^{x_0} \dif\alpha + f(b) \int_{x_0}^b \dif\alpha \\
            &= f(a) \int_a^{x_0} g + f(b) \int_{x_0}^b g,
    \end{align*}
    %
    as desired.
\end{proof}

[TODO change of variables]


\section{Limit and continuity theorems}

\begin{proposition}
    \label{thm:integral-continuity}
    Let $f \colon [a,b] \times [c,d] \to \reals$ be continuous, and let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Then the function $F \colon [c,d] \to \reals$ given by
    %
    \begin{equation*}
        F(y)
            = \int_a^b f(x,y) \dif\alpha(x)
    \end{equation*}
    %
    is continuous.
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. By uniform continuity of $f$, given $\epsilon > 0$ there is a $\delta > 0$ such that $\norm{z - z'} < \delta$ implies $\abs{f(z) - f(z')} < \epsilon$ for $z,z' \in [a,b] \times [c,d]$. Given $y,y' \in [c,d]$ with $\abs{y - y'} < \delta$ we thus have
    %
    \begin{equation*}
        \abs{F(y) - F(y')}
            \leq \int_a^b \abs{f(x,y) - f(x,y')} \dif\alpha(x)
            \leq \epsilon(\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, this shows that $F$ is continuous.
\end{proof}


\begin{proposition}
    Let $f \colon [a,b] \times [c,d] \to \reals$ be bounded, and let $\alpha \colon [a,b] \to \reals$ be of bounded variation. Assume that $f(\,\cdot\,,y) \in \integrable[\alpha]{a,b}$ for all $y \in [c,d]$, that $f(x,\,\cdot\,)$ is continuous on $[c,d]$ and differentiable on $(c,d)$ for all $x \in [a,b]$, and that $D_2 f$ is continuous on $[a,b] \times (c,d)$. Then the function $F \colon [c,d] \to \reals$ given by
    %
    \begin{equation*}
        F(y)
            = \int_a^b f(x,y) \dif\alpha(x)
    \end{equation*}
    %
    is differentiable on $(c,d)$ and
    %
    \begin{equation*}
        F'(y)
            = \int_a^b D_2 f(x,y) \dif\alpha(x).
    \end{equation*}
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $y,y_0 \in (c,d)$ with $y \neq y_0$. By the mean value theorem we have
    %
    \begin{equation*}
        \frac{F(y) - F(y_0)}{y - y_0}
            = \int_a^b \frac{f(x,y) - f(x,y_0)}{y - y_0} \dif\alpha(x)
            = \int_a^b D_2 f(x, y_x) \dif\alpha(x)
    \end{equation*}
    %
    for some $y_x \in (c,d)$ lying between $y$ and $y_0$, depending on $x$. Let $I \subseteq (c,d)$ be a non-trivial compact interval containing $y$. Then $D_2 f$ is uniformly continuous on $[a,b] \times I$, so given $\epsilon > 0$ there is a $\delta > 0$ such that $\norm{z-z'} < \delta$ implies $\abs{D_2 f(z) - D_2 f(z')} < \epsilon$ for $z,z' \in [a,b] \times I$. For $y,y_0 \in I$ with $\abs{y-y_0} < \delta$ we also have $\abs{y_x-y_0} < \delta$ for all $x \in [a,b]$, and so
    %
    \begin{align*}
        \abs[\bigg]{ \int_a^b D_2 f(x, y_x) \dif\alpha(x)
            - \int_a^b D_2 f(x, y_0) \dif\alpha(x) }
            &\leq \int_a^b \abs{ D_2 f(x, y_x) - D_2 f(x, y_0) } \dif\alpha(x) \\
            &\leq \epsilon (\alpha(b) - \alpha(a)).
    \end{align*}
    %
    Since $\epsilon$ was arbitrary, this shows that $F$ is differentiable at $y_0$ with derivative as claimed.
\end{proof}


\begin{proposition}
    Let $\alpha$ be of bounded variation on $[a,b]$, and let $(f_n)_{n\in\naturals}$ be a sequence of $\alpha$-integrable functions on $[a,b]$ that converge uniformly to a function $f$. Then $f$ is also $\alpha$-integrable on $[a,b]$, and
    %
    \begin{equation*}
        \int_a^b f \dif\alpha
            = \lim_{n\to\infty} \int_a^b f_n \dif\alpha.
    \end{equation*}
    %
    In particular, $\integrable[\alpha]{a,b}$ is a closed subspace of $C[a,b]$ equipped with the uniform norm.

    [TODO $\alpha$ must be increasing! Or at least BV.]
\end{proposition}

\begin{proof}
    We may assume that $\alpha$ is increasing. Let $\epsilon_n = \norm{f_n - f}_\infty$ such that
    %
    \begin{equation*}
        f_n - \epsilon_n
            \leq f
            \leq f_n + \epsilon_n
    \end{equation*}
    %
    for $n \in \naturals$. It follows that
    %
    \begin{equation*}
        \int_a^b (f_n - \epsilon_n) \dif\alpha
            \leq \lowint_a^b f \dif\alpha
            \leq \upint_a^b f \dif\alpha
            \leq \int_a^b (f_n + \epsilon_n) \dif\alpha,
    \end{equation*}
    %
    and hence,
    %
    \begin{equation*}
        0
            \leq \upint_a^b f \dif\alpha - \lowint_a^b f \dif\alpha
            \leq 2 \epsilon_n (\alpha(b) - \alpha(a)).
    \end{equation*}
    %
    Thus the upper and lower integrals of $f$ are equal, so $f$ is $\alpha$-integrable. Finally we have
    %
    \begin{equation*}
        \abs[\bigg]{ \int_a^b f_n \dif\alpha - \int_a^b f \dif\alpha }
            \leq \int_a^b \abs{f_n - f} \dif\alpha
            \leq \epsilon_n (\alpha(b) - \alpha(a)),
    \end{equation*}
    %
    proving the claim.
\end{proof}


\section{Line integrals}

\newcommand{\partition}{\mathcal{P}}
\newcommand{\grad}{\nabla}

\noindent Recall that a \emph{path} in a topological space $X$ is a continuous map $\gamma \colon [a,b] \to X$, and that $\gamma$ is \emph{closed} if $\gamma(a) = \gamma(b)$. A subset $\Gamma \subseteq X$ is called a \emph{curve} in $X$ if there is a path $\alpha$ in $X$ whose image is $\Gamma$. The image of a path $\gamma$ is called its \emph{trace} and is denoted $\gamma^*$.

\begin{definition}[Equivalence of paths]
	Let $\alpha \colon [a,b] \to X$ and $\beta \colon [c,d] \to X$ be paths in a topological space $X$. If there is an increasing homeomorphism $\phi \colon [c,d] \to [a,b]$ such that $\beta = \alpha \circ \phi$, then $\alpha$ and $\beta$ are said to be \emph{properly equivalent}.

	If $\alpha$ and $\beta$ are closed paths with $\alpha(a) \neq \beta(c)$, then we also say that they are properly equivalent if there is a point $e \in (c,d)$ such that $\alpha$ and $\gamma$ are properly equivalent in the above sense, where $\gamma \colon [e, d-c+e] \to X$ is given by
	%
	\begin{equation*}
		\gamma(t) =
		\begin{cases}
			\beta(t), & t \in [e,d], \\
			\beta(t-d+c), & t \in [d,d-c+e].
		\end{cases}
	\end{equation*}
	
	If the map $\phi$ above is decreasing, then we say that $\alpha$ and $\beta$ are \emph{improperly equivalent}. The paths $\alpha$ and $\beta$ are \emph{equivalent} if they are either properly or improperly equivalent.
\end{definition}
%
Note that the condition that $\phi$ be an increasing (decreasing) homeomorphism is equivalent to it being continuous, strictly increasing (decreasing) and surjective. Also note that equivalent paths trace out the same curve in $X$.

\begin{definition}[Line integrals] % TODO: Assume f is bounded, as for regular integrals?
	Let $\gamma \colon [a,b] \to \setR^d$ be a path, and let $f \colon \gamma^* \to \setR^d$ be a vector field. Given a tagged partition $(P,T)$ of $[a,b]$ then, with notation as above, we form the sums
    %
    \begin{equation*}
        S_{f,\gamma}(P,T)
            = \sum_{i=1}^n f(\gamma(t_i)) \cdot (\gamma(x_i) - \gamma(x_{i-1})).
    \end{equation*}
    %
    Define the \emph{line integral} of $f$ with respect to $\gamma$ as the limit of the net $S_{f,\gamma}$, if the limit exists. We denote this integral by $\int f \cdot \dif\gamma$.
\end{definition}
%
Notice that if $\alpha$ and $\beta$ are properly equivalent paths, then
%
\begin{equation*}
	\int f \cdot \dif\alpha = \int f \cdot \dif\beta.
\end{equation*}
%
If $\alpha$ and $\beta$ are instead improperly equivalent, then the two integrals are equal but with opposite signs.


\begin{proposition}
	Let $\gamma \colon [a,b] \to \setR^d$ be a path, and let $f \colon \gamma^* \to \setR^d$ be a bounded function. Then
	%
	\begin{equation*}
		\int f \cdot \dif\gamma
			= \sum_{k=1}^d \int_a^b f_k \circ \gamma \dif\gamma_k
	\end{equation*}
	%
	whenever each Riemann--Stieltjes integral on the right exists. If in addition $\gamma$ is piecewise $C^1$, then
	%
	\begin{equation*}
		\int f \cdot \dif\gamma
			= \int_a^b f(\gamma(t)) \cdot \gamma'(t) \dif t.
	\end{equation*}
\end{proposition}

\begin{proof}
	Notice that
	%
	\begin{equation*}
		S_{f,\gamma}(P,T)
			= \sum_{k=1}^d \sum_{i=1}^n f_k(\gamma(t_i)) (\gamma_k(t_i) - \gamma_k(t_{i-1})).
	\end{equation*}
	%
	Since the inner sums on the right-hand side approximate the Riemann--Stieltjes integrals $\int_a^b f_k \circ \gamma \dif\gamma_k$, the first claim follows by taking limits. The second claim follows by [reference].
\end{proof}


\begin{theorem}[Integral of a gradient]
	Let $U \subseteq \setR^d$ be open, and let $\phi \in C^1(U)$. For every pair of points $x,y \in U$ and every piecewise $C^1$ path $\gamma \colon [a,b] \to U$ with $\gamma(a) = x$ and $\gamma(b) = y$ we have
	%
	\begin{equation*}
		\int \grad\phi \cdot \dif\gamma
			= \phi(y) - \phi(x).
	\end{equation*}
\end{theorem}
%
If $f = \grad\phi$, then $\phi$ is called a \emph{potential function} for $f$.

\begin{proof}
	Let $a = t_0 < \cdots < t_n = b$ be a partition of $[a,b]$ such that $\gamma'$ is continuous on each subinterval. By the chain rule,
	%
	\begin{equation*}
		(\phi \circ \gamma)'(t)
			= \grad\phi(\gamma(t)) \cdot \gamma'(t)
	\end{equation*}
	%
	on each open subinterval $(t_{i-1}, t_i)$. By [reference],
	%
	\begin{align*}
		\int \grad\phi \cdot \dif\gamma
			&= \sum_{i=1}^n \int_{t_{i-1}}^{t_i} \grad\phi(\gamma(t)) \cdot \gamma'(t) \dif t
			= \sum_{i=1}^n \int_{t_{i-1}}^{t_i} (\phi \circ \gamma)'(t) \dif t \\
			&= \phi(\gamma(b)) - \phi(\gamma(a))
			= \phi(y) - \phi(x),
	\end{align*}
	%
	as desired.
\end{proof}



\begin{theorem}
	Let $U \subseteq \setR^d$ be an open, connected set, and let $f \colon U \to \setR^d$ be a continuous function. Fix a point $x_0 \in U$. For each $x \in U$ and each pair of polygonal paths $\alpha, \beta \colon [a,b] \to U$ joining $x_0$ and $x$, assume that
	%
	\begin{equation*}
		\int f \cdot \dif\alpha
			= \int f \cdot \dif\beta.
	\end{equation*}
	%
	Then there exists a function $\phi \in C^1(U)$ such that $f = \grad\phi$.
\end{theorem}
%
Notice that since $U$ is connected, such polygonal paths exist between any pair of points.

\begin{proof}
	Let $x \in U$, and let $\alpha \colon [a,b] \to U$ be a polygonal curve joining $x_0$ and $x$. Define
	%
	\begin{equation*}
		\phi(x) = \int f \cdot \dif\alpha.
	\end{equation*}
	%
	By hypothesis, the number $\phi(x)$ does not depend on the particular choice of $\alpha$. We show that each partial derivative $D_k \phi(x)$ exists and equals $f_k(x)$.

	Let $B(x,\delta) \subseteq U$ for some $\delta > 0$, and let $\lambda \in [-\delta/2, \delta/2]$. Define a path $\gamma \colon [0,1] \to B(x,\delta)$ by $\gamma(t) = (1-t)x + t(x + \lambda e_k)$, where $e_k$ is the $k$th standard basis vector. Then
	%
	\begin{equation*}
		\phi(x + \lambda e_k) - \phi(x)
			= \int f \cdot \dif\gamma.
	\end{equation*}
	%
	Furthermore, $\gamma_k'(t) = \lambda$ and $\gamma_i'(t) = 0$ for $i \neq k$. Thus $\gamma$ is $C^1$, and so
	%
	\begin{align*}
		\phi(x + \lambda e_k) - \phi(x)
			&= \sum_{i=1}^d \int_0^1 f_i(\gamma(t)) \gamma_i'(t) \dif t \\
			&= \lambda \int_0^1 f_k(\gamma(t)) \dif t
			 = \lambda \int_0^1 g(t,\lambda) \dif t,
	\end{align*}
	%
	where $g(t,\lambda) = f_k((1-t)x + t(x + \lambda e_k))$. Since $g$ is continuous on $[0,1] \times [-\delta/2, \delta/2]$, \cref{thm:integral-continuity} implies that
	%
	\begin{equation*}
		\lim_{\lambda \to 0} \int_0^1 g(t,\lambda) \dif t
			= \int_0^1 g(t,0) \dif t
			= \int_0^1 f_k(x) \dif t
			= f_k(x),
	\end{equation*}
	%
	proving that $D_k \phi(x) = f_k(x)$. Thus $\grad\phi(x) = f(x)$ for all $x \in U$, and $\phi \in C^1(U)$ since $f$ is continuous.
\end{proof}


\chapter{Convergence}

It is well-known that a Cauchy sequence $(x_n)_{n\in\naturals}$ in a metric space $S$ converges to some $x \in S$ if and only if $(x_n)$ has a \emph{subsequence} that converges to $x$.

In this section we highlight a similar feature of convergence in measure and convergence in mean: If $(f_n)_{n\in\naturals}$ is a Cauchy sequence in measure, and if there is a subsequence that converges \emph{pointwise a.e.} to some function $f$, then $(f_n)$ also converges to $f$ in measure. Similarly for convergence in mean.

Furthermore, Markov's inequality implies that convergence in mean is stronger than convergence in measure. In particular, a sequence that is Cauchy in mean is also Cauchy in measure. Hence when we show that convergence in measure and in mean are complete, it suffices to show that being Cauchy in measure implies the existence of a pointwise a.e. convergent subsequence.

\begin{definition}[Convergence in measure]
    Let $(X,\calE,\mu)$ be a measure space. Let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$, and let further $f \in \measurable(\calE)$. We say that $(f_n)$ \emph{converges to $f$ in $\mu$-measure} if for every $\epsilon > 0$,
    %
    \begin{equation*}
        \lim_{n\to\infty} \mu\bigl( \{ \abs{f_n - f} > \epsilon \}\bigr) = 0.
    \end{equation*}
    %
    Furthermore, $(f_n)$ is called a \emph{Cauchy sequence in $\mu$-measure} if, for every $\epsilon > 0$,
    %
    \begin{equation*}
        \lim_{m,n\to\infty} \mu\bigl( \{ \abs{f_m - f_n} > \epsilon \} \bigr) = 0.
    \end{equation*}
\end{definition}

We prove that convergence in $\mu$-measure is complete.

\begin{lemma}
    \label{thm:convergence-in-measure-lemma}
    Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$. If there exists a sequence $(\epsilon_n)_{n\in\naturals}$ of strictly positive numbers such that
    %
    \begin{equation}
        \label{eq:convergence-in-measure-complete-lemma}
        \sum_{n=1}^\infty \epsilon_n < \infty
        \quad \text{and} \quad
        \sum_{n=1}^\infty \mu \bigl( \{ \abs{f_{n+1} - f_n} > \epsilon_n \} \bigr) < \infty,
    \end{equation}
    %
    then there exists a function $f \in \measurable(\calE)$ such that $(f_n)$ converges to $f$ both $\mu$-a.e. and in $\mu$-measure.
\end{lemma}

\begin{proof}
    For $n \in \naturals$, denote the set $\{ \abs{f_{n+1} - f_n} > \epsilon_n \}$ by $E_n$, and define sets $F_k = \bigunion_{n=k}^\infty E_n$ and $F = \bigintersect_{k\in\naturals} F_k$. Notice that $F = \limsup_{n\to\infty} E_n$, so the first Borel--Cantelli lemma implies that $\mu(F) = 0$.
    
    For $m \geq n$ we find that
    %
    \begin{equation}
        \label{eq:convergence-in-measure-Cauchy}
        \abs{ f_m - f_n }
            \leq \sum_{i=n}^{m-1} \abs{f_{i+1} - f_i}
            \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}.
    \end{equation}
    %
    If furthermore $x \in F_k^c$ and $m \geq n \geq k$, then
    %
    \begin{equation*}
        \abs{ f_m(x) - f_n(x) }
            \leq \sum_{i=n}^\infty \epsilon_i.
    \end{equation*}
    %
    The right-hand side converges to zero as $n \to \infty$, which shows that $(f_n(x))$ is a Cauchy sequence in $\reals$ for $x \in F_k^c$, hence for $x \in F^c$. Letting $f = \lim_{n\to\infty} f_n \indicator{F^c}$ we thus find that $(f_n)$ converges to $f \in \measurable(\calE)$ $\mu$-a.e.

    Next we show that $f_n \to f$ in $\mu$-measure as $n \to \infty$. Letting $m \to \infty$ in \cref{eq:convergence-in-measure-Cauchy} we find that
    %
    \begin{equation*}
        \abs{ f_n - f }
        \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}
    \end{equation*}
    %
    $\mu$-a.e. Now let $\epsilon > 0$, and choose an $N \in \naturals$ such that $\sum_{i=N}^\infty \epsilon_i \leq \epsilon$. For $n \geq N$, $\abs{f_n - f} > \epsilon$ then implies that
    %
    \begin{equation*}
        \sum_{i=n}^\infty \epsilon_i
            \leq \epsilon
            < \abs{f_n - f}
            \leq \sum_{i=n}^\infty \abs{f_{i+1} - f_i}
    \end{equation*}
    %
    $\mu$-a.e., which in turn implies that $\epsilon_i < \abs{f_{i+1} - f_i}$ $\mu$-a.e. for some $i \geq n$. Hence it follows that
    %
    \begin{equation*}
        \mu\bigl( \{ \abs{f_n - f} > \epsilon \}\bigr)
            \leq \mu \Bigl( \bigunion_{i=n}^\infty E_i \Bigr)
            \leq \sum_{i=n}^\infty \mu(E_i),
    \end{equation*}
    %
    which converges to zero by \cref{eq:convergence-in-measure-complete-lemma}.
\end{proof}


\begin{theorem}[Completeness of convergence in measure]
    Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in $\mu$-measure. Then there exists a function $f \in \measurable(\calE)$ such that $f_n \to f$ in $\mu$-measure. Furthermore, $(f_n)$ has a subsequence that converges to $f$ $\mu$-a.e.
\end{theorem}

\begin{proof}
    We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $(X,\calE,\mu)$ be a measure space, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in $\mu$-measure. If $(f_n)$ has a subsequence that converges $\mu$-a.e. to function $f \in \measurable(\calE)$, then $(f_n)$ also converges to $f$ in $\mu$-measure.
    \end{displaytheorem}
    %
    Let $(f_{n_k})$ be such a subsequence. For $\epsilon > 0$ we then have
    %
    \begin{equation*}
        \{ \abs{f_n - f} > \epsilon \}
            \subseteq \{ \abs{f_n - f_{n_k}} > \epsilon/2 \} \union \{ \abs{f_{n_k} - f} > \epsilon/2 \},
    \end{equation*}
    %
    and the measures of the sets on the right-hand side go to zero as $n \to \infty$ (since $n_k \geq n$). This proves the lemma.

    To prove the theorem, choose a subsequence $(g_k) = (f_{n_k})$ such that
    %
    \begin{equation*}
        \mu \bigl( \{ \abs{g_{k+1} - g_k} > 2^{-k} \} \bigr)
            \leq 2^{-k}.
    \end{equation*}
    %
    \cref{thm:convergence-in-measure-lemma} then implies the existence of a function $f \in \measurable(\calE)$ such that $g_k \to f$ both $\mu$-a.e. and in $\mu$-measure. The claim then follows from the above lemma.
\end{proof}


\begin{definition}[Convergence in mean]
    Let $(X,\calE,\mu)$ be a measure space. Let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$, and let further $f \in \measurable(\calE)$. If $p \in (0,\infty)$ we say that $(f_n)$ \emph{converges to $f$ in the $\mu$-$p$-th mean} if
    %
    \begin{equation*}
        \lim_{n\to\infty} \int_X \abs{f_n - f}^p \dif\mu = 0.
    \end{equation*}
    %
    Furthermore, $(f_n)$ is called a \emph{Cauchy sequence in the $\mu$-$p$-th mean} if
    %
    \begin{equation*}
        \lim_{m,n\to\infty} \int_X \abs{f_m - f_n}^p \dif\mu = 0.
    \end{equation*}
\end{definition}

\begin{remark}
    If $(f_n)$ converges to $f$ in the $\mu$-$p$-th mean, then $f_n - f \in \calL^p(\mu)$ for $n \geq N$ for some $N \in \naturals$. Furthermore, if $f \in \calL^p(\mu)$, then $f_n = (f_n - f) + f \in \calL^p(\mu)$ for $n \geq N$.
    
    On the other hand, if $f_n \in \calL^p(\mu)$ for large enough $n$, then $f = (f - f_n) + f_n \in \calL^p(\mu)$. In particular, $\calL^p(\mu)$ is a closed subspace of $\measurable(\calE)$.
\end{remark}


\begin{theorem}[Completeness of convergence in mean]
    Let $(X,\calE,\mu)$ be a measure space, let $p \in (0,\infty)$, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in the $\mu$-$p$-th mean. Then there exists a function $f \in \measurable(\calE)$ such that $f_n \to f$ in the $\mu$-$p$-th mean. Furthermore, $(f_n)$ has a subsequence that converges to $f$ $\mu$-a.e.

    In particular, $\calL^p(\mu)$ is complete.
\end{theorem}

\begin{proof}
    We prove the following lemma:
    %
    \begin{displaytheorem}
        Let $(X,\calE,\mu)$ be a measure space, let $p \in (0,1)$, and let $(f_n)_{n\in\naturals}$ be a sequence in $\measurable(\calE)$ that is Cauchy in the $\mu$-$p$-th mean. If $(f_n)$ has a subsequence that converges $\mu$-a.e. to function $f \in \measurable(\calE)$, then $(f_n)$ also converges to $f$ in the $\mu$-$p$-th mean.
    \end{displaytheorem}
    %
    Let $(f_{n_k})_{k\in\naturals}$ be a subsequence that converges $\mu$-a.e. to $f$. For $n \in \naturals$, Fatou's lemma implies that
    %
    \begin{align*}
        \int_X \abs{f_n - f}^p \dif\mu
            &= \int_X \liminf_{k\to\infty} \abs{f_n - f_{n_k}}^p \dif\mu
             \leq \liminf_{k\to\infty} \int_X \abs{f_n - f_{n_k}}^p \dif\mu \\
            &\leq \sup_{m \geq n} \int_X \abs{f_n - f_m}^p \dif\mu,
    \end{align*}
    %
    which converges to zero as $n \to \infty$ as desired.

    We now prove the theorem. Markov's inequality implies that $(f_n)$ is also a Cauchy sequence in $\mu$-measure, so [reference] yields a function $f \in \measurable(\calE)$ such that $f_n \to f$ in $\mu$-measure, and such that $(f_n)$ has a subsequence $(f_{n_k})_{k\in\naturals}$ that converges to $f$ $\mu$-a.e. The lemma then implies that $(f_n)$ converges to $f$ in the $\mu$-$p$-th mean.

    For the last claim, if $f_n \in \calL^p(\mu)$ for all $n \in \naturals$, then since $\calL^p(\mu)$ is closed we also have $f \in \calL^p(\mu)$.
\end{proof}


% \subsection{Completeness part 2}

% Let $X$ be a topological vector space. If $(x_n)_{n\in\naturals}$ is a sequence in $X$, then the series $\sum_{n=1}^\infty x_n$ is said to \emph{converge} to $x \in X$ if $\sum_{i=1}^n x_i \to x$ as $n \to \infty$. If $X$ is equipped with a pseudometric $\rho$, then the above series is said to be \emph{absolutely convergent} if $\sum_{n=1}^\infty \rho(x_n,0) < \infty$. If $\rho$ is invariant, then notice that
% %
% \begin{equation*}
%     \rho(x+y, 0)
%         = \rho(x, -y)
%         \leq \rho(x, 0) + \rho(0, -y)
%         = \rho(x, 0) + \rho(y, 0)
% \end{equation*}
% %
% for all $x,y \in X$. The most important case is of course when $\rho$ is induced from a seminorm $\norm{\,\cdot\,}$, in which case $\norm{x} = \rho(x,0)$ for $x \in X$.

% \begin{lemma}
%     Let $X$ be a vector space equipped with an invariant pseudometric $\rho$. Then $X$ is complete if and only if every absolutely convergent series in $X$ converges.
% \end{lemma}

% \begin{proof}
%     First assume that $X$ is complete, and consider a sequence $(x_n)_{n\in\naturals}$ in $X$ with $\sum_{n=1}^\infty \rho(x_n,0) < \infty$. Letting $s_n = \sum_{i=1}^n x_i$, if $m < n$ then
%     %
%     \begin{equation*}
%         \rho(s_m, s_n)
%             = \rho(s_n - s_m, 0)
%             \leq \sum_{i = m+1}^n \rho(x_i, 0)
%             \to 0
%     \end{equation*}
%     %
%     as $m,n \to \infty$. Thus $(s_n)$ is Cauchy and hence convergent.

%     Conversely, assume that every absolutely convergent sequence in $X$ converges, and let $(x_n)_{n\in\naturals}$ be a Cauchy sequence in $X$. Choose a strictly increasing sequence $(n_k)_{k\in\naturals}$ of natural numbers such that $\rho(x_m,x_n) < 2^{-k}$ for $m,n \geq n_k$. Let $y_1 = x_{n_1}$ and $y_k = x_{n_k} - x_{n_{k-1}}$ for $k > 1$. Then $\sum_{i=1}^k y_i = x_k$, and
%     %
%     \begin{equation*}
%         \sum_{k=1}^\infty \rho(y_k, 0)
%             \leq \rho(y_1, 0) + \sum_{k=1}^\infty 2^{-k}
%             = \rho(y_1, 0) + 1
%             < \infty.
%     \end{equation*}
%     %
%     Thus $\lim_{k\to\infty} x_{n_k} = \sum_{k=1}^\infty y_k$ exists, and so $(x_n)$ is a Cauchy sequence with a convergent subsequence, and hence is itself convergent.
% \end{proof}


% \subsection{Completeness part 3}


\chapter{Portmanteau theorems}

If $P$ is a Borel probability measure on a topological space $X$, then a subset $A \subseteq X$ is called a \emph{$P$-continuity set} if $P(\boundary A) = 0$.

Recall that a topological space $X$ is called a \emph{perfect space} or a \emph{$G_\delta$ space} if every closed subset of $X$ is a $G_\delta$ set, i.e. a countable intersection of open sets. (Equivalently, if every open subset is an $F_\sigma$ set, a countable union of closed sets.) Furthermore, if $A$ and $B$ are closed subsets of $X$, a \emph{Urysohn function} for the pair $(A,B)$ is a continuous function $f \colon X \to [0,1]$ with $f(A) = 1$ and $f(B) = 0$.\footnote{The ordering of $A$ and $B$ is only relevant insofar as it is relevant on which set the Urysohn function in question vanishes. If $f \colon X \to [0,1]$ is a Urysohn function for the pair $(A,B)$, then the function $1-f$ is a Urysohn function for the pair $(B,A)$.} By Urysohn's lemma, a topological space is normal (i.e. every pair of disjoint closed sets can be separated by disjoint open sets) if and only if there is a Urysohn function for every pair of disjoint closed subsets. Finally, a (not necessarily Hausdorff) normal $G_\delta$ space is called a \emph{perfectly normal space}. If it is also Hausdorff it also called a \emph{$T_6$-space} or a \emph{perfectly $T_4$ space}.

It is easy to show that (pseudo-)metrisable spaces are perfectly normal. Another notable class of perfectly normal (Hausdorff) spaces are the CW complexes. [TODO: Reference, proof somewhere else maybe?]

\begin{theorem}
    Let $(P_n)_{n\in\naturals}$ and $P$ be probability measures on a perfectly normal space $X$. Then the following conditions are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:portmanteau-weak-convergence} $P_n \wto P$.

        \item \label{enum:portmanteau-closed} $\limsup_{n\to\infty} P_n(F) \leq P(F)$ for all closed $F \subseteq X$.

        \item \label{enum:portmanteau-open} $\liminf_{n\to\infty} P_n(G) \geq P(G)$ for all open $G \subseteq X$.

        \item \label{enum:portmanteau-continuity-sets} $P_n(A) \to P(A)$ for all $P$-continuity sets $A \subseteq X$.
    \end{enumthm}
\end{theorem}


\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:portmanteau-weak-convergence} $\implies$ \subcref{enum:portmanteau-closed}]
    Let $F$ be a closed subset of $X$. Since $X$ is perfect, there is a decreasing sequence $(F_k)_{k\in\naturals}$ of open sets such that $F = \bigintersect_{k\in\naturals} F_k$. Furthermore, since $X$ is normal there is for each $k \in \naturals$ a Urysohn function $g_k$ for the pair $(F,F_k^c)$. We clearly have $\indicator{F} \leq g_k \leq \indicator{F_k}$ so
    %
    \begin{equation*}
        \limsup_{n\to\infty} P_n(F)
            \leq \limsup_{n\to\infty} \int_X g_k \dif P_n
            = \int_X g_k \dif P
            \leq P(F_k).
    \end{equation*}
    %
    Since $F_k \downarrow F$ as $k \to \infty$, continuity of $P$ implies the claim.

    \item[\subcref{enum:portmanteau-closed} $\Leftrightarrow$ \subcref{enum:portmanteau-open}]
    This follows easily by taking complements.

    \item[\subcref{enum:portmanteau-closed} \& \subcref{enum:portmanteau-open} $\implies$ \subcref{enum:portmanteau-continuity-sets}]
    For $A \subseteq X$ we have
    %
    \begin{align*}
        P(\interior{A})
            &\leq \liminf_{n\to\infty} P_n(\interior{A})
             \leq \liminf_{n\to\infty} P_n(A) \\
            &\leq \limsup_{n\to\infty} P_n(A)
             \leq \limsup_{n\to\infty} P_n(\closure{A})
             \leq P(\closure{A}).
    \end{align*}
    %
    If $A$ is a $P$-continuity set then $P(\interior{A}) = P(\closure{A})$, which implies \subcref{enum:portmanteau-continuity-sets}.

    \item[\subcref{enum:portmanteau-continuity-sets} $\implies$ \subcref{enum:portmanteau-weak-convergence}]
    Given $f \in C_b(X)$, by linearity we may assume that $0 \leq f \leq 1$. Then
    %
    \begin{equation*}
        \int_X f \dif P
            = \int_0^\infty P( f \geq t ) \dif t
            = \int_0^1 P( f \geq t ) \dif t,
    \end{equation*}
    %
    and similarly for $P_n$. Since $f$ is continuous, we have $\boundary \{ f \geq t \} \subseteq \{ f = t \}$. Now notice that $\{ f = t \}$ is a $P$-null set except for countably many $t \in (0,1)$, since $P$ is a finite measure.\footnote{Indeed, $f$ is a random variable whose discrete support is precisely this set of $t$s. But the discrete support of any random variable is countable.} Hence $\{ f \geq t \}$ is a $P$-continuity set for all but countably many $t$. It then follows from \subcref{enum:portmanteau-continuity-sets} and the dominated convergence theorem that
    %
    \begin{equation*}
        \int_X f \dif P_n
            = \int_0^1 P_n( f \geq t ) \dif t
            \xrightarrow[n\to\infty]{} \int_0^1 P( f \geq t ) \dif t
            = \int_X f \dif P
    \end{equation*}
    %
    as claimed.
\end{proofsec}
\end{proof}


\begin{remark}
    In the case that $X$ is pseudometrisable, we may pick a pseudometric $\rho$. The sets $F_k$ can then be given explicitly by
    %
    \begin{equation*}
        F_k
            = \set[\bigg]{x \in S}{\rho(x,F) < \frac{1}{k}}.
    \end{equation*}
    %
    Furthermore, we may take the Urysohn functions $g_k$ to be given by $g_k(x) = (1 - k\rho(x,F)) \join 0$. Notice that these are Lipschitz since
    %
    \begin{align*}
        \abs{ g_k(x) - g_k(y) }
            &= \abs[\big]{ (1 - k\rho(x,F)) \join 0 - (1 - k\rho(y,F)) \join 0 } \\
            &\leq \abs{ k\rho(x,F) - k\rho(y,F) }
             \leq k \rho(x,y)
    \end{align*}
    %
    for all $x,y \in X$. In particular they are uniformly continuous. Hence weak convergence in a metrisable space may be characterised by the bounded, uniformly continuous functions, or even the bounded Lipschitz functions. Notice also that this is independent of the metric.
\end{remark}


















\chapter{Dynkin systems and monotone classes}

\begin{definition}[Dynkin systems, $\pi$-systems]
    Let $X$ be a set. A collection $\calD$ of subsets of $X$ is a \emph{Dynkin system} in $X$ if it has the following properties:
    %
    \begin{enumdef}
        \item $X \in \calD$,
        \item $B \setminus A \in \calD$ whenever $A,B \in \calD$ and $A \subseteq B$, and
        \item $\bigunion_{n\in\naturals} A_n \in \calD$ for any increasing sequence $(A_n)_{n\in\naturals}$ of sets in $\calD$.
    \end{enumdef}
    %
    Furthermore, a collection $\calS$ of subsets of $X$ is called a \emph{$\pi$-system} if it is closed under intersections.
\end{definition}
%
It is easy to show that if $\calD$ is both a Dynkin system in $X$ and a $\pi$-system, then it is in fact a $\sigma$-algebra in $X$.

\begin{definition}[Monotone classes, set algebras]
    Let $X$ be a set. A collection $\calM$ of subsets of $X$ is a \emph{monotone class} if it is closed under countable increasing unions and countable decreasing intersections.

    Furthermore, a collection $\calA$ of subsets of $X$ is called a \emph{set algebra} in $X$ if it is closed under finite unions and complements.
\end{definition}
%
We note that a set algebra $\calA$ in $X$ is automatically closed under finite intersections, and that it also contains both $\emptyset$ and $X$. It is easy to show that if $\calM$ is both a monotone class and a set algebra in $X$, then it is in fact a $\sigma$-algebra in $X$.

Notice that we have two pairs of properties that ensure that a collection of sets is a $\sigma$-algebra. On the one hand we should think of Dynkin systems and monotone classes as being analogous, and similarly for $\pi$-systems and set algebras on the other. The latter pair of properties are algebraic, while the first pair are somehow analytic (or continuous), in that they involve infinitely many operations.

It turns out that if $\calS$ is a $\pi$-system, then the Dynkin system $\delta(\calS)$ generated by $\calS$ is also a $\pi$-system. Similarly, if $\calA$ is a set algebra, then the monotone class $M(\calA)$ generated by $\calA$ is also a set algebra. We have the following two results whose proofs are basically identical:

\begin{theorem}[Dynkin's lemma]
    Let $\calS$ be a $\pi$-system in a set $X$. Then $\delta(\calS)$ is also a $\pi$-system, and in particular
    %
    \begin{equation}
        \label{eq:Dynkins-lemma}
        \delta(\calS) = \sigma(\calS).
    \end{equation}
\end{theorem}

\begin{proof}
    The identity \cref{eq:Dynkins-lemma} follows if $\delta(\calS)$ is a $\pi$-system: For then it is also a $\sigma$-algebra, and then $\sigma(\calS) \subseteq \delta(\calS)$.

    For $A \in \delta(\calS)$ define
    %
    \begin{equation*}
        \calD_A
            = \set{B \in \delta(\calS)}{A \intersect B \in \delta(\calS)}.
    \end{equation*}
    %
    This is easily seen to be a Dynkin system. Also notice that $B \in \calD_A$ if and only if $A \in \calD_B$ for all $A,B \in \delta(\calS)$. Furthermore, if $A \in \calS$ then $\calS \subseteq \calD_A$, and so $\delta(\calS) \subseteq \calD_A$. In other words,
    %
    \begin{equation*}
        B \in \calD_A
        \quad
        \text{for $A \in \calS$ and $B \in \delta(\calS)$}.
    \end{equation*}
    %
    By symmetry we then have
    %
    \begin{equation*}
        A \in \calD_B
        \quad
        \text{for $A \in \calS$ and $B \in \delta(\calS)$},
    \end{equation*}
    %
    and since $\calD_B$ is a Dynkin system it follows that $\delta(\calS) \subseteq \calD_B$. Hence $\delta(\calS)$ is a $\pi$-system as desired.
\end{proof}


\begin{theorem}[The monotone class lemma]
    Let $\calA$ be a set algebra in a set $X$. Then $M(\calA)$ is also a set algebra, and in particular
    %
    \begin{equation}
        \label{eq:monotone-class-lemma}
        M(\calA) = \sigma(\calA).
    \end{equation}
\end{theorem}

\begin{proof}
    The identity \cref{eq:monotone-class-lemma} follows if $M(\calA)$ is a set algebra: For then it is also a $\sigma$-algebra, and then $\sigma(\calA) \subseteq M(\calA)$.

    For $A \in M(\calA)$ define
    %
    \begin{equation*}
        \calM_A
            = \set{B \in M(\calA)}{\text{$A \setminus B$, $B \setminus A$, and $A \intersect B$ are in $M(\calA)$}}.
    \end{equation*}
    %
    This is easily seen to be a monotone class. Also notice that $B \in \calM_A$ if and only if $A \in \calM_B$ for all $A,B \in M(\calA)$. Furthermore, if $A \in \calA$ then $\calA \subseteq \calM_A$, and so $M(\calA) \subseteq \calM_A$. In other words,
    %
    \begin{equation*}
        B \in \calM_A
        \quad
        \text{for $A \in \calA$ and $B \in M(\calA)$}.
    \end{equation*}
    %
    By symmetry we then have
    %
    \begin{equation*}
        A \in \calM_B
        \quad
        \text{for $A \in \calA$ and $B \in M(\calA)$},
    \end{equation*}
    %
    and since $\calM_B$ is a monotone class it follows that $M(\calA) \subseteq \calM_B$. Hence $M(\calA)$ is a set algebra as desired.
\end{proof}


\chapter{Complex analysis}

[Should it be here?]

\newcommand{\diam}{\operatorname{diam}}
\newcommand{\hol}{\mathcal{H}}

If $V \subseteq \complex$ is open and $f \colon V \to \complex$ is differentiable at every point in $V$, then we say that $f$ is \emph{holomorphic} on $V$. The set of functions holomorphic on $V$ is denoted $\hol(V)$.

\begin{theorem}[The Cauchy--Goursat Lemma]
    If $f \in \hol(V)$, then
    %
    \begin{equation*}
        \int_{\boundary T} f(x) \dif z = 0
    \end{equation*}
    %
    for every triangle $T \subseteq V$.
\end{theorem}

\begin{proof}
    Notice that any triangle $T$ can be subdivided into four smaller triangles $T^1, \ldots, T^4$ whose corners are the corners and midpoints of the sides of $T$. We then clearly have
    %
    \begin{equation*}
        \int_{\boundary T} g(z) \dif z
            = \sum_{i=1}^4 \int_{\boundary T^i} g(z) \dif z
    \end{equation*}
    %
    for all $g \in C(T)$.

    Let $T_0 \subseteq V$ be a triangle, and consider the integral
    %
    \begin{equation*}
        I
            = \int_{\boundary T_0} f(z) \dif z.
    \end{equation*}
    %
    By the above considerations we have
    %
    \begin{equation*}
        \abs{I}
            \leq 4 \abs[\bigg]{ \int_{\boundary T_0^i} f(z) \dif z }
    \end{equation*}
    %
    for at least one value of $i$. For this value of $i$ let $T_1 = T_0^i$. Continuing this process yields a sequence $(T_n)_{n\in\naturals}$ of triangles such that
    %
    \begin{equation*}
        \abs{I}
            \leq 4^n \abs[\bigg]{ \int_{\boundary T_n} f(z) \dif z }
    \end{equation*}
    %
    for $n \in \naturals_0$.

    Furthermore, each of the four triangles in a subdivision of a triangle $T$ have side lengths half of those of $T$, so
    %
    \begin{equation*}
        \diam T_n
            = 2^{-n} \diam T_0
    \end{equation*}
    %
    for $n \in \naturals_0$. Thus there exists a point $z_0 \in \complex$ such that $\bigintersect_{n\in\naturals_0} T_n = \{ z_0 \}$ since $(T_n)$ is a sequence of closed sets whose diameters tend to zero. It follows that
    %
    \begin{equation*}
        \sup_{z \in \boundary T_n} \abs{z - z_0}
            \leq 2^{-n} \diam T_0.
    \end{equation*}
    %
    Given $\epsilon > 0$ there exists an $r > 0$ such that
    %
    \begin{equation*}
        \abs{ f(z) - f(z_0) - f'(z_0)(z - z_0) }
            \leq \epsilon \abs{z - z_0}
    \end{equation*}
    %
    for $z \in B(z_0,r)$, and there further exists an $N \in \naturals$ such that $n \geq N$ implies $T_n \subseteq B(z_0,r)$. Now notice that the function $z \mapsto f(z_0) + f'(z_0)(z - z_0)$ has an antiderivative, so its integral along $\boundary T_n$ is zero. Denoting the length of the curve $\boundary T_n$ by $L_n$ we have $L_n \leq 2 \diam T_n$. Hence,
    %
    \begin{align*}
        \abs[\bigg]{ \int_{\boundary T_n} f(z) \dif z }
            &\leq \int_{\boundary T_n} \abs{ f(z) - f(z_0) - f'(z_0)(z - z_0) } \dif z \\
            &\leq L_n \epsilon \sup_{z \in \boundary T_n} \abs{z - z_0} \\
            &\leq 2^{-2n+1} \epsilon (\diam T_0)^2
    \end{align*}
    %
    for $n \geq N$, and so
    %
    \begin{equation*}
        \abs{I}
            \leq 2 \epsilon (\diam T_0)^2.
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, it follows that $I = 0$ as desired.
\end{proof}


\chapter{The extended real line}

Let $+\infty$ (or simply $\infty$) and $-\infty$ denote elements disjoint from $\reals$. The \emph{extended real line}, as a set, is then the union $\exreals = \reals \union \{\pm\infty\}$. We extend the ordering on $\reals$ to $\exreals$ by declaring that $-\infty < a$ for all $a \neq -\infty$ and that $\infty > a$ for all $a \neq \infty$. This clearly makes $\exreals$ into a totally ordered set. We further equip it with order topology, i.e. the topology generated by open rays $\set{x \in \exreals}{a < x}$ and $\set{x \in \exreals}{x < b}$ for all $a,b \in \exreals$. One easily sees that this is a Hausdorff topology (in fact it is $T_5$, which all order topologies are), so in particular singletons are closed and $\reals$ is open.

Since $\exreals$ is a topological space we can consider the Borel $\sigma$-algebra $\borel(\exreals)$. Before characterising this we recall two elementary results:
%
\begin{enumerate}
    \item If $(X,\calT)$ is a topological space and $A \subseteq X$ is any subspace, then $\borel(A) = \borel(X)_A$. That is, the Borel $\sigma$-algebra generated by the subspace topology on $A$ agrees with the restriction of the Borel $\sigma$-algebra on $X$ to $A$.

    \item If $(X,\calE)$ is a measurable space and $A \in \calE$, then
    %
    \begin{equation*}
        \calE
            = \set{E \union F}{E \in \calE_A, F \in \calE_{A^c}}.
    \end{equation*}
    %
    The inclusion \enquote{$\supseteq$} is obvious, and the other inclusion follows since if $B \in \calE$, then
    %
    \begin{equation*}
        B
            = (B \intersect A) \union (B \intersect A^c),
    \end{equation*}
    %
    and $B \intersect A \in \calE_A$ and $B \intersect A^c \in \calE_{A^c}$ by the definition of the subspace $\sigma$-algebra.
\end{enumerate}
%
This easily implies the following result:

\begin{proposition}
    The Borel $\sigma$-algebra on $\exreals$ is given by
    %
    \begin{equation*}
        \borel(\exreals)
            = \set[\big]{A \union S}{A \in \borel(\reals), S \subseteq \{\pm\infty\}}
            = \set{B \subseteq \exreals}{B \intersect \reals \in \borel(\reals)}.
    \end{equation*}
\end{proposition}

\begin{proof}
    To prove the first equality we only need to verify that $\borel(\{\pm\infty\}) = 2^{\{\pm\infty\}}$. But this is obvious since both sets $\{\infty\}$ and $\{-\infty\}$ are closed. The second equality easily follows from the first. (Note that we cannot simply use the fact that we can characterise $\borel(\reals)$ in terms of $\borel(\exreals)$, which we can do since $\reals$ is a subset of $\exreals$, since we are trying to do the exact opposite.)
\end{proof}

\nocite{*}

\printbibliography


\end{document}
